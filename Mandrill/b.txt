
/home/bombe/target-project/powerpc-module/kernel/linux-2.6.38/block/blk-core.o:     file format elf32-powerpc


Disassembly of section .text:

00000000 <blk_get_backing_dev_info>:
				  struct request *, int, rq_end_io_fn *);
extern void blk_unplug(struct request_queue *q);

static inline struct request_queue *bdev_get_queue(struct block_device *bdev)
{
	return bdev->bd_disk->queue;
       0:	81 23 00 50 	lwz     r9,80(r3)
 *
 * Will return NULL if the request queue cannot be located.
 */
struct backing_dev_info *blk_get_backing_dev_info(struct block_device *bdev)
{
	struct backing_dev_info *ret = NULL;
       4:	38 60 00 00 	li      r3,0
       8:	81 29 01 6c 	lwz     r9,364(r9)
	struct request_queue *q = bdev_get_queue(bdev);

	if (q)
       c:	2f 89 00 00 	cmpwi   cr7,r9,0
      10:	4d 9e 00 20 	beqlr   cr7
		ret = &q->backing_dev_info;
      14:	38 69 00 a0 	addi    r3,r9,160
	return ret;
}
      18:	4e 80 00 20 	blr

0000001c <blk_backing_dev_unplug>:
}
EXPORT_SYMBOL(generic_unplug_device);

static void blk_backing_dev_unplug(struct backing_dev_info *bdi,
				   struct page *page)
{
      1c:	94 21 ff f0 	stwu    r1,-16(r1)
      20:	7c 08 02 a6 	mflr    r0
      24:	90 01 00 14 	stw     r0,20(r1)
	struct request_queue *q = bdi->unplug_io_data;
      28:	80 63 00 20 	lwz     r3,32(r3)
void blk_unplug(struct request_queue *q)
{
	/*
	 * devices don't necessarily have an ->unplug_fn defined
	 */
	if (q->unplug_fn) {
      2c:	80 03 00 48 	lwz     r0,72(r3)
      30:	2f 80 00 00 	cmpwi   cr7,r0,0
      34:	41 9e 00 0c 	beq-    cr7,40 <blk_backing_dev_unplug+0x24>
		trace_block_unplug_io(q);
		q->unplug_fn(q);
      38:	7c 09 03 a6 	mtctr   r0
      3c:	4e 80 04 21 	bctrl
				   struct page *page)
{
	struct request_queue *q = bdi->unplug_io_data;

	blk_unplug(q);
}
      40:	80 01 00 14 	lwz     r0,20(r1)
      44:	38 21 00 10 	addi    r1,r1,16
      48:	7c 08 03 a6 	mtlr    r0
      4c:	4e 80 00 20 	blr

00000050 <blk_unplug_work>:

void blk_unplug_work(struct work_struct *work)
{
      50:	94 21 ff f0 	stwu    r1,-16(r1)
      54:	7c 08 02 a6 	mflr    r0
      58:	90 01 00 14 	stw     r0,20(r1)
	struct request_queue *q =
		container_of(work, struct request_queue, unplug_work);

	trace_block_unplug_io(q);
	q->unplug_fn(q);
      5c:	80 03 ff b8 	lwz     r0,-72(r3)
      60:	38 63 ff 70 	addi    r3,r3,-144
      64:	7c 09 03 a6 	mtctr   r0
      68:	4e 80 04 21 	bctrl
}
      6c:	80 01 00 14 	lwz     r0,20(r1)
      70:	38 21 00 10 	addi    r1,r1,16
      74:	7c 08 03 a6 	mtlr    r0
      78:	4e 80 00 20 	blr

0000007c <blk_unplug>:
	trace_block_unplug_timer(q);
	kblockd_schedule_work(q, &q->unplug_work);
}

void blk_unplug(struct request_queue *q)
{
      7c:	94 21 ff f0 	stwu    r1,-16(r1)
      80:	7c 08 02 a6 	mflr    r0
      84:	90 01 00 14 	stw     r0,20(r1)
	/*
	 * devices don't necessarily have an ->unplug_fn defined
	 */
	if (q->unplug_fn) {
      88:	80 03 00 48 	lwz     r0,72(r3)
      8c:	2f 80 00 00 	cmpwi   cr7,r0,0
      90:	41 9e 00 0c 	beq-    cr7,9c <blk_unplug+0x20>
		trace_block_unplug_io(q);
		q->unplug_fn(q);
      94:	7c 09 03 a6 	mtctr   r0
      98:	4e 80 04 21 	bctrl
	}
}
      9c:	80 01 00 14 	lwz     r0,20(r1)
      a0:	38 21 00 10 	addi    r1,r1,16
      a4:	7c 08 03 a6 	mtlr    r0
      a8:	4e 80 00 20 	blr

000000ac <blk_rq_err_bytes>:
 * Context:
 *     queue_lock must be held.
 */
unsigned int blk_rq_err_bytes(const struct request *rq)
{
	unsigned int ff = rq->cmd_flags & REQ_FAILFAST_MASK;
      ac:	81 43 00 20 	lwz     r10,32(r3)
	unsigned int bytes = 0;
	struct bio *bio;

	if (!(rq->cmd_flags & REQ_MIXED_MERGE))
      b0:	75 40 04 00 	andis.  r0,r10,1024
      b4:	41 82 00 78 	beq-    12c <blk_rq_err_bytes+0x80>
	 * different fastfail types.  We can safely fail portions
	 * which have all the failfast bits that the first one has -
	 * the ones which are at least as eager to fail as the first
	 * one.
	 */
	for (bio = rq->bio; bio; bio = bio->bi_next) {
      b8:	81 23 00 40 	lwz     r9,64(r3)
 *     queue_lock must be held.
 */
unsigned int blk_rq_err_bytes(const struct request *rq)
{
	unsigned int ff = rq->cmd_flags & REQ_FAILFAST_MASK;
	unsigned int bytes = 0;
      bc:	38 00 00 00 	li      r0,0
	 * different fastfail types.  We can safely fail portions
	 * which have all the failfast bits that the first one has -
	 * the ones which are at least as eager to fail as the first
	 * one.
	 */
	for (bio = rq->bio; bio; bio = bio->bi_next) {
      c0:	2f 89 00 00 	cmpwi   cr7,r9,0
      c4:	41 9e 00 40 	beq-    cr7,104 <blk_rq_err_bytes+0x58>
		if ((bio->bi_rw & ff) != ff)
      c8:	81 69 00 14 	lwz     r11,20(r9)
 * Context:
 *     queue_lock must be held.
 */
unsigned int blk_rq_err_bytes(const struct request *rq)
{
	unsigned int ff = rq->cmd_flags & REQ_FAILFAST_MASK;
      cc:	55 4a 07 3c 	rlwinm  r10,r10,0,28,30
	 * which have all the failfast bits that the first one has -
	 * the ones which are at least as eager to fail as the first
	 * one.
	 */
	for (bio = rq->bio; bio; bio = bio->bi_next) {
		if ((bio->bi_rw & ff) != ff)
      d0:	7d 4b 58 38 	and     r11,r10,r11
      d4:	7f 8b 50 00 	cmpw    cr7,r11,r10
      d8:	41 be 00 18 	beq+    cr7,f0 <blk_rq_err_bytes+0x44>
      dc:	48 00 00 28 	b       104 <blk_rq_err_bytes+0x58>
      e0:	81 49 00 14 	lwz     r10,20(r9)
      e4:	7d 6a 50 38 	and     r10,r11,r10
      e8:	7f 8a 58 00 	cmpw    cr7,r10,r11
      ec:	40 9e 00 18 	bne-    cr7,104 <blk_rq_err_bytes+0x58>
			break;
		bytes += bio->bi_size;
      f0:	81 49 00 20 	lwz     r10,32(r9)
	 * different fastfail types.  We can safely fail portions
	 * which have all the failfast bits that the first one has -
	 * the ones which are at least as eager to fail as the first
	 * one.
	 */
	for (bio = rq->bio; bio; bio = bio->bi_next) {
      f4:	81 29 00 08 	lwz     r9,8(r9)
		if ((bio->bi_rw & ff) != ff)
			break;
		bytes += bio->bi_size;
      f8:	7c 00 52 14 	add     r0,r0,r10
	 * different fastfail types.  We can safely fail portions
	 * which have all the failfast bits that the first one has -
	 * the ones which are at least as eager to fail as the first
	 * one.
	 */
	for (bio = rq->bio; bio; bio = bio->bi_next) {
      fc:	2f 89 00 00 	cmpwi   cr7,r9,0
     100:	40 9e ff e0 	bne+    cr7,e0 <blk_rq_err_bytes+0x34>
			break;
		bytes += bio->bi_size;
	}

	/* this could lead to infinite loop */
	BUG_ON(blk_rq_bytes(rq) && !bytes);
     104:	81 63 00 30 	lwz     r11,48(r3)
     108:	39 20 00 00 	li      r9,0
     10c:	2f 8b 00 00 	cmpwi   cr7,r11,0
     110:	40 9e 00 10 	bne-    cr7,120 <blk_rq_err_bytes+0x74>
     114:	0f 09 00 00 	twnei   r9,0
	return bytes;
}
     118:	7c 03 03 78 	mr      r3,r0
     11c:	4e 80 00 20 	blr
			break;
		bytes += bio->bi_size;
	}

	/* this could lead to infinite loop */
	BUG_ON(blk_rq_bytes(rq) && !bytes);
     120:	7c 09 00 34 	cntlzw  r9,r0
     124:	55 29 d9 7e 	rlwinm  r9,r9,27,5,31
     128:	4b ff ff ec 	b       114 <blk_rq_err_bytes+0x68>

	blk_requestq_cachep = kmem_cache_create("blkdev_queue",
			sizeof(struct request_queue), 0, SLAB_PANIC, NULL);

	return 0;
}
     12c:	80 03 00 30 	lwz     r0,48(r3)
	}

	/* this could lead to infinite loop */
	BUG_ON(blk_rq_bytes(rq) && !bytes);
	return bytes;
}
     130:	7c 03 03 78 	mr      r3,r0
     134:	4e 80 00 20 	blr

00000138 <blk_unprep_request>:
 * so represents the appropriate moment to deallocate any resources
 * that were allocated to the request in the prep_rq_fn.  The queue
 * lock is held when calling this.
 */
void blk_unprep_request(struct request *req)
{
     138:	94 21 ff f0 	stwu    r1,-16(r1)
     13c:	7c 08 02 a6 	mflr    r0
     140:	7c 64 1b 78 	mr      r4,r3
     144:	90 01 00 14 	stw     r0,20(r1)
	struct request_queue *q = req->q;

	req->cmd_flags &= ~REQ_DONTPREP;
     148:	80 03 00 20 	lwz     r0,32(r3)
 * that were allocated to the request in the prep_rq_fn.  The queue
 * lock is held when calling this.
 */
void blk_unprep_request(struct request *req)
{
	struct request_queue *q = req->q;
     14c:	80 63 00 1c 	lwz     r3,28(r3)

	req->cmd_flags &= ~REQ_DONTPREP;
     150:	54 00 04 1c 	rlwinm  r0,r0,0,16,14
     154:	90 04 00 20 	stw     r0,32(r4)
	if (q->unprep_rq_fn)
     158:	80 03 00 44 	lwz     r0,68(r3)
     15c:	2f 80 00 00 	cmpwi   cr7,r0,0
     160:	41 9e 00 0c 	beq-    cr7,16c <blk_unprep_request+0x34>
		q->unprep_rq_fn(q, req);
     164:	7c 09 03 a6 	mtctr   r0
     168:	4e 80 04 21 	bctrl
}
     16c:	80 01 00 14 	lwz     r0,20(r1)
     170:	38 21 00 10 	addi    r1,r1,16
     174:	7c 08 03 a6 	mtlr    r0
     178:	4e 80 00 20 	blr

0000017c <blk_lld_busy>:
 * Return:
 *    0 - Not busy (The request stacking driver should dispatch request)
 *    1 - Busy (The request stacking driver should stop dispatching request)
 */
int blk_lld_busy(struct request_queue *q)
{
     17c:	94 21 ff f0 	stwu    r1,-16(r1)
     180:	7c 08 02 a6 	mflr    r0
     184:	90 01 00 14 	stw     r0,20(r1)
	if (q->lld_busy_fn)
		return q->lld_busy_fn(q);

	return 0;
     188:	38 00 00 00 	li      r0,0
 *    0 - Not busy (The request stacking driver should dispatch request)
 *    1 - Busy (The request stacking driver should stop dispatching request)
 */
int blk_lld_busy(struct request_queue *q)
{
	if (q->lld_busy_fn)
     18c:	81 23 00 5c 	lwz     r9,92(r3)
     190:	2f 89 00 00 	cmpwi   cr7,r9,0
     194:	41 9e 00 10 	beq-    cr7,1a4 <blk_lld_busy+0x28>
		return q->lld_busy_fn(q);
     198:	7d 29 03 a6 	mtctr   r9
     19c:	4e 80 04 21 	bctrl
     1a0:	7c 60 1b 78 	mr      r0,r3

	return 0;
}
     1a4:	7c 03 03 78 	mr      r3,r0
     1a8:	80 01 00 14 	lwz     r0,20(r1)
     1ac:	38 21 00 10 	addi    r1,r1,16
     1b0:	7c 08 03 a6 	mtlr    r0
     1b4:	4e 80 00 20 	blr

000001b8 <kblockd_schedule_work>:
	return -ENOMEM;
}
EXPORT_SYMBOL_GPL(blk_rq_prep_clone);

int kblockd_schedule_work(struct request_queue *q, struct work_struct *work)
{
     1b8:	94 21 ff f0 	stwu    r1,-16(r1)
     1bc:	7c 08 02 a6 	mflr    r0
	return queue_work(kblockd_workqueue, work);
     1c0:	3d 20 00 00 	lis     r9,0
     1c4:	80 69 00 00 	lwz     r3,0(r9)
	return -ENOMEM;
}
EXPORT_SYMBOL_GPL(blk_rq_prep_clone);

int kblockd_schedule_work(struct request_queue *q, struct work_struct *work)
{
     1c8:	90 01 00 14 	stw     r0,20(r1)
	return queue_work(kblockd_workqueue, work);
     1cc:	48 00 00 01 	bl      1cc <kblockd_schedule_work+0x14>
}
     1d0:	80 01 00 14 	lwz     r0,20(r1)
     1d4:	38 21 00 10 	addi    r1,r1,16
     1d8:	7c 08 03 a6 	mtlr    r0
     1dc:	4e 80 00 20 	blr

000001e0 <blk_rq_unprep_clone>:
 *
 * Description:
 *     Free all bios in @rq for a cloned request.
 */
void blk_rq_unprep_clone(struct request *rq)
{
     1e0:	94 21 ff f0 	stwu    r1,-16(r1)
     1e4:	7c 08 02 a6 	mflr    r0
     1e8:	93 e1 00 0c 	stw     r31,12(r1)
     1ec:	7c 7f 1b 78 	mr      r31,r3
     1f0:	90 01 00 14 	stw     r0,20(r1)
	struct bio *bio;

	while ((bio = rq->bio) != NULL) {
     1f4:	80 63 00 40 	lwz     r3,64(r3)
     1f8:	2f 83 00 00 	cmpwi   cr7,r3,0
     1fc:	41 9e 00 1c 	beq-    cr7,218 <blk_rq_unprep_clone+0x38>
		rq->bio = bio->bi_next;
     200:	80 03 00 08 	lwz     r0,8(r3)
     204:	90 1f 00 40 	stw     r0,64(r31)

		bio_put(bio);
     208:	48 00 00 01 	bl      208 <blk_rq_unprep_clone+0x28>
 */
void blk_rq_unprep_clone(struct request *rq)
{
	struct bio *bio;

	while ((bio = rq->bio) != NULL) {
     20c:	80 7f 00 40 	lwz     r3,64(r31)
     210:	2f 83 00 00 	cmpwi   cr7,r3,0
     214:	40 9e ff ec 	bne+    cr7,200 <blk_rq_unprep_clone+0x20>
		rq->bio = bio->bi_next;

		bio_put(bio);
	}
}
     218:	80 01 00 14 	lwz     r0,20(r1)
     21c:	83 e1 00 0c 	lwz     r31,12(r1)
     220:	38 21 00 10 	addi    r1,r1,16
     224:	7c 08 03 a6 	mtlr    r0
     228:	4e 80 00 20 	blr

0000022c <rq_flush_dcache_pages>:
 *
 * Description:
 *     Flush all pages in @rq.
 */
void rq_flush_dcache_pages(struct request *rq)
{
     22c:	94 21 ff e0 	stwu    r1,-32(r1)
     230:	7c 08 02 a6 	mflr    r0
     234:	bf a1 00 14 	stmw    r29,20(r1)
     238:	90 01 00 24 	stw     r0,36(r1)
	struct req_iterator iter;
	struct bio_vec *bvec;

	rq_for_each_segment(bvec, rq, iter)
     23c:	83 a3 00 40 	lwz     r29,64(r3)
     240:	2f 9d 00 00 	cmpwi   cr7,r29,0
     244:	41 9e 00 54 	beq-    cr7,298 <rq_flush_dcache_pages+0x6c>
     248:	a3 fd 00 1a 	lhz     r31,26(r29)
     24c:	a0 1d 00 18 	lhz     r0,24(r29)
     250:	81 3d 00 38 	lwz     r9,56(r29)
     254:	7f 9f 00 00 	cmpw    cr7,r31,r0
     258:	40 9c 00 34 	bge-    cr7,28c <rq_flush_dcache_pages+0x60>
     25c:	57 e0 10 3a 	rlwinm  r0,r31,2,0,29
     260:	57 fe 20 36 	rlwinm  r30,r31,4,0,27
     264:	7f c0 f0 50 	subf    r30,r0,r30
 * @rq: the request to be flushed
 *
 * Description:
 *     Flush all pages in @rq.
 */
void rq_flush_dcache_pages(struct request *rq)
     268:	3b ff 00 01 	addi    r31,r31,1
{
	struct req_iterator iter;
	struct bio_vec *bvec;

	rq_for_each_segment(bvec, rq, iter)
     26c:	7f c9 f2 14 	add     r30,r9,r30
		flush_dcache_page(bvec->bv_page);
     270:	80 7e 00 00 	lwz     r3,0(r30)
void rq_flush_dcache_pages(struct request *rq)
{
	struct req_iterator iter;
	struct bio_vec *bvec;

	rq_for_each_segment(bvec, rq, iter)
     274:	3b de 00 0c 	addi    r30,r30,12
		flush_dcache_page(bvec->bv_page);
     278:	48 00 00 01 	bl      278 <rq_flush_dcache_pages+0x4c>
void rq_flush_dcache_pages(struct request *rq)
{
	struct req_iterator iter;
	struct bio_vec *bvec;

	rq_for_each_segment(bvec, rq, iter)
     27c:	a1 3d 00 18 	lhz     r9,24(r29)
     280:	7f 89 f8 00 	cmpw    cr7,r9,r31
     284:	3b ff 00 01 	addi    r31,r31,1
     288:	41 9d ff e8 	bgt+    cr7,270 <rq_flush_dcache_pages+0x44>
     28c:	83 bd 00 08 	lwz     r29,8(r29)
     290:	2f 9d 00 00 	cmpwi   cr7,r29,0
     294:	40 9e ff b4 	bne+    cr7,248 <rq_flush_dcache_pages+0x1c>
		flush_dcache_page(bvec->bv_page);
}
     298:	80 01 00 24 	lwz     r0,36(r1)
     29c:	bb a1 00 14 	lmw     r29,20(r1)
     2a0:	38 21 00 20 	addi    r1,r1,32
     2a4:	7c 08 03 a6 	mtlr    r0
     2a8:	4e 80 00 20 	blr

000002ac <blk_add_request_payload>:
 * Note that this is a quite horrible hack and nothing but handling of
 * discard requests should ever use it.
 */
void blk_add_request_payload(struct request *rq, struct page *page,
		unsigned int len)
{
     2ac:	94 21 ff f0 	stwu    r1,-16(r1)
     2b0:	7c 08 02 a6 	mflr    r0
	struct bio *bio = rq->bio;

	bio->bi_io_vec->bv_page = page;
	bio->bi_io_vec->bv_offset = 0;
     2b4:	39 40 00 00 	li      r10,0
 * Note that this is a quite horrible hack and nothing but handling of
 * discard requests should ever use it.
 */
void blk_add_request_payload(struct request *rq, struct page *page,
		unsigned int len)
{
     2b8:	90 01 00 14 	stw     r0,20(r1)
static inline void *bio_data(struct bio *bio)
{
	if (bio->bi_vcnt)
		return page_address(bio_page(bio)) + bio_offset(bio);

	return NULL;
     2bc:	38 00 00 00 	li      r0,0
     2c0:	bf c1 00 08 	stmw    r30,8(r1)
     2c4:	7c 7e 1b 78 	mr      r30,r3
	struct bio *bio = rq->bio;
     2c8:	83 e3 00 40 	lwz     r31,64(r3)

	bio->bi_io_vec->bv_page = page;
     2cc:	81 3f 00 38 	lwz     r9,56(r31)
     2d0:	90 89 00 00 	stw     r4,0(r9)
	bio->bi_io_vec->bv_offset = 0;
	bio->bi_io_vec->bv_len = len;

	bio->bi_size = len;
	bio->bi_vcnt = 1;
	bio->bi_phys_segments = 1;
     2d4:	39 20 00 01 	li      r9,1
		unsigned int len)
{
	struct bio *bio = rq->bio;

	bio->bi_io_vec->bv_page = page;
	bio->bi_io_vec->bv_offset = 0;
     2d8:	81 7f 00 38 	lwz     r11,56(r31)
     2dc:	91 4b 00 08 	stw     r10,8(r11)
	bio->bi_io_vec->bv_len = len;
     2e0:	81 7f 00 38 	lwz     r11,56(r31)
     2e4:	90 ab 00 04 	stw     r5,4(r11)

	bio->bi_size = len;
	bio->bi_vcnt = 1;
     2e8:	39 60 00 01 	li      r11,1
     2ec:	b1 7f 00 18 	sth     r11,24(r31)
	bio->bi_phys_segments = 1;
     2f0:	91 3f 00 1c 	stw     r9,28(r31)

	bio->bi_io_vec->bv_page = page;
	bio->bi_io_vec->bv_offset = 0;
	bio->bi_io_vec->bv_len = len;

	bio->bi_size = len;
     2f4:	90 bf 00 20 	stw     r5,32(r31)
	bio->bi_vcnt = 1;
	bio->bi_phys_segments = 1;

	rq->__data_len = rq->resid_len = len;
     2f8:	90 a3 00 ac 	stw     r5,172(r3)
     2fc:	90 a3 00 30 	stw     r5,48(r3)
	rq->nr_phys_segments = 1;
     300:	b1 23 00 74 	sth     r9,116(r3)
		return bio->bi_size;
}

static inline void *bio_data(struct bio *bio)
{
	if (bio->bi_vcnt)
     304:	a1 3f 00 18 	lhz     r9,24(r31)
     308:	2f 89 00 00 	cmpwi   cr7,r9,0
     30c:	41 be 00 40 	beq+    cr7,34c <blk_add_request_payload+0xa0>
		return page_address(bio_page(bio)) + bio_offset(bio);
     310:	a0 1f 00 1a 	lhz     r0,26(r31)
     314:	81 3f 00 38 	lwz     r9,56(r31)
     318:	54 0b 10 3a 	rlwinm  r11,r0,2,0,29
     31c:	54 00 20 36 	rlwinm  r0,r0,4,0,27
     320:	7c 0b 00 50 	subf    r0,r11,r0
     324:	7c 69 00 2e 	lwzx    r3,r9,r0
     328:	48 00 00 01 	bl      328 <blk_add_request_payload+0x7c>
     32c:	a0 1f 00 1a 	lhz     r0,26(r31)
     330:	81 3f 00 38 	lwz     r9,56(r31)
     334:	54 0b 10 3a 	rlwinm  r11,r0,2,0,29
     338:	54 00 20 36 	rlwinm  r0,r0,4,0,27
     33c:	7c 0b 00 50 	subf    r0,r11,r0
     340:	7d 29 02 14 	add     r9,r9,r0
     344:	80 09 00 08 	lwz     r0,8(r9)
     348:	7c 03 02 14 	add     r0,r3,r0
	rq->buffer = bio_data(bio);
     34c:	90 1e 00 80 	stw     r0,128(r30)
}
     350:	80 01 00 14 	lwz     r0,20(r1)
     354:	bb c1 00 08 	lmw     r30,8(r1)
     358:	38 21 00 10 	addi    r1,r1,16
     35c:	7c 08 03 a6 	mtlr    r0
     360:	4e 80 00 20 	blr

00000364 <part_round_stats>:
 */
void part_round_stats(int cpu, struct hd_struct *part)
{
	unsigned long now = jiffies;

	if (part->partno)
     364:	80 04 00 e0 	lwz     r0,224(r4)
 * /proc/diskstats.  This accounts immediately for all queue usage up to
 * the current jiffies and restarts the counters again.
 */
void part_round_stats(int cpu, struct hd_struct *part)
{
	unsigned long now = jiffies;
     368:	3d 20 00 00 	lis     r9,0

	if (part->partno)
     36c:	2f 80 00 00 	cmpwi   cr7,r0,0
 * /proc/diskstats.  This accounts immediately for all queue usage up to
 * the current jiffies and restarts the counters again.
 */
void part_round_stats(int cpu, struct hd_struct *part)
{
	unsigned long now = jiffies;
     370:	80 09 00 00 	lwz     r0,0(r9)

	if (part->partno)
     374:	41 9e 00 58 	beq-    cr7,3cc <part_round_stats+0x68>
	int node_id;
};

static inline struct gendisk *part_to_disk(struct hd_struct *part)
{
	if (likely(part)) {
     378:	2f 84 00 00 	cmpwi   cr7,r4,0
		if (part->partno)
			return dev_to_disk(part_to_dev(part)->parent);
		else
			return dev_to_disk(part_to_dev(part));
	}
	return NULL;
     37c:	39 20 00 00 	li      r9,0
	int node_id;
};

static inline struct gendisk *part_to_disk(struct hd_struct *part)
{
	if (likely(part)) {
     380:	41 9e 00 0c 	beq-    cr7,38c <part_round_stats+0x28>
		if (part->partno)
			return dev_to_disk(part_to_dev(part)->parent);
     384:	81 24 00 20 	lwz     r9,32(r4)
     388:	39 29 ff a0 	addi    r9,r9,-96
EXPORT_SYMBOL(blk_insert_request);

static void part_round_stats_single(int cpu, struct hd_struct *part,
				    unsigned long now)
{
	if (now == part->stamp)
     38c:	81 69 01 28 	lwz     r11,296(r9)
     390:	7f 80 58 00 	cmpw    cr7,r0,r11
     394:	41 9e 00 38 	beq-    cr7,3cc <part_round_stats+0x68>
		part_to_disk(part)->part0.in_flight[rw]--;
}

static inline int part_in_flight(struct hd_struct *part)
{
	return part->in_flight[0] + part->in_flight[1];
     398:	81 09 01 2c 	lwz     r8,300(r9)
     39c:	81 49 01 30 	lwz     r10,304(r9)
		return;

	if (part_in_flight(part)) {
     3a0:	7d 48 52 15 	add.    r10,r8,r10
     3a4:	41 82 00 24 	beq-    3c8 <part_round_stats+0x64>
		__part_stat_add(cpu, part, time_in_queue,
     3a8:	7d 6b 00 50 	subf    r11,r11,r0
     3ac:	81 09 01 58 	lwz     r8,344(r9)
     3b0:	7d 4a 59 d6 	mullw   r10,r10,r11
				part_in_flight(part) * (now - part->stamp));
		__part_stat_add(cpu, part, io_ticks, (now - part->stamp));
     3b4:	80 e9 01 54 	lwz     r7,340(r9)
     3b8:	7d 67 5a 14 	add     r11,r7,r11
     3bc:	91 69 01 54 	stw     r11,340(r9)
{
	if (now == part->stamp)
		return;

	if (part_in_flight(part)) {
		__part_stat_add(cpu, part, time_in_queue,
     3c0:	7d 48 52 14 	add     r10,r8,r10
     3c4:	91 49 01 58 	stw     r10,344(r9)
				part_in_flight(part) * (now - part->stamp));
		__part_stat_add(cpu, part, io_ticks, (now - part->stamp));
	}
	part->stamp = now;
     3c8:	90 09 01 28 	stw     r0,296(r9)
EXPORT_SYMBOL(blk_insert_request);

static void part_round_stats_single(int cpu, struct hd_struct *part,
				    unsigned long now)
{
	if (now == part->stamp)
     3cc:	81 24 00 e8 	lwz     r9,232(r4)
     3d0:	7f 80 48 00 	cmpw    cr7,r0,r9
     3d4:	4d 9e 00 20 	beqlr   cr7
     3d8:	81 44 00 ec 	lwz     r10,236(r4)
     3dc:	81 64 00 f0 	lwz     r11,240(r4)
		return;

	if (part_in_flight(part)) {
     3e0:	7d 6a 5a 15 	add.    r11,r10,r11
     3e4:	41 82 00 24 	beq-    408 <part_round_stats+0xa4>
		__part_stat_add(cpu, part, time_in_queue,
     3e8:	7d 29 00 50 	subf    r9,r9,r0
     3ec:	81 44 01 18 	lwz     r10,280(r4)
     3f0:	7d 6b 49 d6 	mullw   r11,r11,r9
				part_in_flight(part) * (now - part->stamp));
		__part_stat_add(cpu, part, io_ticks, (now - part->stamp));
     3f4:	81 04 01 14 	lwz     r8,276(r4)
     3f8:	7d 28 4a 14 	add     r9,r8,r9
     3fc:	91 24 01 14 	stw     r9,276(r4)
{
	if (now == part->stamp)
		return;

	if (part_in_flight(part)) {
		__part_stat_add(cpu, part, time_in_queue,
     400:	7d 6a 5a 14 	add     r11,r10,r11
     404:	91 64 01 18 	stw     r11,280(r4)
				part_in_flight(part) * (now - part->stamp));
		__part_stat_add(cpu, part, io_ticks, (now - part->stamp));
	}
	part->stamp = now;
     408:	90 04 00 e8 	stw     r0,232(r4)
     40c:	4e 80 00 20 	blr

00000410 <blk_dump_rq_flags>:
			q->flush_err = error;
	}
}

void blk_dump_rq_flags(struct request *rq, char *msg)
{
     410:	94 21 ff e0 	stwu    r1,-32(r1)
     414:	7c 08 02 a6 	mflr    r0
     418:	bf a1 00 14 	stmw    r29,20(r1)
     41c:	7c 7f 1b 78 	mr      r31,r3
     420:	90 01 00 24 	stw     r0,36(r1)
	int bit;

	printk(KERN_INFO "%s: dev %s: type=%x, flags=%x\n", msg,
		rq->rq_disk ? rq->rq_disk->disk_name : "?", rq->cmd_type,
     424:	80 a3 00 68 	lwz     r5,104(r3)

void blk_dump_rq_flags(struct request *rq, char *msg)
{
	int bit;

	printk(KERN_INFO "%s: dev %s: type=%x, flags=%x\n", msg,
     428:	2f 85 00 00 	cmpwi   cr7,r5,0
     42c:	41 9e 00 bc 	beq-    cr7,4e8 <blk_dump_rq_flags+0xd8>
     430:	38 a5 00 0c 	addi    r5,r5,12
     434:	80 df 00 24 	lwz     r6,36(r31)
     438:	3c 60 00 00 	lis     r3,0
     43c:	80 ff 00 20 	lwz     r7,32(r31)
     440:	38 63 00 18 	addi    r3,r3,24
     444:	48 00 00 01 	bl      444 <blk_dump_rq_flags+0x34>

	blk_requestq_cachep = kmem_cache_create("blkdev_queue",
			sizeof(struct request_queue), 0, SLAB_PANIC, NULL);

	return 0;
}
     448:	81 3f 00 40 	lwz     r9,64(r31)
	return rq->__data_len;
}

static inline int blk_rq_cur_bytes(const struct request *rq)
{
	return rq->bio ? bio_cur_bytes(rq->bio) : 0;
     44c:	39 00 00 00 	li      r8,0

extern unsigned int blk_rq_err_bytes(const struct request *rq);

static inline unsigned int blk_rq_sectors(const struct request *rq)
{
	return blk_rq_bytes(rq) >> 9;
     450:	80 ff 00 30 	lwz     r7,48(r31)
	return rq->__data_len;
}

static inline int blk_rq_cur_bytes(const struct request *rq)
{
	return rq->bio ? bio_cur_bytes(rq->bio) : 0;
     454:	2f 89 00 00 	cmpwi   cr7,r9,0
     458:	80 bf 00 38 	lwz     r5,56(r31)
     45c:	80 df 00 3c 	lwz     r6,60(r31)

extern unsigned int blk_rq_err_bytes(const struct request *rq);

static inline unsigned int blk_rq_sectors(const struct request *rq)
{
	return blk_rq_bytes(rq) >> 9;
     460:	54 e7 ba 7e 	rlwinm  r7,r7,23,9,31
	return rq->__data_len;
}

static inline int blk_rq_cur_bytes(const struct request *rq)
{
	return rq->bio ? bio_cur_bytes(rq->bio) : 0;
     464:	41 9e 00 30 	beq-    cr7,494 <blk_dump_rq_flags+0x84>
#define bio_segments(bio)	((bio)->bi_vcnt - (bio)->bi_idx)
#define bio_sectors(bio)	((bio)->bi_size >> 9)

static inline unsigned int bio_cur_bytes(struct bio *bio)
{
	if (bio->bi_vcnt)
     468:	a0 09 00 18 	lhz     r0,24(r9)
     46c:	2f 80 00 00 	cmpwi   cr7,r0,0
     470:	41 9e 00 6c 	beq-    cr7,4dc <blk_dump_rq_flags+0xcc>
		return bio_iovec(bio)->bv_len;
     474:	a0 09 00 1a 	lhz     r0,26(r9)
     478:	81 29 00 38 	lwz     r9,56(r9)
     47c:	54 0b 10 3a 	rlwinm  r11,r0,2,0,29
     480:	54 00 20 36 	rlwinm  r0,r0,4,0,27
     484:	7c 0b 00 50 	subf    r0,r11,r0
     488:	7d 29 02 14 	add     r9,r9,r0
     48c:	81 09 00 04 	lwz     r8,4(r9)
	else /* dataless requests such as discard */
		return bio->bi_size;
     490:	7d 08 4e 70 	srawi   r8,r8,9

	printk(KERN_INFO "%s: dev %s: type=%x, flags=%x\n", msg,
		rq->rq_disk ? rq->rq_disk->disk_name : "?", rq->cmd_type,
		rq->cmd_flags);

	printk(KERN_INFO "  sector %llu, nr/cnr %u/%u\n",
     494:	3c 60 00 00 	lis     r3,0
     498:	38 63 00 3c 	addi    r3,r3,60
     49c:	48 00 00 01 	bl      49c <blk_dump_rq_flags+0x8c>
	       (unsigned long long)blk_rq_pos(rq),
	       blk_rq_sectors(rq), blk_rq_cur_sectors(rq));
	printk(KERN_INFO "  bio %p, biotail %p, buffer %p, len %u\n",
     4a0:	80 9f 00 40 	lwz     r4,64(r31)
     4a4:	3c 60 00 00 	lis     r3,0
     4a8:	80 bf 00 44 	lwz     r5,68(r31)
     4ac:	38 63 00 5c 	addi    r3,r3,92
     4b0:	80 df 00 80 	lwz     r6,128(r31)
     4b4:	80 ff 00 30 	lwz     r7,48(r31)
     4b8:	48 00 00 01 	bl      4b8 <blk_dump_rq_flags+0xa8>
	       rq->bio, rq->biotail, rq->buffer, blk_rq_bytes(rq));

	if (rq->cmd_type == REQ_TYPE_BLOCK_PC) {
     4bc:	80 1f 00 24 	lwz     r0,36(r31)
     4c0:	2f 80 00 02 	cmpwi   cr7,r0,2
     4c4:	41 9e 00 30 	beq-    cr7,4f4 <blk_dump_rq_flags+0xe4>
		printk(KERN_INFO "  cdb: ");
		for (bit = 0; bit < BLK_MAX_CDB; bit++)
			printk("%02x ", rq->cmd[bit]);
		printk("\n");
	}
}
     4c8:	80 01 00 24 	lwz     r0,36(r1)
     4cc:	bb a1 00 14 	lmw     r29,20(r1)
     4d0:	38 21 00 20 	addi    r1,r1,32
     4d4:	7c 08 03 a6 	mtlr    r0
     4d8:	4e 80 00 20 	blr
     4dc:	81 09 00 20 	lwz     r8,32(r9)
     4e0:	7d 08 4e 70 	srawi   r8,r8,9
     4e4:	4b ff ff b0 	b       494 <blk_dump_rq_flags+0x84>

void blk_dump_rq_flags(struct request *rq, char *msg)
{
	int bit;

	printk(KERN_INFO "%s: dev %s: type=%x, flags=%x\n", msg,
     4e8:	3c a0 00 00 	lis     r5,0
     4ec:	38 a5 00 14 	addi    r5,r5,20
     4f0:	4b ff ff 44 	b       434 <blk_dump_rq_flags+0x24>
	       blk_rq_sectors(rq), blk_rq_cur_sectors(rq));
	printk(KERN_INFO "  bio %p, biotail %p, buffer %p, len %u\n",
	       rq->bio, rq->biotail, rq->buffer, blk_rq_bytes(rq));

	if (rq->cmd_type == REQ_TYPE_BLOCK_PC) {
		printk(KERN_INFO "  cdb: ");
     4f4:	3c 60 00 00 	lis     r3,0
     4f8:	3f a0 00 00 	lis     r29,0
     4fc:	38 63 00 88 	addi    r3,r3,136
		for (bit = 0; bit < BLK_MAX_CDB; bit++)
     500:	3b c0 00 00 	li      r30,0
	       blk_rq_sectors(rq), blk_rq_cur_sectors(rq));
	printk(KERN_INFO "  bio %p, biotail %p, buffer %p, len %u\n",
	       rq->bio, rq->biotail, rq->buffer, blk_rq_bytes(rq));

	if (rq->cmd_type == REQ_TYPE_BLOCK_PC) {
		printk(KERN_INFO "  cdb: ");
     504:	48 00 00 01 	bl      504 <blk_dump_rq_flags+0xf4>
     508:	3b bd 00 94 	addi    r29,r29,148
		for (bit = 0; bit < BLK_MAX_CDB; bit++)
			printk("%02x ", rq->cmd[bit]);
     50c:	81 3f 00 9c 	lwz     r9,156(r31)
     510:	7f a3 eb 78 	mr      r3,r29
     514:	7c 89 f0 ae 	lbzx    r4,r9,r30
     518:	48 00 00 01 	bl      518 <blk_dump_rq_flags+0x108>
	printk(KERN_INFO "  bio %p, biotail %p, buffer %p, len %u\n",
	       rq->bio, rq->biotail, rq->buffer, blk_rq_bytes(rq));

	if (rq->cmd_type == REQ_TYPE_BLOCK_PC) {
		printk(KERN_INFO "  cdb: ");
		for (bit = 0; bit < BLK_MAX_CDB; bit++)
     51c:	2f 9e 00 0f 	cmpwi   cr7,r30,15
     520:	3b de 00 01 	addi    r30,r30,1
     524:	40 9e ff e8 	bne+    cr7,50c <blk_dump_rq_flags+0xfc>
			printk("%02x ", rq->cmd[bit]);
		printk("\n");
     528:	3c 60 00 00 	lis     r3,0
     52c:	38 63 00 9c 	addi    r3,r3,156
     530:	48 00 00 01 	bl      530 <blk_dump_rq_flags+0x120>
     534:	4b ff ff 94 	b       4c8 <blk_dump_rq_flags+0xb8>

00000538 <req_bio_endio>:
}
EXPORT_SYMBOL(blk_rq_init);

static void req_bio_endio(struct request *rq, struct bio *bio,
			  unsigned int nbytes, int error)
{
     538:	94 21 ff e0 	stwu    r1,-32(r1)
     53c:	7c 08 02 a6 	mflr    r0
     540:	bf a1 00 14 	stmw    r29,20(r1)
     544:	7c 7e 1b 78 	mr      r30,r3
     548:	7c 9f 23 78 	mr      r31,r4
     54c:	90 01 00 24 	stw     r0,36(r1)
     550:	7c dd 33 78 	mr      r29,r6
	struct request_queue *q = rq->q;
     554:	81 23 00 1c 	lwz     r9,28(r3)

	if (&q->flush_rq != rq) {
     558:	38 09 02 58 	addi    r0,r9,600
     55c:	7f 83 00 00 	cmpw    cr7,r3,r0
     560:	41 9e 00 b4 	beq-    cr7,614 <req_bio_endio+0xdc>
		if (error)
     564:	2f 86 00 00 	cmpwi   cr7,r6,0
     568:	40 9e 00 70 	bne-    cr7,5d8 <req_bio_endio+0xa0>
 * @nr: bit number to test
 * @addr: Address to start counting from
 */
static inline int test_bit(int nr, const volatile unsigned long *addr)
{
	return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
     56c:	80 04 00 10 	lwz     r0,16(r4)
			clear_bit(BIO_UPTODATE, &bio->bi_flags);
		else if (!test_bit(BIO_UPTODATE, &bio->bi_flags))
     570:	70 09 00 01 	andi.   r9,r0,1
     574:	40 82 00 08 	bne-    57c <req_bio_endio+0x44>
			error = -EIO;
     578:	3b a0 ff fb 	li      r29,-5

		if (unlikely(nbytes > bio->bi_size)) {
     57c:	80 df 00 20 	lwz     r6,32(r31)
     580:	7f 86 28 40 	cmplw   cr7,r6,r5
     584:	41 9c 00 cc 	blt-    cr7,650 <req_bio_endio+0x118>
			printk(KERN_ERR "%s: want %u bytes done, %u left\n",
			       __func__, nbytes, bio->bi_size);
			nbytes = bio->bi_size;
		}

		if (unlikely(rq->cmd_flags & REQ_QUIET))
     588:	80 1e 00 20 	lwz     r0,32(r30)
     58c:	74 09 00 10 	andis.  r9,r0,16
     590:	40 82 00 a0 	bne-    630 <req_bio_endio+0xf8>
			set_bit(BIO_QUIET, &bio->bi_flags);

		bio->bi_size -= nbytes;
     594:	7c c5 30 50 	subf    r6,r5,r6
		bio->bi_sector += (nbytes >> 9);
     598:	81 1f 00 00 	lwz     r8,0(r31)
     59c:	54 ab ba 7e 	rlwinm  r11,r5,23,9,31

		if (bio_integrity(bio))
			bio_integrity_advance(bio, nbytes);

		if (bio->bi_size == 0)
     5a0:	2f 86 00 00 	cmpwi   cr7,r6,0

		if (unlikely(rq->cmd_flags & REQ_QUIET))
			set_bit(BIO_QUIET, &bio->bi_flags);

		bio->bi_size -= nbytes;
		bio->bi_sector += (nbytes >> 9);
     5a4:	81 3f 00 04 	lwz     r9,4(r31)
     5a8:	39 40 00 00 	li      r10,0
		}

		if (unlikely(rq->cmd_flags & REQ_QUIET))
			set_bit(BIO_QUIET, &bio->bi_flags);

		bio->bi_size -= nbytes;
     5ac:	90 df 00 20 	stw     r6,32(r31)
		bio->bi_sector += (nbytes >> 9);
     5b0:	7d 6b 48 14 	addc    r11,r11,r9
     5b4:	7d 4a 41 14 	adde    r10,r10,r8
     5b8:	91 5f 00 00 	stw     r10,0(r31)
     5bc:	91 7f 00 04 	stw     r11,4(r31)

		if (bio_integrity(bio))
			bio_integrity_advance(bio, nbytes);

		if (bio->bi_size == 0)
     5c0:	41 9e 00 34 	beq-    cr7,5f4 <req_bio_endio+0xbc>
		 * progress, just record the error;
		 */
		if (error && !q->flush_err)
			q->flush_err = error;
	}
}
     5c4:	80 01 00 24 	lwz     r0,36(r1)
     5c8:	bb a1 00 14 	lmw     r29,20(r1)
     5cc:	38 21 00 20 	addi    r1,r1,32
     5d0:	7c 08 03 a6 	mtlr    r0
     5d4:	4e 80 00 20 	blr
	set_bits(BITOP_MASK(nr), addr + BITOP_WORD(nr));
}

static __inline__ void clear_bit(int nr, volatile unsigned long *addr)
{
	clear_bits(BITOP_MASK(nr), addr + BITOP_WORD(nr));
     5d8:	39 24 00 10 	addi    r9,r4,16
	: "r" (mask), "r" (p)			\
	: "cc", "memory");			\
}

DEFINE_BITOP(set_bits, or, "", "")
DEFINE_BITOP(clear_bits, andc, "", "")
     5dc:	38 00 00 01 	li      r0,1
     5e0:	7d 60 48 28 	lwarx   r11,0,r9
     5e4:	7d 6b 00 78 	andc    r11,r11,r0
     5e8:	7d 60 49 2d 	stwcx.  r11,0,r9
     5ec:	40 a2 ff f4 	bne-    5e0 <req_bio_endio+0xa8>
     5f0:	4b ff ff 8c 	b       57c <req_bio_endio+0x44>

		if (bio_integrity(bio))
			bio_integrity_advance(bio, nbytes);

		if (bio->bi_size == 0)
			bio_endio(bio, error);
     5f4:	7f e3 fb 78 	mr      r3,r31
     5f8:	7f a4 eb 78 	mr      r4,r29
     5fc:	48 00 00 01 	bl      5fc <req_bio_endio+0xc4>
		 * progress, just record the error;
		 */
		if (error && !q->flush_err)
			q->flush_err = error;
	}
}
     600:	80 01 00 24 	lwz     r0,36(r1)
     604:	bb a1 00 14 	lmw     r29,20(r1)
     608:	38 21 00 20 	addi    r1,r1,32
     60c:	7c 08 03 a6 	mtlr    r0
     610:	4e 80 00 20 	blr
	} else {
		/*
		 * Okay, this is the sequenced flush request in
		 * progress, just record the error;
		 */
		if (error && !q->flush_err)
     614:	2f 86 00 00 	cmpwi   cr7,r6,0
     618:	41 be ff ac 	beq-    cr7,5c4 <req_bio_endio+0x8c>
     61c:	80 09 02 54 	lwz     r0,596(r9)
     620:	2f 80 00 00 	cmpwi   cr7,r0,0
     624:	40 be ff a0 	bne-    cr7,5c4 <req_bio_endio+0x8c>
			q->flush_err = error;
     628:	90 c9 02 54 	stw     r6,596(r9)
     62c:	4b ff ff 98 	b       5c4 <req_bio_endio+0x8c>
DEFINE_BITOP(clear_bits_unlock, andc, PPC_RELEASE_BARRIER, "")
DEFINE_BITOP(change_bits, xor, "", "")

static __inline__ void set_bit(int nr, volatile unsigned long *addr)
{
	set_bits(BITOP_MASK(nr), addr + BITOP_WORD(nr));
     630:	39 3f 00 10 	addi    r9,r31,16
	: "=&r" (old), "+m" (*p)		\
	: "r" (mask), "r" (p)			\
	: "cc", "memory");			\
}

DEFINE_BITOP(set_bits, or, "", "")
     634:	38 00 08 00 	li      r0,2048
     638:	7d 60 48 28 	lwarx   r11,0,r9
     63c:	7d 6b 03 78 	or      r11,r11,r0
     640:	7d 60 49 2d 	stwcx.  r11,0,r9
     644:	40 a2 ff f4 	bne-    638 <req_bio_endio+0x100>
     648:	80 df 00 20 	lwz     r6,32(r31)
     64c:	4b ff ff 48 	b       594 <req_bio_endio+0x5c>
			clear_bit(BIO_UPTODATE, &bio->bi_flags);
		else if (!test_bit(BIO_UPTODATE, &bio->bi_flags))
			error = -EIO;

		if (unlikely(nbytes > bio->bi_size)) {
			printk(KERN_ERR "%s: want %u bytes done, %u left\n",
     650:	3c 60 00 00 	lis     r3,0
     654:	3c 80 00 00 	lis     r4,0
     658:	38 63 00 a0 	addi    r3,r3,160
     65c:	38 84 00 00 	addi    r4,r4,0
     660:	48 00 00 01 	bl      660 <req_bio_endio+0x128>
			       __func__, nbytes, bio->bi_size);
			nbytes = bio->bi_size;
     664:	80 bf 00 20 	lwz     r5,32(r31)
     668:	7c a6 2b 78 	mr      r6,r5
     66c:	4b ff ff 1c 	b       588 <req_bio_endio+0x50>

00000670 <blk_update_request>:
 * Return:
 *     %false - this request doesn't have any more data
 *     %true  - this request has more data
 **/
bool blk_update_request(struct request *req, int error, unsigned int nr_bytes)
{
     670:	94 21 ff d0 	stwu    r1,-48(r1)
     674:	7c 08 02 a6 	mflr    r0
     678:	bf 01 00 10 	stmw    r24,16(r1)
     67c:	7c 7b 1b 78 	mr      r27,r3
     680:	7c 99 23 78 	mr      r25,r4
     684:	90 01 00 34 	stw     r0,52(r1)
     688:	7c bf 2b 78 	mr      r31,r5
	int total_bytes, bio_nbytes, next_idx = 0;
	struct bio *bio;

	if (!req->bio)
     68c:	80 03 00 40 	lwz     r0,64(r3)
		return false;
     690:	38 60 00 00 	li      r3,0
bool blk_update_request(struct request *req, int error, unsigned int nr_bytes)
{
	int total_bytes, bio_nbytes, next_idx = 0;
	struct bio *bio;

	if (!req->bio)
     694:	2f 80 00 00 	cmpwi   cr7,r0,0
     698:	41 9e 01 00 	beq-    cr7,798 <blk_update_request+0x128>
	 * Reset per-request error on each partial completion.
	 *
	 * TODO: tj: This is too subtle.  It would be better to let
	 * low level drivers do what they see fit.
	 */
	if (req->cmd_type == REQ_TYPE_FS)
     69c:	80 1b 00 24 	lwz     r0,36(r27)
     6a0:	2f 80 00 01 	cmpwi   cr7,r0,1
     6a4:	41 9e 02 e4 	beq-    cr7,988 <blk_update_request+0x318>
		req->errors = 0;

	if (error && req->cmd_type == REQ_TYPE_FS &&
     6a8:	2f 19 00 00 	cmpwi   cr6,r25,0
     6ac:	40 9a 02 0c 	bne-    cr6,8b8 <blk_update_request+0x248>
 *	b) the queue had IO stats enabled when this request was started, and
 *	c) it's a file system request or a discard request
 */
static inline int blk_do_io_stat(struct request *rq)
{
	return rq->rq_disk &&
     6b0:	80 1b 00 68 	lwz     r0,104(r27)
     6b4:	2f 80 00 00 	cmpwi   cr7,r0,0
     6b8:	41 9e 00 10 	beq-    cr7,6c8 <blk_update_request+0x58>
	       (rq->cmd_flags & REQ_IO_STAT) &&
     6bc:	80 1b 00 20 	lwz     r0,32(r27)
 *	b) the queue had IO stats enabled when this request was started, and
 *	c) it's a file system request or a discard request
 */
static inline int blk_do_io_stat(struct request *rq)
{
	return rq->rq_disk &&
     6c0:	74 09 02 00 	andis.  r9,r0,512
     6c4:	40 82 02 48 	bne-    90c <blk_update_request+0x29c>
		struct hd_struct *part;
		int cpu;

		cpu = part_stat_lock();
		part = req->part;
		part_stat_add(cpu, part, sectors[rw], bytes >> 9);
     6c8:	83 1b 00 40 	lwz     r24,64(r27)
		if (nr_bytes >= bio->bi_size) {
			req->bio = bio->bi_next;
			nbytes = bio->bi_size;
			req_bio_endio(req, bio, nbytes, error);
			next_idx = 0;
			bio_nbytes = 0;
     6cc:	3b c0 00 00 	li      r30,0
     6d0:	3b 80 00 00 	li      r28,0
     6d4:	3b a0 00 00 	li      r29,0
		struct hd_struct *part;
		int cpu;

		cpu = part_stat_lock();
		part = req->part;
		part_stat_add(cpu, part, sectors[rw], bytes >> 9);
     6d8:	7f 1a c3 78 	mr      r26,r24
     6dc:	48 00 00 64 	b       740 <blk_update_request+0xd0>
			nbytes = bio->bi_size;
			req_bio_endio(req, bio, nbytes, error);
			next_idx = 0;
			bio_nbytes = 0;
		} else {
			int idx = bio->bi_idx + next_idx;
     6e0:	a0 b8 00 1a 	lhz     r5,26(r24)

			if (unlikely(idx >= bio->bi_vcnt)) {
     6e4:	a0 18 00 18 	lhz     r0,24(r24)
			nbytes = bio->bi_size;
			req_bio_endio(req, bio, nbytes, error);
			next_idx = 0;
			bio_nbytes = 0;
		} else {
			int idx = bio->bi_idx + next_idx;
     6e8:	7c a5 f2 14 	add     r5,r5,r30

			if (unlikely(idx >= bio->bi_vcnt)) {
     6ec:	7f 85 00 00 	cmpw    cr7,r5,r0
				printk(KERN_ERR "%s: bio idx %d >= vcnt %d\n",
				       __func__, idx, bio->bi_vcnt);
				break;
			}

			nbytes = bio_iovec_idx(bio, idx)->bv_len;
     6f0:	54 ab 10 3a 	rlwinm  r11,r5,2,0,29
     6f4:	54 aa 20 36 	rlwinm  r10,r5,4,0,27
     6f8:	7d 6b 50 50 	subf    r11,r11,r10
			next_idx = 0;
			bio_nbytes = 0;
		} else {
			int idx = bio->bi_idx + next_idx;

			if (unlikely(idx >= bio->bi_vcnt)) {
     6fc:	40 9c 00 bc 	bge-    cr7,7b8 <blk_update_request+0x148>
				printk(KERN_ERR "%s: bio idx %d >= vcnt %d\n",
				       __func__, idx, bio->bi_vcnt);
				break;
			}

			nbytes = bio_iovec_idx(bio, idx)->bv_len;
     700:	80 18 00 38 	lwz     r0,56(r24)
     704:	7d 60 5a 14 	add     r11,r0,r11
     708:	80 0b 00 04 	lwz     r0,4(r11)
			BIO_BUG_ON(nbytes > bio->bi_size);
     70c:	7d 20 48 10 	subfc   r9,r0,r9
     710:	7d 29 49 10 	subfe   r9,r9,r9
     714:	7d 29 00 d0 	neg     r9,r9
     718:	0f 09 00 00 	twnei   r9,0

			/*
			 * not a complete bvec done
			 */
			if (unlikely(nbytes > nr_bytes)) {
     71c:	7f 9f 00 40 	cmplw   cr7,r31,r0
     720:	41 9c 03 28 	blt-    cr7,a48 <blk_update_request+0x3d8>
			}

			/*
			 * advance to the next vector
			 */
			next_idx++;
     724:	3b de 00 01 	addi    r30,r30,1
			bio_nbytes += nbytes;
     728:	7f 9c 02 14 	add     r28,r28,r0
		}

		total_bytes += nbytes;
     72c:	7f bd 02 14 	add     r29,r29,r0
		nr_bytes -= nbytes;
     730:	7f e0 f8 50 	subf    r31,r0,r31
		bio = req->bio;
		if (bio) {
			/*
			 * end more in this run, or just return 'not-done'
			 */
			if (unlikely(nr_bytes <= 0))
     734:	2f 9f 00 00 	cmpwi   cr7,r31,0
     738:	41 9e 00 c0 	beq-    cr7,7f8 <blk_update_request+0x188>
     73c:	7f 1a c3 78 	mr      r26,r24
	}

	blk_account_io_completion(req, nr_bytes);

	total_bytes = bio_nbytes = 0;
	while ((bio = req->bio) != NULL) {
     740:	2f 98 00 00 	cmpwi   cr7,r24,0
     744:	41 9e 00 48 	beq-    cr7,78c <blk_update_request+0x11c>
		int nbytes;

		if (nr_bytes >= bio->bi_size) {
     748:	81 38 00 20 	lwz     r9,32(r24)
     74c:	7f 9f 48 40 	cmplw   cr7,r31,r9
     750:	41 9c ff 90 	blt+    cr7,6e0 <blk_update_request+0x70>
			req->bio = bio->bi_next;
     754:	80 18 00 08 	lwz     r0,8(r24)
			nbytes = bio->bi_size;
			req_bio_endio(req, bio, nbytes, error);
     758:	7f 04 c3 78 	mr      r4,r24
     75c:	7f 63 db 78 	mr      r3,r27
     760:	7f 26 cb 78 	mr      r6,r25
	total_bytes = bio_nbytes = 0;
	while ((bio = req->bio) != NULL) {
		int nbytes;

		if (nr_bytes >= bio->bi_size) {
			req->bio = bio->bi_next;
     764:	90 1b 00 40 	stw     r0,64(r27)
			nbytes = bio->bi_size;
     768:	83 d8 00 20 	lwz     r30,32(r24)
			req_bio_endio(req, bio, nbytes, error);
     76c:	7f c5 f3 78 	mr      r5,r30
			 */
			next_idx++;
			bio_nbytes += nbytes;
		}

		total_bytes += nbytes;
     770:	7f bd f2 14 	add     r29,r29,r30
		int nbytes;

		if (nr_bytes >= bio->bi_size) {
			req->bio = bio->bi_next;
			nbytes = bio->bi_size;
			req_bio_endio(req, bio, nbytes, error);
     774:	4b ff fd c5 	bl      538 <req_bio_endio>
		}

		total_bytes += nbytes;
		nr_bytes -= nbytes;

		bio = req->bio;
     778:	83 1b 00 40 	lwz     r24,64(r27)
			next_idx++;
			bio_nbytes += nbytes;
		}

		total_bytes += nbytes;
		nr_bytes -= nbytes;
     77c:	7f fe f8 50 	subf    r31,r30,r31

		bio = req->bio;
		if (bio) {
     780:	2f 98 00 00 	cmpwi   cr7,r24,0
		}

		total_bytes += nbytes;
		nr_bytes -= nbytes;

		bio = req->bio;
     784:	7f 1a c3 78 	mr      r26,r24
		if (bio) {
     788:	40 9e 00 24 	bne-    cr7,7ac <blk_update_request+0x13c>
		/*
		 * Reset counters so that the request stacking driver
		 * can find how many bytes remain in the request
		 * later.
		 */
		req->__data_len = 0;
     78c:	38 00 00 00 	li      r0,0
		return false;
     790:	38 60 00 00 	li      r3,0
		/*
		 * Reset counters so that the request stacking driver
		 * can find how many bytes remain in the request
		 * later.
		 */
		req->__data_len = 0;
     794:	90 1b 00 30 	stw     r0,48(r27)

	/* recalculate the number of segments */
	blk_recalc_rq_segments(req);

	return true;
}
     798:	80 01 00 34 	lwz     r0,52(r1)
     79c:	bb 01 00 10 	lmw     r24,16(r1)
     7a0:	38 21 00 30 	addi    r1,r1,48
     7a4:	7c 08 03 a6 	mtlr    r0
     7a8:	4e 80 00 20 	blr

		if (nr_bytes >= bio->bi_size) {
			req->bio = bio->bi_next;
			nbytes = bio->bi_size;
			req_bio_endio(req, bio, nbytes, error);
			next_idx = 0;
     7ac:	3b c0 00 00 	li      r30,0
			bio_nbytes = 0;
     7b0:	3b 80 00 00 	li      r28,0
     7b4:	4b ff ff 80 	b       734 <blk_update_request+0xc4>
		} else {
			int idx = bio->bi_idx + next_idx;

			if (unlikely(idx >= bio->bi_vcnt)) {
				blk_dump_rq_flags(req, "__end_that");
     7b8:	3c 80 00 00 	lis     r4,0
     7bc:	7f 63 db 78 	mr      r3,r27
     7c0:	90 a1 00 08 	stw     r5,8(r1)
     7c4:	38 84 00 f4 	addi    r4,r4,244
     7c8:	48 00 00 01 	bl      7c8 <blk_update_request+0x158>
				printk(KERN_ERR "%s: bio idx %d >= vcnt %d\n",
     7cc:	3c 80 00 00 	lis     r4,0
     7d0:	a0 d8 00 18 	lhz     r6,24(r24)
     7d4:	3c 60 00 00 	lis     r3,0
     7d8:	38 84 00 00 	addi    r4,r4,0
     7dc:	80 a1 00 08 	lwz     r5,8(r1)
     7e0:	38 63 01 00 	addi    r3,r3,256
     7e4:	38 84 00 10 	addi    r4,r4,16
     7e8:	48 00 00 01 	bl      7e8 <blk_update_request+0x178>
	}

	/*
	 * completely done
	 */
	if (!req->bio) {
     7ec:	83 1b 00 40 	lwz     r24,64(r27)
     7f0:	2f 98 00 00 	cmpwi   cr7,r24,0
     7f4:	41 be ff 98 	beq-    cr7,78c <blk_update_request+0x11c>
	}

	/*
	 * if the request wasn't completed, update state
	 */
	if (bio_nbytes) {
     7f8:	2f 9c 00 00 	cmpwi   cr7,r28,0
     7fc:	40 9e 01 dc 	bne-    cr7,9d8 <blk_update_request+0x368>
		bio->bi_idx += next_idx;
		bio_iovec(bio)->bv_offset += nr_bytes;
		bio_iovec(bio)->bv_len -= nr_bytes;
	}

	req->__data_len -= total_bytes;
     800:	80 1b 00 30 	lwz     r0,48(r27)
static inline void *bio_data(struct bio *bio)
{
	if (bio->bi_vcnt)
		return page_address(bio_page(bio)) + bio_offset(bio);

	return NULL;
     804:	39 20 00 00 	li      r9,0
     808:	7c 1d 00 50 	subf    r0,r29,r0
     80c:	90 1b 00 30 	stw     r0,48(r27)
		return bio->bi_size;
}

static inline void *bio_data(struct bio *bio)
{
	if (bio->bi_vcnt)
     810:	a1 78 00 18 	lhz     r11,24(r24)
     814:	2f 8b 00 00 	cmpwi   cr7,r11,0
     818:	40 9e 01 78 	bne-    cr7,990 <blk_update_request+0x320>
	req->buffer = bio_data(req->bio);

	/* update sector only for requests with clear definition of sector */
	if (req->cmd_type == REQ_TYPE_FS || (req->cmd_flags & REQ_DISCARD))
     81c:	81 7b 00 24 	lwz     r11,36(r27)
		bio_iovec(bio)->bv_offset += nr_bytes;
		bio_iovec(bio)->bv_len -= nr_bytes;
	}

	req->__data_len -= total_bytes;
	req->buffer = bio_data(req->bio);
     820:	91 3b 00 80 	stw     r9,128(r27)

	/* update sector only for requests with clear definition of sector */
	if (req->cmd_type == REQ_TYPE_FS || (req->cmd_flags & REQ_DISCARD))
     824:	2f 8b 00 01 	cmpwi   cr7,r11,1
     828:	80 fb 00 20 	lwz     r7,32(r27)
     82c:	41 9e 00 0c 	beq-    cr7,838 <blk_update_request+0x1c8>
     830:	70 e9 00 40 	andi.   r9,r7,64
     834:	41 82 00 24 	beq-    858 <blk_update_request+0x1e8>
		req->__sector += total_bytes >> 9;
     838:	81 1b 00 38 	lwz     r8,56(r27)
     83c:	7f ab 4e 70 	srawi   r11,r29,9
     840:	7f aa fe 70 	srawi   r10,r29,31
     844:	81 3b 00 3c 	lwz     r9,60(r27)
     848:	7d 6b 48 14 	addc    r11,r11,r9
     84c:	7d 4a 41 14 	adde    r10,r10,r8
     850:	91 5b 00 38 	stw     r10,56(r27)
     854:	91 7b 00 3c 	stw     r11,60(r27)

	/* mixed attributes always follow the first bio */
	if (req->cmd_flags & REQ_MIXED_MERGE) {
     858:	74 e9 04 00 	andis.  r9,r7,1024
     85c:	40 82 00 94 	bne-    8f0 <blk_update_request+0x280>
     860:	2f 98 00 00 	cmpwi   cr7,r24,0
     864:	41 9e 00 34 	beq-    cr7,898 <blk_update_request+0x228>
#define bio_segments(bio)	((bio)->bi_vcnt - (bio)->bi_idx)
#define bio_sectors(bio)	((bio)->bi_size >> 9)

static inline unsigned int bio_cur_bytes(struct bio *bio)
{
	if (bio->bi_vcnt)
     868:	a1 38 00 18 	lhz     r9,24(r24)
     86c:	2f 89 00 00 	cmpwi   cr7,r9,0
     870:	41 9e 01 10 	beq-    cr7,980 <blk_update_request+0x310>
		return bio_iovec(bio)->bv_len;
     874:	a1 38 00 1a 	lhz     r9,26(r24)
     878:	81 78 00 38 	lwz     r11,56(r24)
     87c:	55 2a 10 3a 	rlwinm  r10,r9,2,0,29
     880:	55 29 20 36 	rlwinm  r9,r9,4,0,27
     884:	7d 2a 48 50 	subf    r9,r10,r9
     888:	7d 2b 4a 14 	add     r9,r11,r9
     88c:	81 29 00 04 	lwz     r9,4(r9)

	/*
	 * If total number of sectors is less than the first segment
	 * size, something has gone terribly wrong.
	 */
	if (blk_rq_bytes(req) < blk_rq_cur_bytes(req)) {
     890:	7f 80 48 40 	cmplw   cr7,r0,r9
     894:	41 9c 01 c0 	blt-    cr7,a54 <blk_update_request+0x3e4>
		printk(KERN_ERR "blk: request botched\n");
		req->__data_len = blk_rq_cur_bytes(req);
	}

	/* recalculate the number of segments */
	blk_recalc_rq_segments(req);
     898:	7f 63 db 78 	mr      r3,r27
     89c:	48 00 00 01 	bl      89c <blk_update_request+0x22c>

	return true;
}
     8a0:	80 01 00 34 	lwz     r0,52(r1)
	}

	/* recalculate the number of segments */
	blk_recalc_rq_segments(req);

	return true;
     8a4:	38 60 00 01 	li      r3,1
}
     8a8:	bb 01 00 10 	lmw     r24,16(r1)
     8ac:	38 21 00 30 	addi    r1,r1,48
     8b0:	7c 08 03 a6 	mtlr    r0
     8b4:	4e 80 00 20 	blr
	 * low level drivers do what they see fit.
	 */
	if (req->cmd_type == REQ_TYPE_FS)
		req->errors = 0;

	if (error && req->cmd_type == REQ_TYPE_FS &&
     8b8:	40 9e fd f8 	bne+    cr7,6b0 <blk_update_request+0x40>
	    !(req->cmd_flags & REQ_QUIET)) {
     8bc:	80 1b 00 20 	lwz     r0,32(r27)
	 * low level drivers do what they see fit.
	 */
	if (req->cmd_type == REQ_TYPE_FS)
		req->errors = 0;

	if (error && req->cmd_type == REQ_TYPE_FS &&
     8c0:	74 09 00 10 	andis.  r9,r0,16
     8c4:	40 82 fd ec 	bne+    6b0 <blk_update_request+0x40>
	    !(req->cmd_flags & REQ_QUIET)) {
		printk(KERN_ERR "end_request: I/O error, dev %s, sector %llu\n",
				req->rq_disk ? req->rq_disk->disk_name : "?",
     8c8:	80 9b 00 68 	lwz     r4,104(r27)
	if (req->cmd_type == REQ_TYPE_FS)
		req->errors = 0;

	if (error && req->cmd_type == REQ_TYPE_FS &&
	    !(req->cmd_flags & REQ_QUIET)) {
		printk(KERN_ERR "end_request: I/O error, dev %s, sector %llu\n",
     8cc:	2f 84 00 00 	cmpwi   cr7,r4,0
     8d0:	41 9e 01 d0 	beq-    cr7,aa0 <blk_update_request+0x430>
     8d4:	38 84 00 0c 	addi    r4,r4,12
     8d8:	80 bb 00 38 	lwz     r5,56(r27)
     8dc:	3c 60 00 00 	lis     r3,0
     8e0:	80 db 00 3c 	lwz     r6,60(r27)
     8e4:	38 63 00 c4 	addi    r3,r3,196
     8e8:	48 00 00 01 	bl      8e8 <blk_update_request+0x278>
     8ec:	4b ff fd c4 	b       6b0 <blk_update_request+0x40>
	if (req->cmd_type == REQ_TYPE_FS || (req->cmd_flags & REQ_DISCARD))
		req->__sector += total_bytes >> 9;

	/* mixed attributes always follow the first bio */
	if (req->cmd_flags & REQ_MIXED_MERGE) {
		req->cmd_flags &= ~REQ_FAILFAST_MASK;
     8f0:	54 e7 07 f6 	rlwinm  r7,r7,0,31,27
     8f4:	90 fb 00 20 	stw     r7,32(r27)
		req->cmd_flags |= req->bio->bi_rw & REQ_FAILFAST_MASK;
     8f8:	81 38 00 14 	lwz     r9,20(r24)
     8fc:	55 29 07 3c 	rlwinm  r9,r9,0,28,30
     900:	7c e7 4b 78 	or      r7,r7,r9
     904:	90 fb 00 20 	stw     r7,32(r27)
     908:	4b ff ff 58 	b       860 <blk_update_request+0x1f0>
	       (rq->cmd_flags & REQ_IO_STAT) &&
     90c:	81 3b 00 24 	lwz     r9,36(r27)
     910:	2f 89 00 01 	cmpwi   cr7,r9,1
     914:	41 9e 00 0c 	beq-    cr7,920 <blk_update_request+0x2b0>
	       (rq->cmd_type == REQ_TYPE_FS ||
     918:	70 09 00 40 	andi.   r9,r0,64
     91c:	41 a2 fd ac 	beq-    6c8 <blk_update_request+0x58>
EXPORT_SYMBOL_GPL(blk_rq_err_bytes);

static void blk_account_io_completion(struct request *req, unsigned int bytes)
{
	if (blk_do_io_stat(req)) {
		const int rw = rq_data_dir(req);
     920:	54 08 07 fe 	clrlwi  r8,r0,31
		struct hd_struct *part;
		int cpu;

		cpu = part_stat_lock();
		part = req->part;
     924:	81 7b 00 6c 	lwz     r11,108(r27)
		part_stat_add(cpu, part, sectors[rw], bytes >> 9);
     928:	57 ea ba 7e 	rlwinm  r10,r31,23,9,31
     92c:	39 28 00 3c 	addi    r9,r8,60
     930:	55 29 10 3a 	rlwinm  r9,r9,2,0,29
     934:	7d 2b 4a 14 	add     r9,r11,r9
     938:	80 09 00 04 	lwz     r0,4(r9)
     93c:	7c 0a 02 14 	add     r0,r10,r0
     940:	90 09 00 04 	stw     r0,4(r9)
     944:	80 0b 00 e0 	lwz     r0,224(r11)
     948:	2f 80 00 00 	cmpwi   cr7,r0,0
     94c:	41 be fd 7c 	beq-    cr7,6c8 <blk_update_request+0x58>
	int node_id;
};

static inline struct gendisk *part_to_disk(struct hd_struct *part)
{
	if (likely(part)) {
     950:	2f 8b 00 00 	cmpwi   cr7,r11,0
		if (part->partno)
			return dev_to_disk(part_to_dev(part)->parent);
		else
			return dev_to_disk(part_to_dev(part));
	}
	return NULL;
     954:	39 20 00 00 	li      r9,0
	int node_id;
};

static inline struct gendisk *part_to_disk(struct hd_struct *part)
{
	if (likely(part)) {
     958:	41 9e 00 0c 	beq-    cr7,964 <blk_update_request+0x2f4>
		if (part->partno)
			return dev_to_disk(part_to_dev(part)->parent);
     95c:	81 2b 00 20 	lwz     r9,32(r11)
     960:	39 29 ff a0 	addi    r9,r9,-96
     964:	39 68 00 4c 	addi    r11,r8,76
     968:	55 6b 10 3a 	rlwinm  r11,r11,2,0,29
     96c:	7d 29 5a 14 	add     r9,r9,r11
     970:	80 09 00 04 	lwz     r0,4(r9)
     974:	7c 0a 02 14 	add     r0,r10,r0
     978:	90 09 00 04 	stw     r0,4(r9)
     97c:	4b ff fd 4c 	b       6c8 <blk_update_request+0x58>
	else /* dataless requests such as discard */
		return bio->bi_size;
     980:	81 38 00 20 	lwz     r9,32(r24)
     984:	4b ff ff 0c 	b       890 <blk_update_request+0x220>
	 *
	 * TODO: tj: This is too subtle.  It would be better to let
	 * low level drivers do what they see fit.
	 */
	if (req->cmd_type == REQ_TYPE_FS)
		req->errors = 0;
     988:	90 7b 00 88 	stw     r3,136(r27)
     98c:	4b ff fd 1c 	b       6a8 <blk_update_request+0x38>
}

static inline void *bio_data(struct bio *bio)
{
	if (bio->bi_vcnt)
		return page_address(bio_page(bio)) + bio_offset(bio);
     990:	a0 18 00 1a 	lhz     r0,26(r24)
     994:	81 38 00 38 	lwz     r9,56(r24)
     998:	54 0b 10 3a 	rlwinm  r11,r0,2,0,29
     99c:	54 00 20 36 	rlwinm  r0,r0,4,0,27
     9a0:	7c 0b 00 50 	subf    r0,r11,r0
     9a4:	7c 69 00 2e 	lwzx    r3,r9,r0
     9a8:	48 00 00 01 	bl      9a8 <blk_update_request+0x338>
     9ac:	a0 18 00 1a 	lhz     r0,26(r24)
     9b0:	81 38 00 38 	lwz     r9,56(r24)
     9b4:	54 0b 10 3a 	rlwinm  r11,r0,2,0,29
     9b8:	54 00 20 36 	rlwinm  r0,r0,4,0,27
     9bc:	83 1b 00 40 	lwz     r24,64(r27)
     9c0:	7c 0b 00 50 	subf    r0,r11,r0
     9c4:	7d 29 02 14 	add     r9,r9,r0
     9c8:	80 1b 00 30 	lwz     r0,48(r27)
     9cc:	81 29 00 08 	lwz     r9,8(r9)
     9d0:	7d 23 4a 14 	add     r9,r3,r9
     9d4:	4b ff fe 48 	b       81c <blk_update_request+0x1ac>

	/*
	 * if the request wasn't completed, update state
	 */
	if (bio_nbytes) {
		req_bio_endio(req, bio, bio_nbytes, error);
     9d8:	7f 63 db 78 	mr      r3,r27
     9dc:	7f 44 d3 78 	mr      r4,r26
     9e0:	7f 85 e3 78 	mr      r5,r28
     9e4:	7f 26 cb 78 	mr      r6,r25
     9e8:	4b ff fb 51 	bl      538 <req_bio_endio>
		bio->bi_idx += next_idx;
     9ec:	a0 1a 00 1a 	lhz     r0,26(r26)
		bio_iovec(bio)->bv_offset += nr_bytes;
     9f0:	81 3a 00 38 	lwz     r9,56(r26)
	/*
	 * if the request wasn't completed, update state
	 */
	if (bio_nbytes) {
		req_bio_endio(req, bio, bio_nbytes, error);
		bio->bi_idx += next_idx;
     9f4:	7c 1e 02 14 	add     r0,r30,r0
     9f8:	54 00 04 3e 	clrlwi  r0,r0,16
     9fc:	b0 1a 00 1a 	sth     r0,26(r26)
		bio_iovec(bio)->bv_offset += nr_bytes;
     a00:	54 0b 10 3a 	rlwinm  r11,r0,2,0,29
     a04:	54 00 20 36 	rlwinm  r0,r0,4,0,27
     a08:	7c 0b 00 50 	subf    r0,r11,r0
     a0c:	7d 29 02 14 	add     r9,r9,r0
     a10:	80 09 00 08 	lwz     r0,8(r9)
     a14:	7c 00 fa 14 	add     r0,r0,r31
     a18:	90 09 00 08 	stw     r0,8(r9)
		bio_iovec(bio)->bv_len -= nr_bytes;
     a1c:	a0 1a 00 1a 	lhz     r0,26(r26)
     a20:	81 3a 00 38 	lwz     r9,56(r26)
     a24:	54 0b 10 3a 	rlwinm  r11,r0,2,0,29
     a28:	54 00 20 36 	rlwinm  r0,r0,4,0,27
     a2c:	7c 0b 00 50 	subf    r0,r11,r0
     a30:	7d 29 02 14 	add     r9,r9,r0
     a34:	80 09 00 04 	lwz     r0,4(r9)
     a38:	7f ff 00 50 	subf    r31,r31,r0
     a3c:	93 e9 00 04 	stw     r31,4(r9)
     a40:	83 1b 00 40 	lwz     r24,64(r27)
     a44:	4b ff fd bc 	b       800 <blk_update_request+0x190>

			/*
			 * not a complete bvec done
			 */
			if (unlikely(nbytes > nr_bytes)) {
				bio_nbytes += nr_bytes;
     a48:	7f 9c fa 14 	add     r28,r28,r31
				total_bytes += nr_bytes;
     a4c:	7f bd fa 14 	add     r29,r29,r31
				break;
     a50:	4b ff fd a8 	b       7f8 <blk_update_request+0x188>
	/*
	 * If total number of sectors is less than the first segment
	 * size, something has gone terribly wrong.
	 */
	if (blk_rq_bytes(req) < blk_rq_cur_bytes(req)) {
		printk(KERN_ERR "blk: request botched\n");
     a54:	3c 60 00 00 	lis     r3,0
     a58:	38 63 01 20 	addi    r3,r3,288
     a5c:	48 00 00 01 	bl      a5c <blk_update_request+0x3ec>

	blk_requestq_cachep = kmem_cache_create("blkdev_queue",
			sizeof(struct request_queue), 0, SLAB_PANIC, NULL);

	return 0;
}
     a60:	81 3b 00 40 	lwz     r9,64(r27)
     a64:	38 00 00 00 	li      r0,0
     a68:	2f 89 00 00 	cmpwi   cr7,r9,0
     a6c:	41 9e 00 2c 	beq-    cr7,a98 <blk_update_request+0x428>
#define bio_segments(bio)	((bio)->bi_vcnt - (bio)->bi_idx)
#define bio_sectors(bio)	((bio)->bi_size >> 9)

static inline unsigned int bio_cur_bytes(struct bio *bio)
{
	if (bio->bi_vcnt)
     a70:	a0 09 00 18 	lhz     r0,24(r9)
     a74:	2f 80 00 00 	cmpwi   cr7,r0,0
     a78:	41 9e 00 34 	beq-    cr7,aac <blk_update_request+0x43c>
		return bio_iovec(bio)->bv_len;
     a7c:	a0 09 00 1a 	lhz     r0,26(r9)
     a80:	81 29 00 38 	lwz     r9,56(r9)
     a84:	54 0b 10 3a 	rlwinm  r11,r0,2,0,29
     a88:	54 00 20 36 	rlwinm  r0,r0,4,0,27
     a8c:	7c 0b 00 50 	subf    r0,r11,r0
     a90:	7d 29 02 14 	add     r9,r9,r0
     a94:	80 09 00 04 	lwz     r0,4(r9)
	 * If total number of sectors is less than the first segment
	 * size, something has gone terribly wrong.
	 */
	if (blk_rq_bytes(req) < blk_rq_cur_bytes(req)) {
		printk(KERN_ERR "blk: request botched\n");
		req->__data_len = blk_rq_cur_bytes(req);
     a98:	90 1b 00 30 	stw     r0,48(r27)
     a9c:	4b ff fd fc 	b       898 <blk_update_request+0x228>
	if (req->cmd_type == REQ_TYPE_FS)
		req->errors = 0;

	if (error && req->cmd_type == REQ_TYPE_FS &&
	    !(req->cmd_flags & REQ_QUIET)) {
		printk(KERN_ERR "end_request: I/O error, dev %s, sector %llu\n",
     aa0:	3c 80 00 00 	lis     r4,0
     aa4:	38 84 00 14 	addi    r4,r4,20
     aa8:	4b ff fe 30 	b       8d8 <blk_update_request+0x268>
	else /* dataless requests such as discard */
		return bio->bi_size;
     aac:	80 09 00 20 	lwz     r0,32(r9)
     ab0:	4b ff ff e8 	b       a98 <blk_update_request+0x428>

00000ab4 <drive_stat_acct>:
 * Controlling structure to kblockd
 */
static struct workqueue_struct *kblockd_workqueue;

static void drive_stat_acct(struct request *rq, int new_io)
{
     ab4:	94 21 ff e0 	stwu    r1,-32(r1)
     ab8:	7c 08 02 a6 	mflr    r0
     abc:	bf a1 00 14 	stmw    r29,20(r1)
     ac0:	7c 7f 1b 78 	mr      r31,r3
     ac4:	90 01 00 24 	stw     r0,36(r1)
 *	b) the queue had IO stats enabled when this request was started, and
 *	c) it's a file system request or a discard request
 */
static inline int blk_do_io_stat(struct request *rq)
{
	return rq->rq_disk &&
     ac8:	80 63 00 68 	lwz     r3,104(r3)
	struct hd_struct *part;
	int rw = rq_data_dir(rq);
     acc:	83 df 00 20 	lwz     r30,32(r31)
     ad0:	2f 83 00 00 	cmpwi   cr7,r3,0
     ad4:	41 9e 00 80 	beq-    cr7,b54 <drive_stat_acct+0xa0>
     ad8:	77 c0 02 00 	andis.  r0,r30,512
     adc:	41 82 00 78 	beq-    b54 <drive_stat_acct+0xa0>
	       (rq->cmd_flags & REQ_IO_STAT) &&
     ae0:	80 1f 00 24 	lwz     r0,36(r31)
     ae4:	2f 80 00 01 	cmpwi   cr7,r0,1
     ae8:	41 9e 00 0c 	beq-    cr7,af4 <drive_stat_acct+0x40>
	       (rq->cmd_type == REQ_TYPE_FS ||
     aec:	73 c0 00 40 	andi.   r0,r30,64
     af0:	41 82 00 64 	beq-    b54 <drive_stat_acct+0xa0>
	if (!blk_do_io_stat(rq))
		return;

	cpu = part_stat_lock();

	if (!new_io) {
     af4:	2f 84 00 00 	cmpwi   cr7,r4,0
static struct workqueue_struct *kblockd_workqueue;

static void drive_stat_acct(struct request *rq, int new_io)
{
	struct hd_struct *part;
	int rw = rq_data_dir(rq);
     af8:	57 de 07 fe 	clrlwi  r30,r30,31
	if (!blk_do_io_stat(rq))
		return;

	cpu = part_stat_lock();

	if (!new_io) {
     afc:	40 9e 00 6c 	bne-    cr7,b68 <drive_stat_acct+0xb4>
		part = rq->part;
     b00:	81 3f 00 6c 	lwz     r9,108(r31)
		part_stat_inc(cpu, part, merges[rw]);
     b04:	39 7e 00 40 	addi    r11,r30,64
     b08:	55 6b 10 3a 	rlwinm  r11,r11,2,0,29
     b0c:	7d 69 5a 14 	add     r11,r9,r11
     b10:	81 4b 00 04 	lwz     r10,4(r11)
     b14:	38 0a 00 01 	addi    r0,r10,1
     b18:	90 0b 00 04 	stw     r0,4(r11)
     b1c:	80 09 00 e0 	lwz     r0,224(r9)
     b20:	2f 80 00 00 	cmpwi   cr7,r0,0
     b24:	41 9e 00 30 	beq-    cr7,b54 <drive_stat_acct+0xa0>
	int node_id;
};

static inline struct gendisk *part_to_disk(struct hd_struct *part)
{
	if (likely(part)) {
     b28:	2f 89 00 00 	cmpwi   cr7,r9,0
		if (part->partno)
			return dev_to_disk(part_to_dev(part)->parent);
		else
			return dev_to_disk(part_to_dev(part));
	}
	return NULL;
     b2c:	38 00 00 00 	li      r0,0
	int node_id;
};

static inline struct gendisk *part_to_disk(struct hd_struct *part)
{
	if (likely(part)) {
     b30:	41 9e 00 0c 	beq-    cr7,b3c <drive_stat_acct+0x88>
		if (part->partno)
			return dev_to_disk(part_to_dev(part)->parent);
     b34:	81 29 00 20 	lwz     r9,32(r9)
     b38:	38 09 ff a0 	addi    r0,r9,-96
     b3c:	3b de 00 50 	addi    r30,r30,80
     b40:	57 de 10 3a 	rlwinm  r30,r30,2,0,29
     b44:	7f c0 f2 14 	add     r30,r0,r30
     b48:	81 3e 00 04 	lwz     r9,4(r30)
     b4c:	38 09 00 01 	addi    r0,r9,1
     b50:	90 1e 00 04 	stw     r0,4(r30)
		part_inc_in_flight(part, rw);
		rq->part = part;
	}

	part_stat_unlock();
}
     b54:	80 01 00 24 	lwz     r0,36(r1)
     b58:	bb a1 00 14 	lmw     r29,20(r1)
     b5c:	38 21 00 20 	addi    r1,r1,32
     b60:	7c 08 03 a6 	mtlr    r0
     b64:	4e 80 00 20 	blr

	if (!new_io) {
		part = rq->part;
		part_stat_inc(cpu, part, merges[rw]);
	} else {
		part = disk_map_sector_rcu(rq->rq_disk, blk_rq_pos(rq));
     b68:	80 bf 00 38 	lwz     r5,56(r31)
     b6c:	80 df 00 3c 	lwz     r6,60(r31)
     b70:	48 00 00 01 	bl      b70 <drive_stat_acct+0xbc>
 */
static __inline__ int atomic_add_unless(atomic_t *v, int a, int u)
{
	int t;

	__asm__ __volatile__ (
     b74:	39 40 00 00 	li      r10,0
     b78:	39 60 00 01 	li      r11,1
     b7c:	7c 7d 1b 78 	mr      r29,r3
	bne-	1b \n"
	PPC_ACQUIRE_BARRIER
"	subf	%0,%2,%0 \n\
2:"
	: "=&r" (t)
	: "r" (&v->counter), "r" (a), "r" (u)
     b80:	39 23 01 1c 	addi    r9,r3,284
 */
static __inline__ int atomic_add_unless(atomic_t *v, int a, int u)
{
	int t;

	__asm__ __volatile__ (
     b84:	7c 00 48 28 	lwarx   r0,0,r9
     b88:	7c 00 50 00 	cmpw    r0,r10
     b8c:	41 82 00 14 	beq-    ba0 <drive_stat_acct+0xec>
     b90:	7c 0b 02 14 	add     r0,r11,r0
     b94:	7c 00 49 2d 	stwcx.  r0,0,r9
     b98:	40 a2 ff ec 	bne-    b84 <drive_stat_acct+0xd0>
     b9c:	7c 0b 00 50 	subf    r0,r11,r0
		if (!hd_struct_try_get(part)) {
     ba0:	2f 80 00 00 	cmpwi   cr7,r0,0
     ba4:	40 9e 00 20 	bne-    cr7,bc4 <drive_stat_acct+0x110>
			 *
			 * We take a reference on disk->part0 although that
			 * partition will never be deleted, so we can treat
			 * it as any other partition.
			 */
			part = &rq->rq_disk->part0;
     ba8:	81 3f 00 68 	lwz     r9,104(r31)
     bac:	3b a9 00 40 	addi    r29,r9,64
	smp_mb();
}

static inline void hd_struct_get(struct hd_struct *part)
{
	atomic_inc(&part->ref);
     bb0:	38 09 01 5c 	addi    r0,r9,348

static __inline__ void atomic_inc(atomic_t *v)
{
	int t;

	__asm__ __volatile__(
     bb4:	7d 60 00 28 	lwarx   r11,0,r0
     bb8:	31 6b 00 01 	addic   r11,r11,1
     bbc:	7d 60 01 2d 	stwcx.  r11,0,r0
     bc0:	40 a2 ff f4 	bne-    bb4 <drive_stat_acct+0x100>
			hd_struct_get(part);
		}
		part_round_stats(cpu, part);
     bc4:	38 60 00 00 	li      r3,0
     bc8:	7f a4 eb 78 	mr      r4,r29
     bcc:	48 00 00 01 	bl      bcc <drive_stat_acct+0x118>
#define part_stat_sub(cpu, gendiskp, field, subnd)			\
	part_stat_add(cpu, gendiskp, field, -subnd)

static inline void part_inc_in_flight(struct hd_struct *part, int rw)
{
	part->in_flight[rw]++;
     bd0:	39 3e 00 38 	addi    r9,r30,56
     bd4:	55 29 10 3a 	rlwinm  r9,r9,2,0,29
     bd8:	7d 3d 4a 14 	add     r9,r29,r9
     bdc:	81 69 00 0c 	lwz     r11,12(r9)
     be0:	38 0b 00 01 	addi    r0,r11,1
     be4:	90 09 00 0c 	stw     r0,12(r9)
	if (part->partno)
     be8:	80 1d 00 e0 	lwz     r0,224(r29)
     bec:	2f 80 00 00 	cmpwi   cr7,r0,0
     bf0:	41 9e 00 30 	beq-    cr7,c20 <drive_stat_acct+0x16c>
	int node_id;
};

static inline struct gendisk *part_to_disk(struct hd_struct *part)
{
	if (likely(part)) {
     bf4:	2f 9d 00 00 	cmpwi   cr7,r29,0
		if (part->partno)
			return dev_to_disk(part_to_dev(part)->parent);
		else
			return dev_to_disk(part_to_dev(part));
	}
	return NULL;
     bf8:	38 00 00 00 	li      r0,0
	int node_id;
};

static inline struct gendisk *part_to_disk(struct hd_struct *part)
{
	if (likely(part)) {
     bfc:	41 9e 00 0c 	beq-    cr7,c08 <drive_stat_acct+0x154>
		if (part->partno)
			return dev_to_disk(part_to_dev(part)->parent);
     c00:	81 3d 00 20 	lwz     r9,32(r29)
     c04:	38 09 ff a0 	addi    r0,r9,-96

static inline void part_inc_in_flight(struct hd_struct *part, int rw)
{
	part->in_flight[rw]++;
	if (part->partno)
		part_to_disk(part)->part0.in_flight[rw]++;
     c08:	3b de 00 48 	addi    r30,r30,72
     c0c:	57 de 10 3a 	rlwinm  r30,r30,2,0,29
     c10:	7f c0 f2 14 	add     r30,r0,r30
     c14:	81 3e 00 0c 	lwz     r9,12(r30)
     c18:	38 09 00 01 	addi    r0,r9,1
     c1c:	90 1e 00 0c 	stw     r0,12(r30)
		part_inc_in_flight(part, rw);
		rq->part = part;
     c20:	93 bf 00 6c 	stw     r29,108(r31)
	}

	part_stat_unlock();
}
     c24:	80 01 00 24 	lwz     r0,36(r1)
     c28:	bb a1 00 14 	lmw     r29,20(r1)
     c2c:	38 21 00 20 	addi    r1,r1,32
     c30:	7c 08 03 a6 	mtlr    r0
     c34:	4e 80 00 20 	blr

00000c38 <handle_bad_sector>:
				      bio->bi_sector - p->start_sect);
	}
}

static void handle_bad_sector(struct bio *bio)
{
     c38:	94 21 ff c0 	stwu    r1,-64(r1)
     c3c:	7c 08 02 a6 	mflr    r0
     c40:	bf a1 00 34 	stmw    r29,52(r1)
     c44:	7c 7f 1b 78 	mr      r31,r3
	char b[BDEVNAME_SIZE];

	printk(KERN_INFO "attempt to access beyond end of device\n");
     c48:	3c 60 00 00 	lis     r3,0
     c4c:	38 63 01 3c 	addi    r3,r3,316
				      bio->bi_sector - p->start_sect);
	}
}

static void handle_bad_sector(struct bio *bio)
{
     c50:	90 01 00 44 	stw     r0,68(r1)
	char b[BDEVNAME_SIZE];

	printk(KERN_INFO "attempt to access beyond end of device\n");
     c54:	48 00 00 01 	bl      c54 <handle_bad_sector+0x1c>
	printk(KERN_INFO "%s: rw=%ld, want=%Lu, limit=%Lu\n",
     c58:	80 7f 00 0c 	lwz     r3,12(r31)
     c5c:	38 81 00 08 	addi    r4,r1,8
     c60:	48 00 00 01 	bl      c60 <handle_bad_sector+0x28>
			bdevname(bio->bi_bdev, b),
			bio->bi_rw,
			(unsigned long long)bio->bi_sector + bio_sectors(bio),
			(long long)(i_size_read(bio->bi_bdev->bd_inode) >> 9));
     c64:	81 3f 00 0c 	lwz     r9,12(r31)
static void handle_bad_sector(struct bio *bio)
{
	char b[BDEVNAME_SIZE];

	printk(KERN_INFO "attempt to access beyond end of device\n");
	printk(KERN_INFO "%s: rw=%ld, want=%Lu, limit=%Lu\n",
     c68:	39 00 00 00 	li      r8,0
			bdevname(bio->bi_bdev, b),
			bio->bi_rw,
			(unsigned long long)bio->bi_sector + bio_sectors(bio),
     c6c:	80 1f 00 20 	lwz     r0,32(r31)
static void handle_bad_sector(struct bio *bio)
{
	char b[BDEVNAME_SIZE];

	printk(KERN_INFO "attempt to access beyond end of device\n");
	printk(KERN_INFO "%s: rw=%ld, want=%Lu, limit=%Lu\n",
     c70:	7c 64 1b 78 	mr      r4,r3
     c74:	3c 60 00 00 	lis     r3,0

	blk_requestq_cachep = kmem_cache_create("blkdev_queue",
			sizeof(struct request_queue), 0, SLAB_PANIC, NULL);

	return 0;
}
     c78:	81 29 00 04 	lwz     r9,4(r9)
static void handle_bad_sector(struct bio *bio)
{
	char b[BDEVNAME_SIZE];

	printk(KERN_INFO "attempt to access beyond end of device\n");
	printk(KERN_INFO "%s: rw=%ld, want=%Lu, limit=%Lu\n",
     c7c:	38 63 01 68 	addi    r3,r3,360
     c80:	80 ff 00 04 	lwz     r7,4(r31)
     c84:	80 df 00 00 	lwz     r6,0(r31)
     c88:	81 49 00 70 	lwz     r10,112(r9)
     c8c:	81 69 00 74 	lwz     r11,116(r9)
     c90:	54 09 ba 7e 	rlwinm  r9,r0,23,9,31
     c94:	7f c9 38 14 	addc    r30,r9,r7
     c98:	7f a8 31 14 	adde    r29,r8,r6
     c9c:	80 bf 00 14 	lwz     r5,20(r31)
     ca0:	7f a7 eb 78 	mr      r7,r29
     ca4:	7f c8 f3 78 	mr      r8,r30
     ca8:	55 7e ba 7e 	rlwinm  r30,r11,23,9,31
     cac:	51 5e b8 10 	rlwimi  r30,r10,23,0,8
     cb0:	7d 5d 4e 70 	srawi   r29,r10,9
     cb4:	7f a9 eb 78 	mr      r9,r29
     cb8:	7f ca f3 78 	mr      r10,r30
     cbc:	48 00 00 01 	bl      cbc <handle_bad_sector+0x84>
DEFINE_BITOP(clear_bits_unlock, andc, PPC_RELEASE_BARRIER, "")
DEFINE_BITOP(change_bits, xor, "", "")

static __inline__ void set_bit(int nr, volatile unsigned long *addr)
{
	set_bits(BITOP_MASK(nr), addr + BITOP_WORD(nr));
     cc0:	39 3f 00 10 	addi    r9,r31,16
	: "=&r" (old), "+m" (*p)		\
	: "r" (mask), "r" (p)			\
	: "cc", "memory");			\
}

DEFINE_BITOP(set_bits, or, "", "")
     cc4:	38 00 00 04 	li      r0,4
     cc8:	7d 60 48 28 	lwarx   r11,0,r9
     ccc:	7d 6b 03 78 	or      r11,r11,r0
     cd0:	7d 60 49 2d 	stwcx.  r11,0,r9
     cd4:	40 a2 ff f4 	bne-    cc8 <handle_bad_sector+0x90>
			bio->bi_rw,
			(unsigned long long)bio->bi_sector + bio_sectors(bio),
			(long long)(i_size_read(bio->bi_bdev->bd_inode) >> 9));

	set_bit(BIO_EOF, &bio->bi_flags);
}
     cd8:	80 01 00 44 	lwz     r0,68(r1)
     cdc:	bb a1 00 34 	lmw     r29,52(r1)
     ce0:	38 21 00 40 	addi    r1,r1,64
     ce4:	7c 08 03 a6 	mtlr    r0
     ce8:	4e 80 00 20 	blr

00000cec <blk_requeue_request>:
 *    Drivers often keep queueing requests until the hardware cannot accept
 *    more, when that condition happens we need to put the request back
 *    on the queue. Must be called with queue lock held.
 */
void blk_requeue_request(struct request_queue *q, struct request *rq)
{
     cec:	94 21 ff f0 	stwu    r1,-16(r1)
     cf0:	7c 08 02 a6 	mflr    r0
     cf4:	bf c1 00 08 	stmw    r30,8(r1)
     cf8:	7c 9f 23 78 	mr      r31,r4
     cfc:	7c 7e 1b 78 	mr      r30,r3
	blk_delete_timer(rq);
     d00:	7c 83 23 78 	mr      r3,r4
 *    Drivers often keep queueing requests until the hardware cannot accept
 *    more, when that condition happens we need to put the request back
 *    on the queue. Must be called with queue lock held.
 */
void blk_requeue_request(struct request_queue *q, struct request *rq)
{
     d04:	90 01 00 14 	stw     r0,20(r1)
	blk_delete_timer(rq);
     d08:	48 00 00 01 	bl      d08 <blk_requeue_request+0x1c>
	set_bits(BITOP_MASK(nr), addr + BITOP_WORD(nr));
}

static __inline__ void clear_bit(int nr, volatile unsigned long *addr)
{
	clear_bits(BITOP_MASK(nr), addr + BITOP_WORD(nr));
     d0c:	39 3f 00 28 	addi    r9,r31,40
	: "r" (mask), "r" (p)			\
	: "cc", "memory");			\
}

DEFINE_BITOP(set_bits, or, "", "")
DEFINE_BITOP(clear_bits, andc, "", "")
     d10:	38 00 00 01 	li      r0,1
     d14:	7d 60 48 28 	lwarx   r11,0,r9
     d18:	7d 6b 00 78 	andc    r11,r11,r0
     d1c:	7d 60 49 2d 	stwcx.  r11,0,r9
     d20:	40 a2 ff f4 	bne-    d14 <blk_requeue_request+0x28>
	blk_clear_rq_complete(rq);
	trace_block_rq_requeue(q, rq);

	if (blk_rq_tagged(rq))
     d24:	80 1f 00 20 	lwz     r0,32(r31)
     d28:	74 09 00 02 	andis.  r9,r0,2
     d2c:	41 a2 00 10 	beq+    d3c <blk_requeue_request+0x50>
		blk_queue_end_tag(q, rq);
     d30:	7f c3 f3 78 	mr      r3,r30
     d34:	7f e4 fb 78 	mr      r4,r31
     d38:	48 00 00 01 	bl      d38 <blk_requeue_request+0x4c>

	BUG_ON(blk_queued_rq(rq));
     d3c:	80 1f 00 00 	lwz     r0,0(r31)
     d40:	7f e0 02 78 	xor     r0,r31,r0
     d44:	7c 00 00 34 	cntlzw  r0,r0
     d48:	54 00 d9 7e 	rlwinm  r0,r0,27,5,31
     d4c:	68 00 00 01 	xori    r0,r0,1
     d50:	0f 00 00 00 	twnei   r0,0

	elv_requeue_request(q, rq);
     d54:	7f c3 f3 78 	mr      r3,r30
     d58:	7f e4 fb 78 	mr      r4,r31
     d5c:	48 00 00 01 	bl      d5c <blk_requeue_request+0x70>
}
     d60:	80 01 00 14 	lwz     r0,20(r1)
     d64:	bb c1 00 08 	lmw     r30,8(r1)
     d68:	38 21 00 10 	addi    r1,r1,16
     d6c:	7c 08 03 a6 	mtlr    r0
     d70:	4e 80 00 20 	blr

00000d74 <blk_alloc_queue_node>:
	return blk_alloc_queue_node(gfp_mask, -1);
}
EXPORT_SYMBOL(blk_alloc_queue);

struct request_queue *blk_alloc_queue_node(gfp_t gfp_mask, int node_id)
{
     d74:	94 21 ff f0 	stwu    r1,-16(r1)
     d78:	7c 08 02 a6 	mflr    r0
	struct request_queue *q;
	int err;

	q = kmem_cache_alloc_node(blk_requestq_cachep,
     d7c:	60 64 80 00 	ori     r4,r3,32768
	return blk_alloc_queue_node(gfp_mask, -1);
}
EXPORT_SYMBOL(blk_alloc_queue);

struct request_queue *blk_alloc_queue_node(gfp_t gfp_mask, int node_id)
{
     d80:	bf c1 00 08 	stmw    r30,8(r1)
void *kmem_cache_alloc(struct kmem_cache *, gfp_t);

static inline void *kmem_cache_alloc_node(struct kmem_cache *cachep,
					gfp_t flags, int node)
{
	return kmem_cache_alloc(cachep, flags);
     d84:	3f c0 00 00 	lis     r30,0
     d88:	80 7e 00 00 	lwz     r3,0(r30)
     d8c:	90 01 00 14 	stw     r0,20(r1)
     d90:	48 00 00 01 	bl      d90 <blk_alloc_queue_node+0x1c>
	struct request_queue *q;
	int err;

	q = kmem_cache_alloc_node(blk_requestq_cachep,
				gfp_mask | __GFP_ZERO, node_id);
	if (!q)
     d94:	7c 7f 1b 79 	mr.     r31,r3
     d98:	41 82 00 fc 	beq-    e94 <blk_alloc_queue_node+0x120>
		return NULL;

	q->backing_dev_info.unplug_io_fn = blk_backing_dev_unplug;
     d9c:	3d 20 00 00 	lis     r9,0
	q->backing_dev_info.unplug_io_data = q;
     da0:	93 ff 00 c0 	stw     r31,192(r31)
			(VM_MAX_READAHEAD * 1024) / PAGE_CACHE_SIZE;
	q->backing_dev_info.state = 0;
	q->backing_dev_info.capabilities = BDI_CAP_MAP_COPY;
	q->backing_dev_info.name = "block";

	err = bdi_init(&q->backing_dev_info);
     da4:	38 7f 00 a0 	addi    r3,r31,160
	q = kmem_cache_alloc_node(blk_requestq_cachep,
				gfp_mask | __GFP_ZERO, node_id);
	if (!q)
		return NULL;

	q->backing_dev_info.unplug_io_fn = blk_backing_dev_unplug;
     da8:	38 09 00 1c 	addi    r0,r9,28
	q->backing_dev_info.unplug_io_data = q;
	q->backing_dev_info.ra_pages =
			(VM_MAX_READAHEAD * 1024) / PAGE_CACHE_SIZE;
	q->backing_dev_info.state = 0;
	q->backing_dev_info.capabilities = BDI_CAP_MAP_COPY;
	q->backing_dev_info.name = "block";
     dac:	3d 20 00 00 	lis     r9,0
	q = kmem_cache_alloc_node(blk_requestq_cachep,
				gfp_mask | __GFP_ZERO, node_id);
	if (!q)
		return NULL;

	q->backing_dev_info.unplug_io_fn = blk_backing_dev_unplug;
     db0:	90 1f 00 bc 	stw     r0,188(r31)
	q->backing_dev_info.unplug_io_data = q;
	q->backing_dev_info.ra_pages =
     db4:	38 00 00 20 	li      r0,32
     db8:	90 1f 00 a8 	stw     r0,168(r31)
			(VM_MAX_READAHEAD * 1024) / PAGE_CACHE_SIZE;
	q->backing_dev_info.state = 0;
     dbc:	38 00 00 00 	li      r0,0
     dc0:	90 1f 00 ac 	stw     r0,172(r31)
	q->backing_dev_info.capabilities = BDI_CAP_MAP_COPY;
     dc4:	38 00 00 04 	li      r0,4
     dc8:	90 1f 00 b0 	stw     r0,176(r31)
	q->backing_dev_info.name = "block";
     dcc:	38 09 01 8c 	addi    r0,r9,396
     dd0:	90 1f 00 c4 	stw     r0,196(r31)

	err = bdi_init(&q->backing_dev_info);
     dd4:	48 00 00 01 	bl      dd4 <blk_alloc_queue_node+0x60>
	if (err) {
     dd8:	2f 83 00 00 	cmpwi   cr7,r3,0
     ddc:	40 9e 00 d0 	bne-    cr7,eac <blk_alloc_queue_node+0x138>
				const char *name,
				struct lock_class_key *key,
				void (*function)(unsigned long),
				unsigned long data)
{
	timer->function = function;
     de0:	3d 20 00 00 	lis     r9,0
	timer->data = data;
     de4:	93 ff 01 60 	stw     r31,352(r31)
	init_timer_key(timer, name, key);
     de8:	38 7f 01 4c 	addi    r3,r31,332
				const char *name,
				struct lock_class_key *key,
				void (*function)(unsigned long),
				unsigned long data)
{
	timer->function = function;
     dec:	38 09 00 00 	addi    r0,r9,0
	timer->data = data;
	init_timer_key(timer, name, key);
     df0:	38 80 00 00 	li      r4,0
				const char *name,
				struct lock_class_key *key,
				void (*function)(unsigned long),
				unsigned long data)
{
	timer->function = function;
     df4:	90 1f 01 5c 	stw     r0,348(r31)
	timer->data = data;
	init_timer_key(timer, name, key);
     df8:	38 a0 00 00 	li      r5,0
     dfc:	48 00 00 01 	bl      dfc <blk_alloc_queue_node+0x88>
		return NULL;
	}

	setup_timer(&q->backing_dev_info.laptop_mode_wb_timer,
		    laptop_mode_timer_fn, (unsigned long) q);
	init_timer(&q->unplug_timer);
     e00:	38 7f 00 6c 	addi    r3,r31,108
     e04:	38 80 00 00 	li      r4,0
     e08:	38 a0 00 00 	li      r5,0
     e0c:	48 00 00 01 	bl      e0c <blk_alloc_queue_node+0x98>
				const char *name,
				struct lock_class_key *key,
				void (*function)(unsigned long),
				unsigned long data)
{
	timer->function = function;
     e10:	3d 20 00 00 	lis     r9,0
	timer->data = data;
     e14:	93 ff 01 f4 	stw     r31,500(r31)
	init_timer_key(timer, name, key);
     e18:	38 a0 00 00 	li      r5,0
				const char *name,
				struct lock_class_key *key,
				void (*function)(unsigned long),
				unsigned long data)
{
	timer->function = function;
     e1c:	38 09 00 00 	addi    r0,r9,0
	timer->data = data;
	init_timer_key(timer, name, key);
     e20:	38 7f 01 e0 	addi    r3,r31,480
				const char *name,
				struct lock_class_key *key,
				void (*function)(unsigned long),
				unsigned long data)
{
	timer->function = function;
     e24:	90 1f 01 f0 	stw     r0,496(r31)
	timer->data = data;
	init_timer_key(timer, name, key);
     e28:	38 80 00 00 	li      r4,0
     e2c:	48 00 00 01 	bl      e2c <blk_alloc_queue_node+0xb8>
	setup_timer(&q->timeout, blk_rq_timed_out_timer, (unsigned long) q);
	INIT_LIST_HEAD(&q->timeout_list);
	INIT_LIST_HEAD(&q->pending_flushes);
     e30:	39 3f 03 34 	addi    r9,r31,820
	INIT_WORK(&q->unplug_work, blk_unplug_work);
     e34:	38 1f 00 94 	addi    r0,r31,148
#define LIST_HEAD(name) \
	struct list_head name = LIST_HEAD_INIT(name)

static inline void INIT_LIST_HEAD(struct list_head *list)
{
	list->next = list;
     e38:	91 3f 03 34 	stw     r9,820(r31)

	setup_timer(&q->backing_dev_info.laptop_mode_wb_timer,
		    laptop_mode_timer_fn, (unsigned long) q);
	init_timer(&q->unplug_timer);
	setup_timer(&q->timeout, blk_rq_timed_out_timer, (unsigned long) q);
	INIT_LIST_HEAD(&q->timeout_list);
     e3c:	39 7f 01 fc 	addi    r11,r31,508
	INIT_LIST_HEAD(&q->pending_flushes);
	INIT_WORK(&q->unplug_work, blk_unplug_work);

	kobject_init(&q->kobj, &blk_queue_ktype);
     e40:	3c 80 00 00 	lis     r4,0
	list->prev = list;
     e44:	91 3f 03 38 	stw     r9,824(r31)
		    laptop_mode_timer_fn, (unsigned long) q);
	init_timer(&q->unplug_timer);
	setup_timer(&q->timeout, blk_rq_timed_out_timer, (unsigned long) q);
	INIT_LIST_HEAD(&q->timeout_list);
	INIT_LIST_HEAD(&q->pending_flushes);
	INIT_WORK(&q->unplug_work, blk_unplug_work);
     e48:	39 20 02 00 	li      r9,512

	kobject_init(&q->kobj, &blk_queue_ktype);
     e4c:	38 7f 01 80 	addi    r3,r31,384
		    laptop_mode_timer_fn, (unsigned long) q);
	init_timer(&q->unplug_timer);
	setup_timer(&q->timeout, blk_rq_timed_out_timer, (unsigned long) q);
	INIT_LIST_HEAD(&q->timeout_list);
	INIT_LIST_HEAD(&q->pending_flushes);
	INIT_WORK(&q->unplug_work, blk_unplug_work);
     e50:	91 3f 00 90 	stw     r9,144(r31)
     e54:	3d 20 00 00 	lis     r9,0

	kobject_init(&q->kobj, &blk_queue_ktype);
     e58:	38 84 00 00 	addi    r4,r4,0
#define LIST_HEAD(name) \
	struct list_head name = LIST_HEAD_INIT(name)

static inline void INIT_LIST_HEAD(struct list_head *list)
{
	list->next = list;
     e5c:	90 1f 00 94 	stw     r0,148(r31)
	list->prev = list;
     e60:	90 1f 00 98 	stw     r0,152(r31)
		    laptop_mode_timer_fn, (unsigned long) q);
	init_timer(&q->unplug_timer);
	setup_timer(&q->timeout, blk_rq_timed_out_timer, (unsigned long) q);
	INIT_LIST_HEAD(&q->timeout_list);
	INIT_LIST_HEAD(&q->pending_flushes);
	INIT_WORK(&q->unplug_work, blk_unplug_work);
     e64:	38 09 00 00 	addi    r0,r9,0
#define LIST_HEAD(name) \
	struct list_head name = LIST_HEAD_INIT(name)

static inline void INIT_LIST_HEAD(struct list_head *list)
{
	list->next = list;
     e68:	91 7f 01 fc 	stw     r11,508(r31)
	list->prev = list;
     e6c:	91 7f 02 00 	stw     r11,512(r31)
     e70:	90 1f 00 9c 	stw     r0,156(r31)

	kobject_init(&q->kobj, &blk_queue_ktype);
     e74:	48 00 00 01 	bl      e74 <blk_alloc_queue_node+0x100>

	mutex_init(&q->sysfs_lock);
     e78:	3c a0 00 00 	lis     r5,0
     e7c:	3c 80 00 00 	lis     r4,0
     e80:	38 a5 00 00 	addi    r5,r5,0
     e84:	38 7f 03 3c 	addi    r3,r31,828
     e88:	38 84 01 94 	addi    r4,r4,404
     e8c:	38 a5 00 04 	addi    r5,r5,4
     e90:	48 00 00 01 	bl      e90 <blk_alloc_queue_node+0x11c>
	spin_lock_init(&q->__queue_lock);

	return q;
}
     e94:	80 01 00 14 	lwz     r0,20(r1)
     e98:	7f e3 fb 78 	mr      r3,r31
     e9c:	bb c1 00 08 	lmw     r30,8(r1)
     ea0:	38 21 00 10 	addi    r1,r1,16
     ea4:	7c 08 03 a6 	mtlr    r0
     ea8:	4e 80 00 20 	blr
	q->backing_dev_info.capabilities = BDI_CAP_MAP_COPY;
	q->backing_dev_info.name = "block";

	err = bdi_init(&q->backing_dev_info);
	if (err) {
		kmem_cache_free(blk_requestq_cachep, q);
     eac:	80 7e 00 00 	lwz     r3,0(r30)
     eb0:	7f e4 fb 78 	mr      r4,r31
		return NULL;
     eb4:	3b e0 00 00 	li      r31,0
	q->backing_dev_info.capabilities = BDI_CAP_MAP_COPY;
	q->backing_dev_info.name = "block";

	err = bdi_init(&q->backing_dev_info);
	if (err) {
		kmem_cache_free(blk_requestq_cachep, q);
     eb8:	48 00 00 01 	bl      eb8 <blk_alloc_queue_node+0x144>
		return NULL;
     ebc:	4b ff ff d8 	b       e94 <blk_alloc_queue_node+0x120>

00000ec0 <blk_alloc_queue>:
	return 0;
}

struct request_queue *blk_alloc_queue(gfp_t gfp_mask)
{
	return blk_alloc_queue_node(gfp_mask, -1);
     ec0:	38 80 ff ff 	li      r4,-1
     ec4:	48 00 00 00 	b       ec4 <blk_alloc_queue+0x4>

00000ec8 <blk_sync_queue>:
 *     that its ->make_request_fn will not re-add plugging prior to calling
 *     this function.
 *
 */
void blk_sync_queue(struct request_queue *q)
{
     ec8:	94 21 ff f0 	stwu    r1,-16(r1)
     ecc:	7c 08 02 a6 	mflr    r0
     ed0:	93 e1 00 0c 	stw     r31,12(r1)
     ed4:	7c 7f 1b 78 	mr      r31,r3
	del_timer_sync(&q->unplug_timer);
     ed8:	38 63 00 6c 	addi    r3,r3,108
 *     that its ->make_request_fn will not re-add plugging prior to calling
 *     this function.
 *
 */
void blk_sync_queue(struct request_queue *q)
{
     edc:	90 01 00 14 	stw     r0,20(r1)
	del_timer_sync(&q->unplug_timer);
     ee0:	48 00 00 01 	bl      ee0 <blk_sync_queue+0x18>
	del_timer_sync(&q->timeout);
     ee4:	38 7f 01 e0 	addi    r3,r31,480
     ee8:	48 00 00 01 	bl      ee8 <blk_sync_queue+0x20>
	cancel_work_sync(&q->unplug_work);
     eec:	38 7f 00 90 	addi    r3,r31,144
     ef0:	48 00 00 01 	bl      ef0 <blk_sync_queue+0x28>
	throtl_shutdown_timer_wq(q);
}
     ef4:	80 01 00 14 	lwz     r0,20(r1)
     ef8:	83 e1 00 0c 	lwz     r31,12(r1)
     efc:	38 21 00 10 	addi    r1,r1,16
     f00:	7c 08 03 a6 	mtlr    r0
     f04:	4e 80 00 20 	blr

00000f08 <blk_cleanup_queue>:
{
	kobject_put(&q->kobj);
}

void blk_cleanup_queue(struct request_queue *q)
{
     f08:	94 21 ff f0 	stwu    r1,-16(r1)
     f0c:	7c 08 02 a6 	mflr    r0
     f10:	bf c1 00 08 	stmw    r30,8(r1)
     f14:	7c 7f 1b 78 	mr      r31,r3
	 * not have processes doing IO to this device.
	 */
	blk_sync_queue(q);

	del_timer_sync(&q->backing_dev_info.laptop_mode_wb_timer);
	mutex_lock(&q->sysfs_lock);
     f18:	3b c3 03 3c 	addi    r30,r3,828
{
	kobject_put(&q->kobj);
}

void blk_cleanup_queue(struct request_queue *q)
{
     f1c:	90 01 00 14 	stw     r0,20(r1)
	 * We know we have process context here, so we can be a little
	 * cautious and ensure that pending block actions on this device
	 * are done before moving on. Going into this function, we should
	 * not have processes doing IO to this device.
	 */
	blk_sync_queue(q);
     f20:	48 00 00 01 	bl      f20 <blk_cleanup_queue+0x18>

	del_timer_sync(&q->backing_dev_info.laptop_mode_wb_timer);
     f24:	38 7f 01 4c 	addi    r3,r31,332
     f28:	48 00 00 01 	bl      f28 <blk_cleanup_queue+0x20>
	mutex_lock(&q->sysfs_lock);
     f2c:	7f c3 f3 78 	mr      r3,r30
     f30:	48 00 00 01 	bl      f30 <blk_cleanup_queue+0x28>
static inline void __set_bit(int nr, volatile unsigned long *addr)
{
	unsigned long mask = BIT_MASK(nr);
	unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);

	*p  |= mask;
     f34:	80 1f 01 78 	lwz     r0,376(r31)
	queue_flag_set_unlocked(QUEUE_FLAG_DEAD, q);
	mutex_unlock(&q->sysfs_lock);
     f38:	7f c3 f3 78 	mr      r3,r30
     f3c:	60 00 00 20 	ori     r0,r0,32
     f40:	90 1f 01 78 	stw     r0,376(r31)
     f44:	48 00 00 01 	bl      f44 <blk_cleanup_queue+0x3c>

	if (q->elevator)
     f48:	80 7f 00 0c 	lwz     r3,12(r31)
     f4c:	2f 83 00 00 	cmpwi   cr7,r3,0
     f50:	41 9e 00 08 	beq-    cr7,f58 <blk_cleanup_queue+0x50>
		elevator_exit(q->elevator);
     f54:	48 00 00 01 	bl      f54 <blk_cleanup_queue+0x4c>
}
EXPORT_SYMBOL(blk_run_queue);

void blk_put_queue(struct request_queue *q)
{
	kobject_put(&q->kobj);
     f58:	38 7f 01 80 	addi    r3,r31,384
     f5c:	48 00 00 01 	bl      f5c <blk_cleanup_queue+0x54>

	if (q->elevator)
		elevator_exit(q->elevator);

	blk_put_queue(q);
}
     f60:	80 01 00 14 	lwz     r0,20(r1)
     f64:	bb c1 00 08 	lmw     r30,8(r1)
     f68:	38 21 00 10 	addi    r1,r1,16
     f6c:	7c 08 03 a6 	mtlr    r0
     f70:	4e 80 00 20 	blr

00000f74 <blk_rq_init>:
	return ret;
}
EXPORT_SYMBOL(blk_get_backing_dev_info);

void blk_rq_init(struct request_queue *q, struct request *rq)
{
     f74:	94 21 ff f0 	stwu    r1,-16(r1)
     f78:	7c 08 02 a6 	mflr    r0
	memset(rq, 0, sizeof(*rq));
     f7c:	38 a0 00 d8 	li      r5,216
	return ret;
}
EXPORT_SYMBOL(blk_get_backing_dev_info);

void blk_rq_init(struct request_queue *q, struct request *rq)
{
     f80:	bf c1 00 08 	stmw    r30,8(r1)
     f84:	7c 9f 23 78 	mr      r31,r4
     f88:	7c 7e 1b 78 	mr      r30,r3
	memset(rq, 0, sizeof(*rq));
     f8c:	38 80 00 00 	li      r4,0
     f90:	7f e3 fb 78 	mr      r3,r31
	return ret;
}
EXPORT_SYMBOL(blk_get_backing_dev_info);

void blk_rq_init(struct request_queue *q, struct request *rq)
{
     f94:	90 01 00 14 	stw     r0,20(r1)
	memset(rq, 0, sizeof(*rq));
     f98:	48 00 00 01 	bl      f98 <blk_rq_init+0x24>
#define LIST_HEAD(name) \
	struct list_head name = LIST_HEAD_INIT(name)

static inline void INIT_LIST_HEAD(struct list_head *list)
{
	list->next = list;
     f9c:	93 ff 00 00 	stw     r31,0(r31)
	RB_CLEAR_NODE(&rq->rb_node);
	rq->cmd = rq->__cmd;
	rq->cmd_len = BLK_MAX_CDB;
	rq->tag = -1;
	rq->ref_count = 1;
	rq->start_time = jiffies;
     fa0:	3d 60 00 00 	lis     r11,0
{
	memset(rq, 0, sizeof(*rq));

	INIT_LIST_HEAD(&rq->queuelist);
	INIT_LIST_HEAD(&rq->timeout_list);
	rq->cpu = -1;
     fa4:	38 00 ff ff 	li      r0,-1
void blk_rq_init(struct request_queue *q, struct request *rq)
{
	memset(rq, 0, sizeof(*rq));

	INIT_LIST_HEAD(&rq->queuelist);
	INIT_LIST_HEAD(&rq->timeout_list);
     fa8:	39 3f 00 b8 	addi    r9,r31,184
	rq->cpu = -1;
	rq->q = q;
	rq->__sector = (sector_t) -1;
	INIT_HLIST_NODE(&rq->hash);
	RB_CLEAR_NODE(&rq->rb_node);
	rq->cmd = rq->__cmd;
     fac:	39 5f 00 8c 	addi    r10,r31,140
{
	memset(rq, 0, sizeof(*rq));

	INIT_LIST_HEAD(&rq->queuelist);
	INIT_LIST_HEAD(&rq->timeout_list);
	rq->cpu = -1;
     fb0:	90 1f 00 2c 	stw     r0,44(r31)
	rq->__sector = (sector_t) -1;
	INIT_HLIST_NODE(&rq->hash);
	RB_CLEAR_NODE(&rq->rb_node);
	rq->cmd = rq->__cmd;
	rq->cmd_len = BLK_MAX_CDB;
	rq->tag = -1;
     fb4:	90 1f 00 84 	stw     r0,132(r31)
	rq->ref_count = 1;
     fb8:	38 00 00 01 	li      r0,1
#define rb_set_red(r)  do { (r)->rb_parent_color &= ~1; } while (0)
#define rb_set_black(r)  do { (r)->rb_parent_color |= 1; } while (0)

static inline void rb_set_parent(struct rb_node *rb, struct rb_node *p)
{
	rb->rb_parent_color = (rb->rb_parent_color & 3) | (unsigned long)p;
     fbc:	39 1f 00 50 	addi    r8,r31,80
	rq->start_time = jiffies;
     fc0:	81 6b 00 00 	lwz     r11,0(r11)
     fc4:	91 3f 00 b8 	stw     r9,184(r31)
	list->prev = list;
     fc8:	91 3f 00 bc 	stw     r9,188(r31)
	rq->q = q;
	rq->__sector = (sector_t) -1;
	INIT_HLIST_NODE(&rq->hash);
	RB_CLEAR_NODE(&rq->rb_node);
	rq->cmd = rq->__cmd;
	rq->cmd_len = BLK_MAX_CDB;
     fcc:	39 20 00 10 	li      r9,16
	rq->cpu = -1;
	rq->q = q;
	rq->__sector = (sector_t) -1;
	INIT_HLIST_NODE(&rq->hash);
	RB_CLEAR_NODE(&rq->rb_node);
	rq->cmd = rq->__cmd;
     fd0:	91 5f 00 9c 	stw     r10,156(r31)

	INIT_LIST_HEAD(&rq->queuelist);
	INIT_LIST_HEAD(&rq->timeout_list);
	rq->cpu = -1;
	rq->q = q;
	rq->__sector = (sector_t) -1;
     fd4:	39 40 ff ff 	li      r10,-1
	INIT_HLIST_NODE(&rq->hash);
	RB_CLEAR_NODE(&rq->rb_node);
	rq->cmd = rq->__cmd;
	rq->cmd_len = BLK_MAX_CDB;
	rq->tag = -1;
	rq->ref_count = 1;
     fd8:	90 1f 00 78 	stw     r0,120(r31)
	rq->start_time = jiffies;
	set_start_time_ns(rq);
	rq->part = NULL;
     fdc:	38 00 00 00 	li      r0,0
	RB_CLEAR_NODE(&rq->rb_node);
	rq->cmd = rq->__cmd;
	rq->cmd_len = BLK_MAX_CDB;
	rq->tag = -1;
	rq->ref_count = 1;
	rq->start_time = jiffies;
     fe0:	91 7f 00 70 	stw     r11,112(r31)

	INIT_LIST_HEAD(&rq->queuelist);
	INIT_LIST_HEAD(&rq->timeout_list);
	rq->cpu = -1;
	rq->q = q;
	rq->__sector = (sector_t) -1;
     fe4:	39 60 ff ff 	li      r11,-1
     fe8:	93 ff 00 04 	stw     r31,4(r31)
	memset(rq, 0, sizeof(*rq));

	INIT_LIST_HEAD(&rq->queuelist);
	INIT_LIST_HEAD(&rq->timeout_list);
	rq->cpu = -1;
	rq->q = q;
     fec:	93 df 00 1c 	stw     r30,28(r31)
     ff0:	91 1f 00 50 	stw     r8,80(r31)
	rq->__sector = (sector_t) -1;
	INIT_HLIST_NODE(&rq->hash);
	RB_CLEAR_NODE(&rq->rb_node);
	rq->cmd = rq->__cmd;
	rq->cmd_len = BLK_MAX_CDB;
     ff4:	b1 3f 00 a0 	sth     r9,160(r31)
	rq->tag = -1;
	rq->ref_count = 1;
	rq->start_time = jiffies;
	set_start_time_ns(rq);
	rq->part = NULL;
     ff8:	90 1f 00 6c 	stw     r0,108(r31)

	INIT_LIST_HEAD(&rq->queuelist);
	INIT_LIST_HEAD(&rq->timeout_list);
	rq->cpu = -1;
	rq->q = q;
	rq->__sector = (sector_t) -1;
     ffc:	91 5f 00 38 	stw     r10,56(r31)
    1000:	91 7f 00 3c 	stw     r11,60(r31)
	rq->tag = -1;
	rq->ref_count = 1;
	rq->start_time = jiffies;
	set_start_time_ns(rq);
	rq->part = NULL;
}
    1004:	80 01 00 14 	lwz     r0,20(r1)
    1008:	bb c1 00 08 	lmw     r30,8(r1)
    100c:	38 21 00 10 	addi    r1,r1,16
    1010:	7c 08 03 a6 	mtlr    r0
    1014:	4e 80 00 20 	blr

00001018 <blk_rq_prep_clone>:
 */
int blk_rq_prep_clone(struct request *rq, struct request *rq_src,
		      struct bio_set *bs, gfp_t gfp_mask,
		      int (*bio_ctr)(struct bio *, struct bio *, void *),
		      void *data)
{
    1018:	94 21 ff d0 	stwu    r1,-48(r1)
    101c:	7d 80 00 26 	mfcr    r12
    1020:	7c 08 02 a6 	mflr    r0
    1024:	bf 21 00 14 	stmw    r25,20(r1)
	struct bio *bio, *bio_src;

	if (!bs)
    1028:	7c be 2b 79 	mr.     r30,r5
 */
int blk_rq_prep_clone(struct request *rq, struct request *rq_src,
		      struct bio_set *bs, gfp_t gfp_mask,
		      int (*bio_ctr)(struct bio *, struct bio *, void *),
		      void *data)
{
    102c:	7c 7f 1b 78 	mr      r31,r3
    1030:	90 01 00 34 	stw     r0,52(r1)
    1034:	7c 9d 23 78 	mr      r29,r4
    1038:	7c dc 33 78 	mr      r28,r6
    103c:	91 81 00 10 	stw     r12,16(r1)
    1040:	7d 19 43 78 	mr      r25,r8
    1044:	90 e1 00 08 	stw     r7,8(r1)
	struct bio *bio, *bio_src;

	if (!bs)
    1048:	41 82 01 50 	beq-    1198 <blk_rq_prep_clone+0x180>
		bs = fs_bio_set;

	blk_rq_init(NULL, rq);
    104c:	38 60 00 00 	li      r3,0
    1050:	7f e4 fb 78 	mr      r4,r31
    1054:	48 00 00 01 	bl      1054 <blk_rq_prep_clone+0x3c>

	__rq_for_each_bio(bio_src, rq_src) {
    1058:	83 7d 00 40 	lwz     r27,64(r29)
    105c:	2f 9b 00 00 	cmpwi   cr7,r27,0
    1060:	41 9e 00 8c 	beq-    cr7,10ec <blk_rq_prep_clone+0xd4>

		if (bio_integrity(bio_src) &&
		    bio_integrity_clone(bio, bio_src, gfp_mask, bs))
			goto free_and_out;

		if (bio_ctr && bio_ctr(bio, bio_src, data))
    1064:	80 01 00 08 	lwz     r0,8(r1)
    1068:	2e 00 00 00 	cmpwi   cr4,r0,0
    106c:	48 00 00 1c 	b       1088 <blk_rq_prep_clone+0x70>
			goto free_and_out;

		if (rq->bio) {
			rq->biotail->bi_next = bio;
    1070:	81 3f 00 44 	lwz     r9,68(r31)
    1074:	93 49 00 08 	stw     r26,8(r9)
			rq->biotail = bio;
    1078:	93 5f 00 44 	stw     r26,68(r31)
	if (!bs)
		bs = fs_bio_set;

	blk_rq_init(NULL, rq);

	__rq_for_each_bio(bio_src, rq_src) {
    107c:	83 7b 00 08 	lwz     r27,8(r27)
    1080:	2f 9b 00 00 	cmpwi   cr7,r27,0
    1084:	41 9e 00 68 	beq-    cr7,10ec <blk_rq_prep_clone+0xd4>
		bio = bio_alloc_bioset(gfp_mask, bio_src->bi_max_vecs, bs);
    1088:	80 9b 00 2c 	lwz     r4,44(r27)
    108c:	7f c5 f3 78 	mr      r5,r30
    1090:	7f 83 e3 78 	mr      r3,r28
    1094:	48 00 00 01 	bl      1094 <blk_rq_prep_clone+0x7c>
		if (!bio)
			goto free_and_out;

		__bio_clone(bio, bio_src);
    1098:	7f 64 db 78 	mr      r4,r27

	blk_rq_init(NULL, rq);

	__rq_for_each_bio(bio_src, rq_src) {
		bio = bio_alloc_bioset(gfp_mask, bio_src->bi_max_vecs, bs);
		if (!bio)
    109c:	7c 7a 1b 79 	mr.     r26,r3
    10a0:	41 82 00 d0 	beq-    1170 <blk_rq_prep_clone+0x158>
			goto free_and_out;

		__bio_clone(bio, bio_src);
    10a4:	48 00 00 01 	bl      10a4 <blk_rq_prep_clone+0x8c>

		if (bio_integrity(bio_src) &&
		    bio_integrity_clone(bio, bio_src, gfp_mask, bs))
			goto free_and_out;

		if (bio_ctr && bio_ctr(bio, bio_src, data))
    10a8:	7f 64 db 78 	mr      r4,r27
    10ac:	7f 43 d3 78 	mr      r3,r26
    10b0:	7f 25 cb 78 	mr      r5,r25
    10b4:	41 92 00 18 	beq-    cr4,10cc <blk_rq_prep_clone+0xb4>
    10b8:	80 01 00 08 	lwz     r0,8(r1)
    10bc:	7c 09 03 a6 	mtctr   r0
    10c0:	4e 80 04 21 	bctrl
    10c4:	2f 83 00 00 	cmpwi   cr7,r3,0
    10c8:	40 9e 00 9c 	bne-    cr7,1164 <blk_rq_prep_clone+0x14c>
			goto free_and_out;

		if (rq->bio) {
    10cc:	80 1f 00 40 	lwz     r0,64(r31)
    10d0:	2f 80 00 00 	cmpwi   cr7,r0,0
    10d4:	40 9e ff 9c 	bne+    cr7,1070 <blk_rq_prep_clone+0x58>
			rq->biotail->bi_next = bio;
			rq->biotail = bio;
		} else
			rq->bio = rq->biotail = bio;
    10d8:	93 5f 00 44 	stw     r26,68(r31)
    10dc:	93 5f 00 40 	stw     r26,64(r31)
	if (!bs)
		bs = fs_bio_set;

	blk_rq_init(NULL, rq);

	__rq_for_each_bio(bio_src, rq_src) {
    10e0:	83 7b 00 08 	lwz     r27,8(r27)
    10e4:	2f 9b 00 00 	cmpwi   cr7,r27,0
    10e8:	40 9e ff a0 	bne+    cr7,1088 <blk_rq_prep_clone+0x70>
 * Copy attributes of the original request to the clone request.
 * The actual data parts (e.g. ->cmd, ->buffer, ->sense) are not copied.
 */
static void __blk_rq_prep_clone(struct request *dst, struct request *src)
{
	dst->cpu = src->cpu;
    10ec:	80 1d 00 2c 	lwz     r0,44(r29)
			rq->bio = rq->biotail = bio;
	}

	__blk_rq_prep_clone(rq, rq_src);

	return 0;
    10f0:	38 60 00 00 	li      r3,0
 * Copy attributes of the original request to the clone request.
 * The actual data parts (e.g. ->cmd, ->buffer, ->sense) are not copied.
 */
static void __blk_rq_prep_clone(struct request *dst, struct request *src)
{
	dst->cpu = src->cpu;
    10f4:	90 1f 00 2c 	stw     r0,44(r31)
	dst->cmd_flags = (src->cmd_flags & REQ_CLONE_MASK) | REQ_NOMERGE;
    10f8:	3c 00 01 00 	lis     r0,256
    10fc:	60 00 20 ff 	ori     r0,r0,8447
    1100:	81 3d 00 20 	lwz     r9,32(r29)
    1104:	7c 00 48 38 	and     r0,r0,r9
    1108:	60 00 40 00 	ori     r0,r0,16384
    110c:	90 1f 00 20 	stw     r0,32(r31)
	dst->cmd_type = src->cmd_type;
    1110:	80 1d 00 24 	lwz     r0,36(r29)
    1114:	90 1f 00 24 	stw     r0,36(r31)

	blk_requestq_cachep = kmem_cache_create("blkdev_queue",
			sizeof(struct request_queue), 0, SLAB_PANIC, NULL);

	return 0;
}
    1118:	81 5d 00 38 	lwz     r10,56(r29)
    111c:	81 7d 00 3c 	lwz     r11,60(r29)
static void __blk_rq_prep_clone(struct request *dst, struct request *src)
{
	dst->cpu = src->cpu;
	dst->cmd_flags = (src->cmd_flags & REQ_CLONE_MASK) | REQ_NOMERGE;
	dst->cmd_type = src->cmd_type;
	dst->__sector = blk_rq_pos(src);
    1120:	91 5f 00 38 	stw     r10,56(r31)
    1124:	91 7f 00 3c 	stw     r11,60(r31)

	blk_requestq_cachep = kmem_cache_create("blkdev_queue",
			sizeof(struct request_queue), 0, SLAB_PANIC, NULL);

	return 0;
}
    1128:	80 1d 00 30 	lwz     r0,48(r29)
{
	dst->cpu = src->cpu;
	dst->cmd_flags = (src->cmd_flags & REQ_CLONE_MASK) | REQ_NOMERGE;
	dst->cmd_type = src->cmd_type;
	dst->__sector = blk_rq_pos(src);
	dst->__data_len = blk_rq_bytes(src);
    112c:	90 1f 00 30 	stw     r0,48(r31)
	dst->nr_phys_segments = src->nr_phys_segments;
    1130:	a0 1d 00 74 	lhz     r0,116(r29)
    1134:	b0 1f 00 74 	sth     r0,116(r31)
	dst->ioprio = src->ioprio;
    1138:	a0 1d 00 76 	lhz     r0,118(r29)
    113c:	b0 1f 00 76 	sth     r0,118(r31)
	dst->extra_len = src->extra_len;
    1140:	80 1d 00 a4 	lwz     r0,164(r29)
    1144:	90 1f 00 a4 	stw     r0,164(r31)
	if (bio)
		bio_free(bio, bs);
	blk_rq_unprep_clone(rq);

	return -ENOMEM;
}
    1148:	80 01 00 34 	lwz     r0,52(r1)
    114c:	81 81 00 10 	lwz     r12,16(r1)
    1150:	7c 08 03 a6 	mtlr    r0
    1154:	bb 21 00 14 	lmw     r25,20(r1)
    1158:	38 21 00 30 	addi    r1,r1,48
    115c:	7d 80 81 20 	mtcrf   8,r12
    1160:	4e 80 00 20 	blr

	return 0;

free_and_out:
	if (bio)
		bio_free(bio, bs);
    1164:	7f 43 d3 78 	mr      r3,r26
    1168:	7f c4 f3 78 	mr      r4,r30
    116c:	48 00 00 01 	bl      116c <blk_rq_prep_clone+0x154>
	blk_rq_unprep_clone(rq);
    1170:	7f e3 fb 78 	mr      r3,r31
    1174:	48 00 00 01 	bl      1174 <blk_rq_prep_clone+0x15c>

	return -ENOMEM;
}
    1178:	80 01 00 34 	lwz     r0,52(r1)
free_and_out:
	if (bio)
		bio_free(bio, bs);
	blk_rq_unprep_clone(rq);

	return -ENOMEM;
    117c:	38 60 ff f4 	li      r3,-12
}
    1180:	81 81 00 10 	lwz     r12,16(r1)
    1184:	7c 08 03 a6 	mtlr    r0
    1188:	bb 21 00 14 	lmw     r25,20(r1)
    118c:	38 21 00 30 	addi    r1,r1,48
    1190:	7d 80 81 20 	mtcrf   8,r12
    1194:	4e 80 00 20 	blr
		      void *data)
{
	struct bio *bio, *bio_src;

	if (!bs)
		bs = fs_bio_set;
    1198:	3d 20 00 00 	lis     r9,0
    119c:	83 c9 00 00 	lwz     r30,0(r9)
    11a0:	4b ff fe ac 	b       104c <blk_rq_prep_clone+0x34>

000011a4 <generic_make_request>:
 * If it is NULL, then no make_request is active.  If it is non-NULL,
 * then a make_request is active, and new requests should be added
 * at the tail
 */
void generic_make_request(struct bio *bio)
{
    11a4:	94 21 ff 90 	stwu    r1,-112(r1)
    11a8:	7d 80 00 26 	mfcr    r12
    11ac:	7c 08 02 a6 	mflr    r0
    11b0:	be 81 00 40 	stmw    r20,64(r1)
    11b4:	7c 7f 1b 78 	mr      r31,r3
    11b8:	90 01 00 74 	stw     r0,116(r1)
    11bc:	91 81 00 3c 	stw     r12,60(r1)
	struct bio_list bio_list_on_stack;

	if (current->bio_list) {
    11c0:	81 22 04 24 	lwz     r9,1060(r2)
    11c4:	2f 89 00 00 	cmpwi   cr7,r9,0
    11c8:	41 9e 00 40 	beq-    cr7,1208 <generic_make_request+0x64>
	return sz;
}

static inline void bio_list_add(struct bio_list *bl, struct bio *bio)
{
	bio->bi_next = NULL;
    11cc:	38 00 00 00 	li      r0,0
    11d0:	90 03 00 08 	stw     r0,8(r3)

	if (bl->tail)
    11d4:	81 69 00 04 	lwz     r11,4(r9)
    11d8:	2f 8b 00 00 	cmpwi   cr7,r11,0
    11dc:	41 9e 02 a0 	beq-    cr7,147c <generic_make_request+0x2d8>
		bl->tail->bi_next = bio;
    11e0:	90 6b 00 08 	stw     r3,8(r11)
	else
		bl->head = bio;

	bl->tail = bio;
    11e4:	93 e9 00 04 	stw     r31,4(r9)
	do {
		__generic_make_request(bio);
		bio = bio_list_pop(current->bio_list);
	} while (bio);
	current->bio_list = NULL; /* deactivate */
}
    11e8:	80 01 00 74 	lwz     r0,116(r1)
    11ec:	81 81 00 3c 	lwz     r12,60(r1)
    11f0:	7c 08 03 a6 	mtlr    r0
    11f4:	ba 81 00 40 	lmw     r20,64(r1)
    11f8:	38 21 00 70 	addi    r1,r1,112
    11fc:	7d 81 01 20 	mtcrf   16,r12
    1200:	7d 80 81 20 	mtcrf   8,r12
    1204:	4e 80 00 20 	blr
	 *
	 * The loop was structured like this to make only one call to
	 * __generic_make_request (which is important as it is large and
	 * inlined) and to keep the structure simple.
	 */
	BUG_ON(bio->bi_next);
    1208:	80 03 00 08 	lwz     r0,8(r3)
    120c:	0f 00 00 00 	twnei   r0,0
	return bl->head == NULL;
}

static inline void bio_list_init(struct bio_list *bl)
{
	bl->head = bl->tail = NULL;
    1210:	38 00 00 00 	li      r0,0
    1214:	7c 29 0b 78 	mr      r9,r1
    1218:	94 09 00 08 	stwu    r0,8(r9)
		/*
		 * Filter flush bio's early so that make_request based
		 * drivers without flush support don't have to worry
		 * about them.
		 */
		if ((bio->bi_rw & (REQ_FLUSH | REQ_FUA)) && !q->flush_flags) {
    121c:	3f c0 01 00 	lis     r30,256
			bio->bi_rw &= ~(REQ_FLUSH | REQ_FUA);
    1220:	3f 20 fe ff 	lis     r25,-257
    1224:	90 01 00 0c 	stw     r0,12(r1)
	do {
		char b[BDEVNAME_SIZE];

		q = bdev_get_queue(bio->bi_bdev);
		if (unlikely(!q)) {
			printk(KERN_ERR
    1228:	3e a0 00 00 	lis     r21,0
			goto end_io;
		}

		if (unlikely(!(bio->bi_rw & REQ_DISCARD) &&
			     nr_sectors > queue_max_hw_sectors(q))) {
			printk(KERN_ERR "bio too big device %s (%u > %u)\n",
    122c:	3e 80 00 00 	lis     r20,0
		/*
		 * Filter flush bio's early so that make_request based
		 * drivers without flush support don't have to worry
		 * about them.
		 */
		if ((bio->bi_rw & (REQ_FLUSH | REQ_FUA)) && !q->flush_flags) {
    1230:	63 de 20 00 	ori     r30,r30,8192
			bio->bi_rw &= ~(REQ_FLUSH | REQ_FUA);
    1234:	63 39 df ff 	ori     r25,r25,57343
	 * __generic_make_request (which is important as it is large and
	 * inlined) and to keep the structure simple.
	 */
	BUG_ON(bio->bi_next);
	bio_list_init(&bio_list_on_stack);
	current->bio_list = &bio_list_on_stack;
    1238:	91 22 04 24 	stw     r9,1060(r2)
	do {
		char b[BDEVNAME_SIZE];

		q = bdev_get_queue(bio->bi_bdev);
		if (unlikely(!q)) {
			printk(KERN_ERR
    123c:	3a b5 01 a4 	addi    r21,r21,420
		 * about them.
		 */
		if ((bio->bi_rw & (REQ_FLUSH | REQ_FUA)) && !q->flush_flags) {
			bio->bi_rw &= ~(REQ_FLUSH | REQ_FUA);
			if (!nr_sectors) {
				err = 0;
    1240:	3b 00 00 00 	li      r24,0
			goto end_io;
		}

		if (unlikely(!(bio->bi_rw & REQ_DISCARD) &&
			     nr_sectors > queue_max_hw_sectors(q))) {
			printk(KERN_ERR "bio too big device %s (%u > %u)\n",
    1244:	3a 94 01 f4 	addi    r20,r20,500
 */
static inline void __generic_make_request(struct bio *bio)
{
	struct request_queue *q;
	sector_t old_sector;
	int ret, nr_sectors = bio_sectors(bio);
    1248:	83 bf 00 20 	lwz     r29,32(r31)
			/*
			 * This may well happen - the kernel calls bread()
			 * without checking the size of the device, e.g., when
			 * mounting a device.
			 */
			handle_bad_sector(bio);
    124c:	80 7f 00 0c 	lwz     r3,12(r31)
 */
static inline int bio_check_eod(struct bio *bio, unsigned int nr_sectors)
{
	sector_t maxsector;

	if (!nr_sectors)
    1250:	57 bd ba 7e 	rlwinm  r29,r29,23,9,31
    1254:	2e 1d 00 00 	cmpwi   cr4,r29,0
    1258:	41 92 00 54 	beq-    cr4,12ac <generic_make_request+0x108>

	blk_requestq_cachep = kmem_cache_create("blkdev_queue",
			sizeof(struct request_queue), 0, SLAB_PANIC, NULL);

	return 0;
}
    125c:	81 23 00 04 	lwz     r9,4(r3)

	if (!nr_sectors)
		return 0;

	/* Test device or partition size, when known. */
	maxsector = i_size_read(bio->bi_bdev->bd_inode) >> 9;
    1260:	81 09 00 70 	lwz     r8,112(r9)
    1264:	81 29 00 74 	lwz     r9,116(r9)
    1268:	55 2b ba 7e 	rlwinm  r11,r9,23,9,31
    126c:	51 0b b8 10 	rlwimi  r11,r8,23,0,8
    1270:	7d 0a 4e 70 	srawi   r10,r8,9
	if (maxsector) {
    1274:	7d 40 5b 79 	or.     r0,r10,r11
    1278:	41 82 00 34 	beq-    12ac <generic_make_request+0x108>
		sector_t sector = bio->bi_sector;

		if (maxsector < nr_sectors || maxsector - nr_sectors < sector) {
    127c:	3a c0 00 00 	li      r22,0
    1280:	7f b7 eb 78 	mr      r23,r29
		return 0;

	/* Test device or partition size, when known. */
	maxsector = i_size_read(bio->bi_bdev->bd_inode) >> 9;
	if (maxsector) {
		sector_t sector = bio->bi_sector;
    1284:	80 1f 00 00 	lwz     r0,0(r31)

		if (maxsector < nr_sectors || maxsector - nr_sectors < sector) {
    1288:	7f 96 50 00 	cmpw    cr7,r22,r10
		return 0;

	/* Test device or partition size, when known. */
	maxsector = i_size_read(bio->bi_bdev->bd_inode) >> 9;
	if (maxsector) {
		sector_t sector = bio->bi_sector;
    128c:	81 3f 00 04 	lwz     r9,4(r31)

		if (maxsector < nr_sectors || maxsector - nr_sectors < sector) {
    1290:	41 9e 01 f4 	beq-    cr7,1484 <generic_make_request+0x2e0>
    1294:	7d 77 58 10 	subfc   r11,r23,r11
    1298:	7d 56 51 10 	subfe   r10,r22,r10
    129c:	7f 80 50 40 	cmplw   cr7,r0,r10
    12a0:	41 9d 00 dc 	bgt-    cr7,137c <generic_make_request+0x1d8>
    12a4:	7f 80 50 00 	cmpw    cr7,r0,r10
    12a8:	41 9e 01 e8 	beq-    cr7,1490 <generic_make_request+0x2ec>

		/*
		 * If bio = NULL, bio has been throttled and will be submitted
		 * later.
		 */
		if (!bio)
    12ac:	2d 9f 00 00 	cmpwi   cr3,r31,0
				  struct request *, int, rq_end_io_fn *);
extern void blk_unplug(struct request_queue *q);

static inline struct request_queue *bdev_get_queue(struct block_device *bdev)
{
	return bdev->bd_disk->queue;
    12b0:	81 23 00 50 	lwz     r9,80(r3)
    12b4:	83 89 01 6c 	lwz     r28,364(r9)
	old_dev = 0;
	do {
		char b[BDEVNAME_SIZE];

		q = bdev_get_queue(bio->bi_bdev);
		if (unlikely(!q)) {
    12b8:	2f 9c 00 00 	cmpwi   cr7,r28,0
    12bc:	41 9e 02 24 	beq-    cr7,14e0 <generic_make_request+0x33c>
				bdevname(bio->bi_bdev, b),
				(long long) bio->bi_sector);
			goto end_io;
		}

		if (unlikely(!(bio->bi_rw & REQ_DISCARD) &&
    12c0:	80 1f 00 14 	lwz     r0,20(r31)
    12c4:	70 09 00 40 	andi.   r9,r0,64
    12c8:	41 82 01 e4 	beq-    14ac <generic_make_request+0x308>
 * @nr: bit number to test
 * @addr: Address to start counting from
 */
static inline int test_bit(int nr, const volatile unsigned long *addr)
{
	return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    12cc:	81 3c 01 78 	lwz     r9,376(r28)
			       bio_sectors(bio),
			       queue_max_hw_sectors(q));
			goto end_io;
		}

		if (unlikely(test_bit(QUEUE_FLAG_DEAD, &q->queue_flags)))
    12d0:	71 2b 00 20 	andi.   r11,r9,32
    12d4:	40 82 01 d0 	bne-    14a4 <generic_make_request+0x300>
 */
static inline void blk_partition_remap(struct bio *bio)
{
	struct block_device *bdev = bio->bi_bdev;

	if (bio_sectors(bio) && bdev != bdev->bd_contains) {
    12d8:	81 3f 00 20 	lwz     r9,32(r31)
    12dc:	55 2b ba 7f 	rlwinm. r11,r9,23,9,31
    12e0:	41 82 00 3c 	beq-    131c <generic_make_request+0x178>
    12e4:	81 23 00 3c 	lwz     r9,60(r3)
    12e8:	7f 89 18 00 	cmpw    cr7,r9,r3
    12ec:	41 9e 00 30 	beq-    cr7,131c <generic_make_request+0x178>
		struct hd_struct *p = bdev->bd_part;

		bio->bi_sector += p->start_sect;
    12f0:	81 63 00 44 	lwz     r11,68(r3)
    12f4:	81 3f 00 04 	lwz     r9,4(r31)
    12f8:	81 4b 00 00 	lwz     r10,0(r11)
    12fc:	81 1f 00 00 	lwz     r8,0(r31)
    1300:	81 6b 00 04 	lwz     r11,4(r11)
    1304:	7d 6b 48 14 	addc    r11,r11,r9
    1308:	7d 4a 41 14 	adde    r10,r10,r8
    130c:	91 5f 00 00 	stw     r10,0(r31)
    1310:	91 7f 00 04 	stw     r11,4(r31)
		bio->bi_bdev = bdev->bd_contains;
    1314:	81 23 00 3c 	lwz     r9,60(r3)
    1318:	91 3f 00 0c 	stw     r9,12(r31)
 */
static inline int bio_check_eod(struct bio *bio, unsigned int nr_sectors)
{
	sector_t maxsector;

	if (!nr_sectors)
    131c:	41 92 00 a8 	beq-    cr4,13c4 <generic_make_request+0x220>
		return 0;

	/* Test device or partition size, when known. */
	maxsector = i_size_read(bio->bi_bdev->bd_inode) >> 9;
    1320:	81 3f 00 0c 	lwz     r9,12(r31)

	blk_requestq_cachep = kmem_cache_create("blkdev_queue",
			sizeof(struct request_queue), 0, SLAB_PANIC, NULL);

	return 0;
}
    1324:	81 29 00 04 	lwz     r9,4(r9)

	if (!nr_sectors)
		return 0;

	/* Test device or partition size, when known. */
	maxsector = i_size_read(bio->bi_bdev->bd_inode) >> 9;
    1328:	81 09 00 70 	lwz     r8,112(r9)
    132c:	81 29 00 74 	lwz     r9,116(r9)
    1330:	55 2b ba 7e 	rlwinm  r11,r9,23,9,31
    1334:	51 0b b8 10 	rlwimi  r11,r8,23,0,8
    1338:	7d 0a 4e 70 	srawi   r10,r8,9
	if (maxsector) {
    133c:	7d 49 5b 79 	or.     r9,r10,r11
    1340:	41 82 00 84 	beq-    13c4 <generic_make_request+0x220>
		sector_t sector = bio->bi_sector;

		if (maxsector < nr_sectors || maxsector - nr_sectors < sector) {
    1344:	3b 40 00 00 	li      r26,0
    1348:	7f bb eb 78 	mr      r27,r29
		return 0;

	/* Test device or partition size, when known. */
	maxsector = i_size_read(bio->bi_bdev->bd_inode) >> 9;
	if (maxsector) {
		sector_t sector = bio->bi_sector;
    134c:	81 3f 00 00 	lwz     r9,0(r31)

		if (maxsector < nr_sectors || maxsector - nr_sectors < sector) {
    1350:	7f 9a 50 00 	cmpw    cr7,r26,r10
		return 0;

	/* Test device or partition size, when known. */
	maxsector = i_size_read(bio->bi_bdev->bd_inode) >> 9;
	if (maxsector) {
		sector_t sector = bio->bi_sector;
    1354:	81 1f 00 04 	lwz     r8,4(r31)

		if (maxsector < nr_sectors || maxsector - nr_sectors < sector) {
    1358:	41 9e 00 f4 	beq-    cr7,144c <generic_make_request+0x2a8>
    135c:	7d 7b 58 10 	subfc   r11,r27,r11
    1360:	7d 5a 51 10 	subfe   r10,r26,r10
    1364:	7f 89 50 40 	cmplw   cr7,r9,r10
    1368:	41 9d 00 14 	bgt-    cr7,137c <generic_make_request+0x1d8>
    136c:	7f 89 50 00 	cmpw    cr7,r9,r10
    1370:	40 9e 00 54 	bne-    cr7,13c4 <generic_make_request+0x220>
    1374:	7f 88 58 40 	cmplw   cr7,r8,r11
    1378:	40 9d 00 4c 	ble-    cr7,13c4 <generic_make_request+0x220>
			/*
			 * This may well happen - the kernel calls bread()
			 * without checking the size of the device, e.g., when
			 * mounting a device.
			 */
			handle_bad_sector(bio);
    137c:	7f e3 fb 78 	mr      r3,r31
    1380:	4b ff f8 b9 	bl      c38 <handle_bad_sector>
{
	struct request_queue *q;
	sector_t old_sector;
	int ret, nr_sectors = bio_sectors(bio);
	dev_t old_dev;
	int err = -EIO;
    1384:	38 80 ff fb 	li      r4,-5
	} while (ret);

	return;

end_io:
	bio_endio(bio, err);
    1388:	7f e3 fb 78 	mr      r3,r31
    138c:	48 00 00 01 	bl      138c <generic_make_request+0x1e8>
	BUG_ON(bio->bi_next);
	bio_list_init(&bio_list_on_stack);
	current->bio_list = &bio_list_on_stack;
	do {
		__generic_make_request(bio);
		bio = bio_list_pop(current->bio_list);
    1390:	81 22 04 24 	lwz     r9,1060(r2)
	return bl->head;
}

static inline struct bio *bio_list_pop(struct bio_list *bl)
{
	struct bio *bio = bl->head;
    1394:	83 e9 00 00 	lwz     r31,0(r9)

	if (bio) {
    1398:	2f 9f 00 00 	cmpwi   cr7,r31,0
    139c:	41 9e 00 bc 	beq-    cr7,1458 <generic_make_request+0x2b4>
		bl->head = bl->head->bi_next;
    13a0:	80 1f 00 08 	lwz     r0,8(r31)
		if (!bl->head)
    13a4:	2f 80 00 00 	cmpwi   cr7,r0,0
static inline struct bio *bio_list_pop(struct bio_list *bl)
{
	struct bio *bio = bl->head;

	if (bio) {
		bl->head = bl->head->bi_next;
    13a8:	90 09 00 00 	stw     r0,0(r9)
		if (!bl->head)
    13ac:	41 9e 00 0c 	beq-    cr7,13b8 <generic_make_request+0x214>
			bl->tail = NULL;

		bio->bi_next = NULL;
    13b0:	93 1f 00 08 	stw     r24,8(r31)
    13b4:	4b ff fe 94 	b       1248 <generic_make_request+0xa4>
	struct bio *bio = bl->head;

	if (bio) {
		bl->head = bl->head->bi_next;
		if (!bl->head)
			bl->tail = NULL;
    13b8:	90 09 00 04 	stw     r0,4(r9)

		bio->bi_next = NULL;
    13bc:	93 1f 00 08 	stw     r24,8(r31)
    13c0:	4b ff fe 88 	b       1248 <generic_make_request+0xa4>
		/*
		 * Filter flush bio's early so that make_request based
		 * drivers without flush support don't have to worry
		 * about them.
		 */
		if ((bio->bi_rw & (REQ_FLUSH | REQ_FUA)) && !q->flush_flags) {
    13c4:	7c 0b f0 39 	and.    r11,r0,r30
    13c8:	7c 09 03 78 	mr      r9,r0
    13cc:	41 82 00 1c 	beq-    13e8 <generic_make_request+0x244>
    13d0:	81 7c 02 4c 	lwz     r11,588(r28)
    13d4:	2f 8b 00 00 	cmpwi   cr7,r11,0
    13d8:	40 9e 00 10 	bne-    cr7,13e8 <generic_make_request+0x244>
			bio->bi_rw &= ~(REQ_FLUSH | REQ_FUA);
    13dc:	7c 09 c8 38 	and     r9,r0,r25
    13e0:	91 3f 00 14 	stw     r9,20(r31)
			if (!nr_sectors) {
    13e4:	41 92 00 b8 	beq-    cr4,149c <generic_make_request+0x2f8>
				err = 0;
				goto end_io;
			}
		}

		if ((bio->bi_rw & REQ_DISCARD) &&
    13e8:	71 20 00 40 	andi.   r0,r9,64
    13ec:	41 82 00 30 	beq-    141c <generic_make_request+0x278>
    13f0:	80 1c 01 78 	lwz     r0,376(r28)
    13f4:	74 0b 00 01 	andis.  r11,r0,1
    13f8:	41 82 00 4c 	beq-    1444 <generic_make_request+0x2a0>
		    (!blk_queue_discard(q) ||
    13fc:	75 20 08 00 	andis.  r0,r9,2048
    1400:	41 82 00 1c 	beq-    141c <generic_make_request+0x278>
    1404:	80 1c 01 78 	lwz     r0,376(r28)
		     ((bio->bi_rw & REQ_SECURE) &&
    1408:	74 09 00 01 	andis.  r9,r0,1
    140c:	41 82 00 38 	beq-    1444 <generic_make_request+0x2a0>
    1410:	80 1c 01 78 	lwz     r0,376(r28)
		      !blk_queue_secdiscard(q)))) {
    1414:	74 0b 00 08 	andis.  r11,r0,8
    1418:	41 82 00 2c 	beq-    1444 <generic_make_request+0x2a0>

		/*
		 * If bio = NULL, bio has been throttled and will be submitted
		 * later.
		 */
		if (!bio)
    141c:	41 ae ff 74 	beq-    cr3,1390 <generic_make_request+0x1ec>
			break;

		trace_block_bio_queue(q, bio);

		ret = q->make_request_fn(q, bio);
    1420:	80 1c 00 3c 	lwz     r0,60(r28)
    1424:	7f 83 e3 78 	mr      r3,r28
    1428:	7f e4 fb 78 	mr      r4,r31
    142c:	7c 09 03 a6 	mtctr   r0
    1430:	4e 80 04 21 	bctrl
	} while (ret);
    1434:	2f 83 00 00 	cmpwi   cr7,r3,0
    1438:	41 be ff 58 	beq-    cr7,1390 <generic_make_request+0x1ec>
    143c:	80 7f 00 0c 	lwz     r3,12(r31)
    1440:	4b ff fe 70 	b       12b0 <generic_make_request+0x10c>

		if ((bio->bi_rw & REQ_DISCARD) &&
		    (!blk_queue_discard(q) ||
		     ((bio->bi_rw & REQ_SECURE) &&
		      !blk_queue_secdiscard(q)))) {
			err = -EOPNOTSUPP;
    1444:	38 80 ff a1 	li      r4,-95
    1448:	4b ff ff 40 	b       1388 <generic_make_request+0x1e4>
	/* Test device or partition size, when known. */
	maxsector = i_size_read(bio->bi_bdev->bd_inode) >> 9;
	if (maxsector) {
		sector_t sector = bio->bi_sector;

		if (maxsector < nr_sectors || maxsector - nr_sectors < sector) {
    144c:	7f 9d 58 40 	cmplw   cr7,r29,r11
    1450:	40 9d ff 0c 	ble+    cr7,135c <generic_make_request+0x1b8>
    1454:	4b ff ff 28 	b       137c <generic_make_request+0x1d8>
	current->bio_list = &bio_list_on_stack;
	do {
		__generic_make_request(bio);
		bio = bio_list_pop(current->bio_list);
	} while (bio);
	current->bio_list = NULL; /* deactivate */
    1458:	93 e2 04 24 	stw     r31,1060(r2)
}
    145c:	80 01 00 74 	lwz     r0,116(r1)
    1460:	81 81 00 3c 	lwz     r12,60(r1)
    1464:	7c 08 03 a6 	mtlr    r0
    1468:	ba 81 00 40 	lmw     r20,64(r1)
    146c:	38 21 00 70 	addi    r1,r1,112
    1470:	7d 81 01 20 	mtcrf   16,r12
    1474:	7d 80 81 20 	mtcrf   8,r12
    1478:	4e 80 00 20 	blr
	bio->bi_next = NULL;

	if (bl->tail)
		bl->tail->bi_next = bio;
	else
		bl->head = bio;
    147c:	90 69 00 00 	stw     r3,0(r9)
    1480:	4b ff fd 64 	b       11e4 <generic_make_request+0x40>
	/* Test device or partition size, when known. */
	maxsector = i_size_read(bio->bi_bdev->bd_inode) >> 9;
	if (maxsector) {
		sector_t sector = bio->bi_sector;

		if (maxsector < nr_sectors || maxsector - nr_sectors < sector) {
    1484:	7f 9d 58 40 	cmplw   cr7,r29,r11
    1488:	40 9d fe 0c 	ble+    cr7,1294 <generic_make_request+0xf0>
    148c:	4b ff fe f0 	b       137c <generic_make_request+0x1d8>
    1490:	7f 89 58 40 	cmplw   cr7,r9,r11
    1494:	40 bd fe 18 	ble-    cr7,12ac <generic_make_request+0x108>
    1498:	4b ff fe e4 	b       137c <generic_make_request+0x1d8>
		 * about them.
		 */
		if ((bio->bi_rw & (REQ_FLUSH | REQ_FUA)) && !q->flush_flags) {
			bio->bi_rw &= ~(REQ_FLUSH | REQ_FUA);
			if (!nr_sectors) {
				err = 0;
    149c:	38 80 00 00 	li      r4,0
    14a0:	4b ff fe e8 	b       1388 <generic_make_request+0x1e4>
{
	struct request_queue *q;
	sector_t old_sector;
	int ret, nr_sectors = bio_sectors(bio);
	dev_t old_dev;
	int err = -EIO;
    14a4:	38 80 ff fb 	li      r4,-5
    14a8:	4b ff fe e0 	b       1388 <generic_make_request+0x1e4>
				bdevname(bio->bi_bdev, b),
				(long long) bio->bi_sector);
			goto end_io;
		}

		if (unlikely(!(bio->bi_rw & REQ_DISCARD) &&
    14ac:	81 3c 02 0c 	lwz     r9,524(r28)
    14b0:	7f 9d 48 40 	cmplw   cr7,r29,r9
    14b4:	40 9d fe 18 	ble+    cr7,12cc <generic_make_request+0x128>
			     nr_sectors > queue_max_hw_sectors(q))) {
			printk(KERN_ERR "bio too big device %s (%u > %u)\n",
    14b8:	38 81 00 10 	addi    r4,r1,16
    14bc:	48 00 00 01 	bl      14bc <generic_make_request+0x318>
    14c0:	80 bf 00 20 	lwz     r5,32(r31)
    14c4:	80 dc 02 0c 	lwz     r6,524(r28)
    14c8:	7c 64 1b 78 	mr      r4,r3
    14cc:	7e 83 a3 78 	mr      r3,r20
    14d0:	54 a5 ba 7e 	rlwinm  r5,r5,23,9,31
    14d4:	48 00 00 01 	bl      14d4 <generic_make_request+0x330>
{
	struct request_queue *q;
	sector_t old_sector;
	int ret, nr_sectors = bio_sectors(bio);
	dev_t old_dev;
	int err = -EIO;
    14d8:	38 80 ff fb 	li      r4,-5
    14dc:	4b ff fe ac 	b       1388 <generic_make_request+0x1e4>
	do {
		char b[BDEVNAME_SIZE];

		q = bdev_get_queue(bio->bi_bdev);
		if (unlikely(!q)) {
			printk(KERN_ERR
    14e0:	38 81 00 10 	addi    r4,r1,16
    14e4:	48 00 00 01 	bl      14e4 <generic_make_request+0x340>
    14e8:	80 bf 00 00 	lwz     r5,0(r31)
    14ec:	80 df 00 04 	lwz     r6,4(r31)
    14f0:	7c 64 1b 78 	mr      r4,r3
    14f4:	7e a3 ab 78 	mr      r3,r21
    14f8:	48 00 00 01 	bl      14f8 <generic_make_request+0x354>
{
	struct request_queue *q;
	sector_t old_sector;
	int ret, nr_sectors = bio_sectors(bio);
	dev_t old_dev;
	int err = -EIO;
    14fc:	38 80 ff fb 	li      r4,-5
    1500:	4b ff fe 88 	b       1388 <generic_make_request+0x1e4>

00001504 <submit_bio>:
 * uses that function to do most of the work. Both are fairly rough
 * interfaces; @bio must be presetup and ready for I/O.
 *
 */
void submit_bio(int rw, struct bio *bio)
{
    1504:	94 21 ff b0 	stwu    r1,-80(r1)
    1508:	7c 08 02 a6 	mflr    r0
/*
 * Check whether this bio carries any data or not. A NULL bio is allowed.
 */
static inline int bio_has_data(struct bio *bio)
{
	return bio && bio->bi_io_vec != NULL;
    150c:	2f 84 00 00 	cmpwi   cr7,r4,0
    1510:	bf c1 00 48 	stmw    r30,72(r1)
    1514:	7c 9f 23 78 	mr      r31,r4
    1518:	90 01 00 54 	stw     r0,84(r1)
	int count = bio_sectors(bio);

	bio->bi_rw |= rw;
    151c:	80 04 00 14 	lwz     r0,20(r4)
 * interfaces; @bio must be presetup and ready for I/O.
 *
 */
void submit_bio(int rw, struct bio *bio)
{
	int count = bio_sectors(bio);
    1520:	81 44 00 20 	lwz     r10,32(r4)

	bio->bi_rw |= rw;
    1524:	7c 00 1b 78 	or      r0,r0,r3
    1528:	90 04 00 14 	stw     r0,20(r4)
    152c:	41 9e 00 44 	beq-    cr7,1570 <submit_bio+0x6c>
    1530:	80 04 00 38 	lwz     r0,56(r4)
    1534:	2f 80 00 00 	cmpwi   cr7,r0,0
    1538:	41 9e 00 38 	beq-    cr7,1570 <submit_bio+0x6c>

	/*
	 * If it's a regular read/write or a barrier with data attached,
	 * go through the normal accounting stuff before submission.
	 */
	if (bio_has_data(bio) && !(rw & REQ_DISCARD)) {
    153c:	70 60 00 40 	andi.   r0,r3,64
    1540:	40 82 00 30 	bne-    1570 <submit_bio+0x6c>
		if (rw & WRITE) {
    1544:	70 60 00 01 	andi.   r0,r3,1
	__this_cpu_add(vm_event_states.event[item], delta);
}

static inline void count_vm_events(enum vm_event_item item, long delta)
{
	this_cpu_add(vm_event_states.event[item], delta);
    1548:	3d 20 00 00 	lis     r9,0
 * interfaces; @bio must be presetup and ready for I/O.
 *
 */
void submit_bio(int rw, struct bio *bio)
{
	int count = bio_sectors(bio);
    154c:	55 4a ba 7e 	rlwinm  r10,r10,23,9,31
	/*
	 * If it's a regular read/write or a barrier with data attached,
	 * go through the normal accounting stuff before submission.
	 */
	if (bio_has_data(bio) && !(rw & REQ_DISCARD)) {
		if (rw & WRITE) {
    1550:	40 82 00 3c 	bne-    158c <submit_bio+0x88>
    1554:	80 09 00 00 	lwz     r0,0(r9)
    1558:	7c 0a 02 14 	add     r0,r10,r0
    155c:	90 09 00 00 	stw     r0,0(r9)
		} else {
			task_io_account_read(bio->bi_size);
			count_vm_events(PGPGIN, count);
		}

		if (unlikely(block_dump)) {
    1560:	3d 20 00 00 	lis     r9,0
    1564:	80 09 00 00 	lwz     r0,0(r9)
    1568:	2f 80 00 00 	cmpwi   cr7,r0,0
    156c:	40 9e 00 34 	bne-    cr7,15a0 <submit_bio+0x9c>
				bdevname(bio->bi_bdev, b),
				count);
		}
	}

	generic_make_request(bio);
    1570:	7f e3 fb 78 	mr      r3,r31
    1574:	48 00 00 01 	bl      1574 <submit_bio+0x70>
}
    1578:	80 01 00 54 	lwz     r0,84(r1)
    157c:	bb c1 00 48 	lmw     r30,72(r1)
    1580:	38 21 00 50 	addi    r1,r1,80
    1584:	7c 08 03 a6 	mtlr    r0
    1588:	4e 80 00 20 	blr
    158c:	39 29 00 00 	addi    r9,r9,0
    1590:	80 09 00 04 	lwz     r0,4(r9)
    1594:	7c 0a 02 14 	add     r0,r10,r0
    1598:	90 09 00 04 	stw     r0,4(r9)
    159c:	4b ff ff c4 	b       1560 <submit_bio+0x5c>
		}

		if (unlikely(block_dump)) {
			char b[BDEVNAME_SIZE];
			printk(KERN_DEBUG "%s(%d): %s block %Lu on %s (%u sectors)\n",
			current->comm, task_pid_nr(current),
    15a0:	3b c2 01 a4 	addi    r30,r2,420
pid_t __task_pid_nr_ns(struct task_struct *task, enum pid_type type,
			struct pid_namespace *ns);

static inline pid_t task_pid_nr(struct task_struct *tsk)
{
	return tsk->pid;
    15a4:	80 a2 00 c4 	lwz     r5,196(r2)
			count_vm_events(PGPGIN, count);
		}

		if (unlikely(block_dump)) {
			char b[BDEVNAME_SIZE];
			printk(KERN_DEBUG "%s(%d): %s block %Lu on %s (%u sectors)\n",
    15a8:	41 82 00 60 	beq-    1608 <submit_bio+0x104>
    15ac:	3c c0 00 00 	lis     r6,0
    15b0:	38 c6 02 18 	addi    r6,r6,536
    15b4:	80 ff 00 00 	lwz     r7,0(r31)
    15b8:	38 81 00 08 	addi    r4,r1,8
    15bc:	81 1f 00 04 	lwz     r8,4(r31)
    15c0:	80 7f 00 0c 	lwz     r3,12(r31)
    15c4:	90 a1 00 38 	stw     r5,56(r1)
    15c8:	90 c1 00 28 	stw     r6,40(r1)
    15cc:	90 e1 00 30 	stw     r7,48(r1)
    15d0:	91 01 00 34 	stw     r8,52(r1)
    15d4:	91 41 00 2c 	stw     r10,44(r1)
    15d8:	48 00 00 01 	bl      15d8 <submit_bio+0xd4>
    15dc:	7f c4 f3 78 	mr      r4,r30
    15e0:	7c 69 1b 78 	mr      r9,r3
    15e4:	80 a1 00 38 	lwz     r5,56(r1)
    15e8:	3c 60 00 00 	lis     r3,0
    15ec:	80 c1 00 28 	lwz     r6,40(r1)
    15f0:	38 63 02 28 	addi    r3,r3,552
    15f4:	80 e1 00 30 	lwz     r7,48(r1)
    15f8:	81 01 00 34 	lwz     r8,52(r1)
    15fc:	81 41 00 2c 	lwz     r10,44(r1)
    1600:	48 00 00 01 	bl      1600 <submit_bio+0xfc>
    1604:	4b ff ff 6c 	b       1570 <submit_bio+0x6c>
    1608:	3c c0 00 00 	lis     r6,0
    160c:	38 c6 02 20 	addi    r6,r6,544
    1610:	4b ff ff a4 	b       15b4 <submit_bio+0xb0>

00001614 <blk_rq_check_limits>:
 *    the new queue limits again when they dispatch those requests,
 *    although such checkings are also done against the old queue limits
 *    when submitting requests.
 */
int blk_rq_check_limits(struct request_queue *q, struct request *rq)
{
    1614:	94 21 ff e0 	stwu    r1,-32(r1)
    1618:	7c 08 02 a6 	mflr    r0
    161c:	bf a1 00 14 	stmw    r29,20(r1)
    1620:	7c 9f 23 78 	mr      r31,r4
    1624:	7c 7e 1b 78 	mr      r30,r3
    1628:	90 01 00 24 	stw     r0,36(r1)
	if (rq->cmd_flags & REQ_DISCARD)
		return 0;
    162c:	3b a0 00 00 	li      r29,0
 *    although such checkings are also done against the old queue limits
 *    when submitting requests.
 */
int blk_rq_check_limits(struct request_queue *q, struct request *rq)
{
	if (rq->cmd_flags & REQ_DISCARD)
    1630:	80 04 00 20 	lwz     r0,32(r4)
    1634:	70 09 00 40 	andi.   r9,r0,64
    1638:	40 82 00 40 	bne-    1678 <blk_rq_check_limits+0x64>

	blk_requestq_cachep = kmem_cache_create("blkdev_queue",
			sizeof(struct request_queue), 0, SLAB_PANIC, NULL);

	return 0;
}
    163c:	80 04 00 30 	lwz     r0,48(r4)
int blk_rq_check_limits(struct request_queue *q, struct request *rq)
{
	if (rq->cmd_flags & REQ_DISCARD)
		return 0;

	if (blk_rq_sectors(rq) > queue_max_sectors(q) ||
    1640:	81 63 02 10 	lwz     r11,528(r3)

extern unsigned int blk_rq_err_bytes(const struct request *rq);

static inline unsigned int blk_rq_sectors(const struct request *rq)
{
	return blk_rq_bytes(rq) >> 9;
    1644:	54 09 ba 7e 	rlwinm  r9,r0,23,9,31
    1648:	7f 8b 48 40 	cmplw   cr7,r11,r9
    164c:	41 9c 00 44 	blt-    cr7,1690 <blk_rq_check_limits+0x7c>
	    blk_rq_bytes(rq) > queue_max_hw_sectors(q) << 9) {
    1650:	81 23 02 0c 	lwz     r9,524(r3)
    1654:	55 29 48 2c 	rlwinm  r9,r9,9,0,22
int blk_rq_check_limits(struct request_queue *q, struct request *rq)
{
	if (rq->cmd_flags & REQ_DISCARD)
		return 0;

	if (blk_rq_sectors(rq) > queue_max_sectors(q) ||
    1658:	7f 89 00 40 	cmplw   cr7,r9,r0
    165c:	41 9c 00 34 	blt-    cr7,1690 <blk_rq_check_limits+0x7c>
	 * queue's settings related to segment counting like q->bounce_pfn
	 * may differ from that of other stacking queues.
	 * Recalculate it to check the request correctly on this queue's
	 * limitation.
	 */
	blk_recalc_rq_segments(rq);
    1660:	7c 83 23 78 	mr      r3,r4
    1664:	48 00 00 01 	bl      1664 <blk_rq_check_limits+0x50>
	if (rq->nr_phys_segments > queue_max_segments(q)) {
    1668:	a1 3f 00 74 	lhz     r9,116(r31)
    166c:	a0 1e 02 36 	lhz     r0,566(r30)
    1670:	7f 89 00 40 	cmplw   cr7,r9,r0
    1674:	41 9d 00 3c 	bgt-    cr7,16b0 <blk_rq_check_limits+0x9c>
		printk(KERN_ERR "%s: over max segments limit.\n", __func__);
		return -EIO;
	}

	return 0;
}
    1678:	80 01 00 24 	lwz     r0,36(r1)
    167c:	7f a3 eb 78 	mr      r3,r29
    1680:	bb a1 00 14 	lmw     r29,20(r1)
    1684:	38 21 00 20 	addi    r1,r1,32
    1688:	7c 08 03 a6 	mtlr    r0
    168c:	4e 80 00 20 	blr
	if (rq->cmd_flags & REQ_DISCARD)
		return 0;

	if (blk_rq_sectors(rq) > queue_max_sectors(q) ||
	    blk_rq_bytes(rq) > queue_max_hw_sectors(q) << 9) {
		printk(KERN_ERR "%s: over max size limit.\n", __func__);
    1690:	3c 60 00 00 	lis     r3,0
    1694:	38 63 02 54 	addi    r3,r3,596
	 * Recalculate it to check the request correctly on this queue's
	 * limitation.
	 */
	blk_recalc_rq_segments(rq);
	if (rq->nr_phys_segments > queue_max_segments(q)) {
		printk(KERN_ERR "%s: over max segments limit.\n", __func__);
    1698:	3c 80 00 00 	lis     r4,0
    169c:	3b a0 ff fb 	li      r29,-5
    16a0:	38 84 00 00 	addi    r4,r4,0
    16a4:	38 84 00 24 	addi    r4,r4,36
    16a8:	48 00 00 01 	bl      16a8 <blk_rq_check_limits+0x94>
    16ac:	4b ff ff cc 	b       1678 <blk_rq_check_limits+0x64>
    16b0:	3c 60 00 00 	lis     r3,0
    16b4:	38 63 02 74 	addi    r3,r3,628
    16b8:	4b ff ff e0 	b       1698 <blk_rq_check_limits+0x84>

000016bc <blk_insert_cloned_request>:
 * blk_insert_cloned_request - Helper for stacking drivers to submit a request
 * @q:  the queue to submit the request
 * @rq: the request being queued
 */
int blk_insert_cloned_request(struct request_queue *q, struct request *rq)
{
    16bc:	94 21 ff e0 	stwu    r1,-32(r1)
    16c0:	7c 08 02 a6 	mflr    r0
    16c4:	bf a1 00 14 	stmw    r29,20(r1)
    16c8:	7c 7d 1b 78 	mr      r29,r3
    16cc:	7c 9e 23 78 	mr      r30,r4
    16d0:	90 01 00 24 	stw     r0,36(r1)
	unsigned long flags;

	if (blk_rq_check_limits(q, rq))
    16d4:	48 00 00 01 	bl      16d4 <blk_insert_cloned_request+0x18>
    16d8:	2f 83 00 00 	cmpwi   cr7,r3,0
		return -EIO;
    16dc:	38 60 ff fb 	li      r3,-5
 */
int blk_insert_cloned_request(struct request_queue *q, struct request *rq)
{
	unsigned long flags;

	if (blk_rq_check_limits(q, rq))
    16e0:	40 9e 00 4c 	bne-    cr7,172c <blk_insert_cloned_request+0x70>

#define SET_MSR_EE(x)	mtmsr(x)

static inline unsigned long arch_local_save_flags(void)
{
	return mfmsr();
    16e4:	7f e0 00 a6 	mfmsr   r31

static inline unsigned long arch_local_irq_save(void)
{
	unsigned long flags = arch_local_save_flags();
#ifdef CONFIG_BOOKE
	asm volatile("wrteei 0" : : : "memory");
    16e8:	7c 00 01 46 	.long 0x7c000146

	/*
	 * Submitting request must be dequeued before calling this function
	 * because it will be linked to another request_queue
	 */
	BUG_ON(blk_queued_rq(rq));
    16ec:	80 1e 00 00 	lwz     r0,0(r30)
    16f0:	7f c0 02 78 	xor     r0,r30,r0
    16f4:	7c 00 00 34 	cntlzw  r0,r0
    16f8:	54 00 d9 7e 	rlwinm  r0,r0,27,5,31
    16fc:	68 00 00 01 	xori    r0,r0,1
    1700:	0f 00 00 00 	twnei   r0,0

	drive_stat_acct(rq, 1);
    1704:	38 80 00 01 	li      r4,1
    1708:	7f c3 f3 78 	mr      r3,r30
    170c:	4b ff f3 a9 	bl      ab4 <drive_stat_acct>
	__elv_add_request(q, rq, ELEVATOR_INSERT_BACK, 0);
    1710:	7f a3 eb 78 	mr      r3,r29
    1714:	7f c4 f3 78 	mr      r4,r30
    1718:	38 a0 00 02 	li      r5,2
    171c:	38 c0 00 00 	li      r6,0
    1720:	48 00 00 01 	bl      1720 <blk_insert_cloned_request+0x64>
}

static inline void arch_local_irq_restore(unsigned long flags)
{
#if defined(CONFIG_BOOKE)
	asm volatile("wrtee %0" : : "r" (flags) : "memory");
    1724:	7f e0 01 06 	.long 0x7fe00106

	spin_unlock_irqrestore(q->queue_lock, flags);

	return 0;
    1728:	38 60 00 00 	li      r3,0
}
    172c:	80 01 00 24 	lwz     r0,36(r1)
    1730:	bb a1 00 14 	lmw     r29,20(r1)
    1734:	38 21 00 20 	addi    r1,r1,32
    1738:	7c 08 03 a6 	mtlr    r0
    173c:	4e 80 00 20 	blr

00001740 <blk_update_bidi_request>:
EXPORT_SYMBOL_GPL(blk_update_request);

static bool blk_update_bidi_request(struct request *rq, int error,
				    unsigned int nr_bytes,
				    unsigned int bidi_bytes)
{
    1740:	94 21 ff e0 	stwu    r1,-32(r1)
    1744:	7c 08 02 a6 	mflr    r0
    1748:	bf 81 00 10 	stmw    r28,16(r1)
    174c:	7c 7f 1b 78 	mr      r31,r3
    1750:	7c 9e 23 78 	mr      r30,r4
    1754:	90 01 00 24 	stw     r0,36(r1)
    1758:	7c dc 33 78 	mr      r28,r6
	if (blk_update_request(rq, error, nr_bytes))
    175c:	48 00 00 01 	bl      175c <blk_update_bidi_request+0x1c>
		return true;
    1760:	3b a0 00 01 	li      r29,1

static bool blk_update_bidi_request(struct request *rq, int error,
				    unsigned int nr_bytes,
				    unsigned int bidi_bytes)
{
	if (blk_update_request(rq, error, nr_bytes))
    1764:	2f 83 00 00 	cmpwi   cr7,r3,0
    1768:	40 9e 00 24 	bne-    cr7,178c <blk_update_bidi_request+0x4c>
		return true;

	/* Bidi request must be completed as a whole */
	if (unlikely(blk_bidi_rq(rq)) &&
    176c:	80 7f 00 d0 	lwz     r3,208(r31)
    1770:	2f 83 00 00 	cmpwi   cr7,r3,0
    1774:	40 9e 00 50 	bne-    cr7,17c4 <blk_update_bidi_request+0x84>
	    blk_update_request(rq->next_rq, error, bidi_bytes))
		return true;

	if (blk_queue_add_random(rq->q))
    1778:	81 3f 00 1c 	lwz     r9,28(r31)
    177c:	3b a0 00 00 	li      r29,0
    1780:	80 09 01 78 	lwz     r0,376(r9)
    1784:	74 09 00 04 	andis.  r9,r0,4
    1788:	40 82 00 1c 	bne-    17a4 <blk_update_bidi_request+0x64>
		add_disk_randomness(rq->rq_disk);

	return false;
}
    178c:	80 01 00 24 	lwz     r0,36(r1)
    1790:	7f a3 eb 78 	mr      r3,r29
    1794:	bb 81 00 10 	lmw     r28,16(r1)
    1798:	38 21 00 20 	addi    r1,r1,32
    179c:	7c 08 03 a6 	mtlr    r0
    17a0:	4e 80 00 20 	blr
	if (unlikely(blk_bidi_rq(rq)) &&
	    blk_update_request(rq->next_rq, error, bidi_bytes))
		return true;

	if (blk_queue_add_random(rq->q))
		add_disk_randomness(rq->rq_disk);
    17a4:	80 7f 00 68 	lwz     r3,104(r31)
    17a8:	48 00 00 01 	bl      17a8 <blk_update_bidi_request+0x68>

	return false;
}
    17ac:	80 01 00 24 	lwz     r0,36(r1)
    17b0:	7f a3 eb 78 	mr      r3,r29
    17b4:	bb 81 00 10 	lmw     r28,16(r1)
    17b8:	38 21 00 20 	addi    r1,r1,32
    17bc:	7c 08 03 a6 	mtlr    r0
    17c0:	4e 80 00 20 	blr
	if (blk_update_request(rq, error, nr_bytes))
		return true;

	/* Bidi request must be completed as a whole */
	if (unlikely(blk_bidi_rq(rq)) &&
	    blk_update_request(rq->next_rq, error, bidi_bytes))
    17c4:	7f c4 f3 78 	mr      r4,r30
    17c8:	7f 85 e3 78 	mr      r5,r28
    17cc:	48 00 00 01 	bl      17cc <blk_update_bidi_request+0x8c>
{
	if (blk_update_request(rq, error, nr_bytes))
		return true;

	/* Bidi request must be completed as a whole */
	if (unlikely(blk_bidi_rq(rq)) &&
    17d0:	2f 83 00 00 	cmpwi   cr7,r3,0
    17d4:	41 9e ff a4 	beq+    cr7,1778 <blk_update_bidi_request+0x38>
    17d8:	4b ff ff b4 	b       178c <blk_update_bidi_request+0x4c>

000017dc <__freed_request>:
	ioc->nr_batch_requests = q->nr_batching;
	ioc->last_waited = jiffies;
}

static void __freed_request(struct request_queue *q, int sync)
{
    17dc:	94 21 ff e0 	stwu    r1,-32(r1)
    17e0:	7c 08 02 a6 	mflr    r0
    17e4:	bf a1 00 14 	stmw    r29,20(r1)
	struct request_list *rl = &q->rq;

	if (rl->count[sync] < queue_congestion_off_threshold(q))
    17e8:	54 9d 10 3a 	rlwinm  r29,r4,2,0,29
	ioc->nr_batch_requests = q->nr_batching;
	ioc->last_waited = jiffies;
}

static void __freed_request(struct request_queue *q, int sync)
{
    17ec:	7c 9e 23 78 	mr      r30,r4
    17f0:	90 01 00 24 	stw     r0,36(r1)
	struct request_list *rl = &q->rq;

	if (rl->count[sync] < queue_congestion_off_threshold(q))
    17f4:	7f a3 ea 14 	add     r29,r3,r29
	ioc->nr_batch_requests = q->nr_batching;
	ioc->last_waited = jiffies;
}

static void __freed_request(struct request_queue *q, int sync)
{
    17f8:	7c 7f 1b 78 	mr      r31,r3
	struct request_list *rl = &q->rq;

	if (rl->count[sync] < queue_congestion_off_threshold(q))
    17fc:	81 3d 00 10 	lwz     r9,16(r29)
    1800:	80 03 01 ac 	lwz     r0,428(r3)
    1804:	7f 89 00 00 	cmpw    cr7,r9,r0
    1808:	41 9c 00 88 	blt-    cr7,1890 <__freed_request+0xb4>
		blk_clear_queue_congested(q, sync);

	if (rl->count[sync] + 1 <= q->nr_requests) {
    180c:	80 1f 01 a4 	lwz     r0,420(r31)
    1810:	39 29 00 01 	addi    r9,r9,1
    1814:	7f 89 00 40 	cmplw   cr7,r9,r0
    1818:	41 9d 00 48 	bgt-    cr7,1860 <__freed_request+0x84>
		if (waitqueue_active(&rl->wait[sync]))
    181c:	38 7e 00 02 	addi    r3,r30,2
    1820:	38 1f 00 10 	addi    r0,r31,16
    1824:	54 63 18 38 	rlwinm  r3,r3,3,0,28
    1828:	7c 60 1a 14 	add     r3,r0,r3
    182c:	80 03 00 08 	lwz     r0,8(r3)
    1830:	38 63 00 08 	addi    r3,r3,8
    1834:	7f 83 00 00 	cmpw    cr7,r3,r0
    1838:	41 9e 00 14 	beq-    cr7,184c <__freed_request+0x70>
			wake_up(&rl->wait[sync]);
    183c:	38 80 00 03 	li      r4,3
    1840:	38 a0 00 01 	li      r5,1
    1844:	38 c0 00 00 	li      r6,0
    1848:	48 00 00 01 	bl      1848 <__freed_request+0x6c>
		queue_flag_set(QUEUE_FLAG_ASYNCFULL, q);
}

static inline void blk_clear_queue_full(struct request_queue *q, int sync)
{
	if (sync)
    184c:	2f 9e 00 00 	cmpwi   cr7,r30,0
static inline void __clear_bit(int nr, volatile unsigned long *addr)
{
	unsigned long mask = BIT_MASK(nr);
	unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);

	*p &= ~mask;
    1850:	80 1f 01 78 	lwz     r0,376(r31)
    1854:	40 9e 00 20 	bne-    cr7,1874 <__freed_request+0x98>
    1858:	54 00 07 34 	rlwinm  r0,r0,0,28,26
    185c:	90 1f 01 78 	stw     r0,376(r31)

		blk_clear_queue_full(q, sync);
	}
}
    1860:	80 01 00 24 	lwz     r0,36(r1)
    1864:	bb a1 00 14 	lmw     r29,20(r1)
    1868:	38 21 00 20 	addi    r1,r1,32
    186c:	7c 08 03 a6 	mtlr    r0
    1870:	4e 80 00 20 	blr
    1874:	54 00 07 76 	rlwinm  r0,r0,0,29,27
    1878:	90 1f 01 78 	stw     r0,376(r31)
    187c:	80 01 00 24 	lwz     r0,36(r1)
    1880:	bb a1 00 14 	lmw     r29,20(r1)
    1884:	38 21 00 20 	addi    r1,r1,32
    1888:	7c 08 03 a6 	mtlr    r0
    188c:	4e 80 00 20 	blr
 * congested queues, and wake up anyone who was waiting for requests to be
 * put back.
 */
static inline void blk_clear_queue_congested(struct request_queue *q, int sync)
{
	clear_bdi_congested(&q->backing_dev_info, sync);
    1890:	38 63 00 a0 	addi    r3,r3,160
    1894:	48 00 00 01 	bl      1894 <__freed_request+0xb8>
    1898:	81 3d 00 10 	lwz     r9,16(r29)
    189c:	4b ff ff 70 	b       180c <__freed_request+0x30>

000018a0 <freed_request>:
/*
 * A request has just been released.  Account for it, update the full and
 * congestion status, wake up any waiters.   Called under q->queue_lock.
 */
static void freed_request(struct request_queue *q, int sync, int priv)
{
    18a0:	94 21 ff f0 	stwu    r1,-16(r1)
    18a4:	7c 08 02 a6 	mflr    r0
	struct request_list *rl = &q->rq;

	rl->count[sync]--;
	if (priv)
    18a8:	2f 85 00 00 	cmpwi   cr7,r5,0
/*
 * A request has just been released.  Account for it, update the full and
 * congestion status, wake up any waiters.   Called under q->queue_lock.
 */
static void freed_request(struct request_queue *q, int sync, int priv)
{
    18ac:	bf c1 00 08 	stmw    r30,8(r1)
    18b0:	7c 9e 23 78 	mr      r30,r4
    18b4:	7c 7f 1b 78 	mr      r31,r3
    18b8:	90 01 00 14 	stw     r0,20(r1)
	struct request_list *rl = &q->rq;

	rl->count[sync]--;
    18bc:	54 80 10 3a 	rlwinm  r0,r4,2,0,29
    18c0:	7d 23 02 14 	add     r9,r3,r0
    18c4:	81 69 00 10 	lwz     r11,16(r9)
    18c8:	38 0b ff ff 	addi    r0,r11,-1
    18cc:	90 09 00 10 	stw     r0,16(r9)
	if (priv)
    18d0:	41 9e 00 10 	beq-    cr7,18e0 <freed_request+0x40>
		rl->elvpriv--;
    18d4:	81 23 00 20 	lwz     r9,32(r3)
    18d8:	38 09 ff ff 	addi    r0,r9,-1
    18dc:	90 03 00 20 	stw     r0,32(r3)

	__freed_request(q, sync);
    18e0:	7f c4 f3 78 	mr      r4,r30
    18e4:	7f e3 fb 78 	mr      r3,r31

	if (unlikely(rl->starved[sync ^ 1]))
    18e8:	6b de 00 01 	xori    r30,r30,1

	rl->count[sync]--;
	if (priv)
		rl->elvpriv--;

	__freed_request(q, sync);
    18ec:	4b ff fe f1 	bl      17dc <__freed_request>

	if (unlikely(rl->starved[sync ^ 1]))
    18f0:	57 c9 10 3a 	rlwinm  r9,r30,2,0,29
    18f4:	7d 3f 4a 14 	add     r9,r31,r9
    18f8:	80 09 00 18 	lwz     r0,24(r9)
    18fc:	2f 80 00 00 	cmpwi   cr7,r0,0
    1900:	40 9e 00 18 	bne-    cr7,1918 <freed_request+0x78>
		__freed_request(q, sync ^ 1);
}
    1904:	80 01 00 14 	lwz     r0,20(r1)
    1908:	bb c1 00 08 	lmw     r30,8(r1)
    190c:	38 21 00 10 	addi    r1,r1,16
    1910:	7c 08 03 a6 	mtlr    r0
    1914:	4e 80 00 20 	blr
    1918:	80 01 00 14 	lwz     r0,20(r1)
		rl->elvpriv--;

	__freed_request(q, sync);

	if (unlikely(rl->starved[sync ^ 1]))
		__freed_request(q, sync ^ 1);
    191c:	7f e3 fb 78 	mr      r3,r31
    1920:	7f c4 f3 78 	mr      r4,r30
}
    1924:	bb c1 00 08 	lmw     r30,8(r1)
    1928:	38 21 00 10 	addi    r1,r1,16
    192c:	7c 08 03 a6 	mtlr    r0
		rl->elvpriv--;

	__freed_request(q, sync);

	if (unlikely(rl->starved[sync ^ 1]))
		__freed_request(q, sync ^ 1);
    1930:	4b ff fe ac 	b       17dc <__freed_request>

00001934 <__blk_put_request>:

/*
 * queue lock must be held
 */
void __blk_put_request(struct request_queue *q, struct request *req)
{
    1934:	94 21 ff e0 	stwu    r1,-32(r1)
    1938:	7c 08 02 a6 	mflr    r0
    193c:	bf 81 00 10 	stmw    r28,16(r1)
	if (unlikely(!q))
    1940:	7c 7e 1b 79 	mr.     r30,r3

/*
 * queue lock must be held
 */
void __blk_put_request(struct request_queue *q, struct request *req)
{
    1944:	7c 9f 23 78 	mr      r31,r4
    1948:	90 01 00 24 	stw     r0,36(r1)
	if (unlikely(!q))
    194c:	41 82 00 b4 	beq-    1a00 <__blk_put_request+0xcc>
		return;
	if (unlikely(--req->ref_count))
    1950:	81 24 00 78 	lwz     r9,120(r4)
    1954:	38 09 ff ff 	addi    r0,r9,-1
    1958:	2f 80 00 00 	cmpwi   cr7,r0,0
    195c:	90 04 00 78 	stw     r0,120(r4)
    1960:	40 9e 00 a0 	bne-    cr7,1a00 <__blk_put_request+0xcc>
		return;

	elv_completed_request(q, req);
    1964:	48 00 00 01 	bl      1964 <__blk_put_request+0x30>

	/* this is a bio leak */
	WARN_ON(req->bio != NULL);
    1968:	80 1f 00 40 	lwz     r0,64(r31)
    196c:	7c 00 00 34 	cntlzw  r0,r0
    1970:	54 00 d9 7e 	rlwinm  r0,r0,27,5,31
    1974:	68 00 00 01 	xori    r0,r0,1
    1978:	0f 00 00 00 	twnei   r0,0

	/*
	 * Request may not have originated from ll_rw_blk. if not,
	 * it didn't come out of our reserved rq pools
	 */
	if (req->cmd_flags & REQ_ALLOCED) {
    197c:	80 1f 00 20 	lwz     r0,32(r31)
    1980:	74 09 00 40 	andis.  r9,r0,64
    1984:	41 82 00 7c 	beq-    1a00 <__blk_put_request+0xcc>
/*
 * We regard a request as sync, if either a read or a sync write
 */
static inline bool rw_is_sync(unsigned int rw_flags)
{
	return !(rw_flags & REQ_WRITE) || (rw_flags & REQ_SYNC);
    1988:	70 1c 00 11 	andi.   r28,r0,17
		int is_sync = rq_is_sync(req) != 0;
		int priv = req->cmd_flags & REQ_ELVPRIV;

		BUG_ON(!list_empty(&req->queuelist));
    198c:	81 3f 00 00 	lwz     r9,0(r31)
	 * Request may not have originated from ll_rw_blk. if not,
	 * it didn't come out of our reserved rq pools
	 */
	if (req->cmd_flags & REQ_ALLOCED) {
		int is_sync = rq_is_sync(req) != 0;
		int priv = req->cmd_flags & REQ_ELVPRIV;
    1990:	54 1d 03 5a 	rlwinm  r29,r0,0,13,13

		BUG_ON(!list_empty(&req->queuelist));
    1994:	7f e9 4a 78 	xor     r9,r31,r9
	/*
	 * Request may not have originated from ll_rw_blk. if not,
	 * it didn't come out of our reserved rq pools
	 */
	if (req->cmd_flags & REQ_ALLOCED) {
		int is_sync = rq_is_sync(req) != 0;
    1998:	6b 9c 00 01 	xori    r28,r28,1
		int priv = req->cmd_flags & REQ_ELVPRIV;

		BUG_ON(!list_empty(&req->queuelist));
    199c:	7d 29 00 34 	cntlzw  r9,r9
	/*
	 * Request may not have originated from ll_rw_blk. if not,
	 * it didn't come out of our reserved rq pools
	 */
	if (req->cmd_flags & REQ_ALLOCED) {
		int is_sync = rq_is_sync(req) != 0;
    19a0:	7f 9c 00 34 	cntlzw  r28,r28
		int priv = req->cmd_flags & REQ_ELVPRIV;

		BUG_ON(!list_empty(&req->queuelist));
    19a4:	55 29 d9 7e 	rlwinm  r9,r9,27,5,31
	/*
	 * Request may not have originated from ll_rw_blk. if not,
	 * it didn't come out of our reserved rq pools
	 */
	if (req->cmd_flags & REQ_ALLOCED) {
		int is_sync = rq_is_sync(req) != 0;
    19a8:	57 9c d9 7e 	rlwinm  r28,r28,27,5,31
		int priv = req->cmd_flags & REQ_ELVPRIV;

		BUG_ON(!list_empty(&req->queuelist));
    19ac:	69 29 00 01 	xori    r9,r9,1
	/*
	 * Request may not have originated from ll_rw_blk. if not,
	 * it didn't come out of our reserved rq pools
	 */
	if (req->cmd_flags & REQ_ALLOCED) {
		int is_sync = rq_is_sync(req) != 0;
    19b0:	6b 9c 00 01 	xori    r28,r28,1
		int priv = req->cmd_flags & REQ_ELVPRIV;

		BUG_ON(!list_empty(&req->queuelist));
    19b4:	0f 09 00 00 	twnei   r9,0
		BUG_ON(!hlist_unhashed(&req->hash));
    19b8:	81 3f 00 4c 	lwz     r9,76(r31)
    19bc:	7d 29 00 34 	cntlzw  r9,r9
    19c0:	55 29 d9 7e 	rlwinm  r9,r9,27,5,31
    19c4:	69 29 00 01 	xori    r9,r9,1
    19c8:	0f 09 00 00 	twnei   r9,0
	return 1;
}

static inline void blk_free_request(struct request_queue *q, struct request *rq)
{
	if (rq->cmd_flags & REQ_ELVPRIV)
    19cc:	74 09 00 04 	andis.  r9,r0,4
    19d0:	40 82 00 44 	bne-    1a14 <__blk_put_request+0xe0>
		elv_put_request(q, rq);
	mempool_free(rq, q->rq.rq_pool);
    19d4:	80 9e 00 24 	lwz     r4,36(r30)
    19d8:	7f e3 fb 78 	mr      r3,r31
    19dc:	48 00 00 01 	bl      19dc <__blk_put_request+0xa8>
		BUG_ON(!hlist_unhashed(&req->hash));

		blk_free_request(q, req);
		freed_request(q, is_sync, priv);
	}
}
    19e0:	80 01 00 24 	lwz     r0,36(r1)

		BUG_ON(!list_empty(&req->queuelist));
		BUG_ON(!hlist_unhashed(&req->hash));

		blk_free_request(q, req);
		freed_request(q, is_sync, priv);
    19e4:	7f c3 f3 78 	mr      r3,r30
    19e8:	7f 84 e3 78 	mr      r4,r28
    19ec:	7f a5 eb 78 	mr      r5,r29
	}
}
    19f0:	bb 81 00 10 	lmw     r28,16(r1)
    19f4:	38 21 00 20 	addi    r1,r1,32
    19f8:	7c 08 03 a6 	mtlr    r0

		BUG_ON(!list_empty(&req->queuelist));
		BUG_ON(!hlist_unhashed(&req->hash));

		blk_free_request(q, req);
		freed_request(q, is_sync, priv);
    19fc:	4b ff fe a4 	b       18a0 <freed_request>
	}
}
    1a00:	80 01 00 24 	lwz     r0,36(r1)
    1a04:	bb 81 00 10 	lmw     r28,16(r1)
    1a08:	38 21 00 20 	addi    r1,r1,32
    1a0c:	7c 08 03 a6 	mtlr    r0
    1a10:	4e 80 00 20 	blr
}

static inline void blk_free_request(struct request_queue *q, struct request *rq)
{
	if (rq->cmd_flags & REQ_ELVPRIV)
		elv_put_request(q, rq);
    1a14:	7f c3 f3 78 	mr      r3,r30
    1a18:	7f e4 fb 78 	mr      r4,r31
    1a1c:	48 00 00 01 	bl      1a1c <__blk_put_request+0xe8>
	mempool_free(rq, q->rq.rq_pool);
    1a20:	80 9e 00 24 	lwz     r4,36(r30)
    1a24:	7f e3 fb 78 	mr      r3,r31
    1a28:	48 00 00 01 	bl      1a28 <__blk_put_request+0xf4>
		BUG_ON(!hlist_unhashed(&req->hash));

		blk_free_request(q, req);
		freed_request(q, is_sync, priv);
	}
}
    1a2c:	80 01 00 24 	lwz     r0,36(r1)

		BUG_ON(!list_empty(&req->queuelist));
		BUG_ON(!hlist_unhashed(&req->hash));

		blk_free_request(q, req);
		freed_request(q, is_sync, priv);
    1a30:	7f c3 f3 78 	mr      r3,r30
    1a34:	7f 84 e3 78 	mr      r4,r28
    1a38:	7f a5 eb 78 	mr      r5,r29
	}
}
    1a3c:	bb 81 00 10 	lmw     r28,16(r1)
    1a40:	38 21 00 20 	addi    r1,r1,32
    1a44:	7c 08 03 a6 	mtlr    r0

		BUG_ON(!list_empty(&req->queuelist));
		BUG_ON(!hlist_unhashed(&req->hash));

		blk_free_request(q, req);
		freed_request(q, is_sync, priv);
    1a48:	4b ff fe 58 	b       18a0 <freed_request>

00001a4c <blk_finish_request>:

/*
 * queue lock must be held
 */
static void blk_finish_request(struct request *req, int error)
{
    1a4c:	94 21 ff e0 	stwu    r1,-32(r1)
    1a50:	7c 08 02 a6 	mflr    r0
    1a54:	bf 81 00 10 	stmw    r28,16(r1)
    1a58:	7c 7f 1b 78 	mr      r31,r3
    1a5c:	7c 9c 23 78 	mr      r28,r4
    1a60:	90 01 00 24 	stw     r0,36(r1)
	if (blk_rq_tagged(req))
    1a64:	80 03 00 20 	lwz     r0,32(r3)
    1a68:	74 09 00 02 	andis.  r9,r0,2
    1a6c:	40 82 02 30 	bne-    1c9c <blk_finish_request+0x250>
		blk_queue_end_tag(req->q, req);

	BUG_ON(blk_queued_rq(req));
    1a70:	80 1f 00 00 	lwz     r0,0(r31)
    1a74:	7f e0 02 78 	xor     r0,r31,r0
    1a78:	7c 00 00 34 	cntlzw  r0,r0
    1a7c:	54 00 d9 7e 	rlwinm  r0,r0,27,5,31
    1a80:	68 00 00 01 	xori    r0,r0,1
    1a84:	0f 00 00 00 	twnei   r0,0

	if (unlikely(laptop_mode) && req->cmd_type == REQ_TYPE_FS)
    1a88:	3d 20 00 00 	lis     r9,0
    1a8c:	80 09 00 00 	lwz     r0,0(r9)
    1a90:	2f 80 00 00 	cmpwi   cr7,r0,0
    1a94:	40 9e 02 34 	bne-    cr7,1cc8 <blk_finish_request+0x27c>
		laptop_io_completion(&req->q->backing_dev_info);

	blk_delete_timer(req);
    1a98:	7f e3 fb 78 	mr      r3,r31
    1a9c:	48 00 00 01 	bl      1a9c <blk_finish_request+0x50>

	if (req->cmd_flags & REQ_DONTPREP)
    1aa0:	80 1f 00 20 	lwz     r0,32(r31)
    1aa4:	74 09 00 01 	andis.  r9,r0,1
    1aa8:	41 82 00 28 	beq-    1ad0 <blk_finish_request+0x84>
 */
void blk_unprep_request(struct request *req)
{
	struct request_queue *q = req->q;

	req->cmd_flags &= ~REQ_DONTPREP;
    1aac:	54 00 04 1c 	rlwinm  r0,r0,0,16,14
 * that were allocated to the request in the prep_rq_fn.  The queue
 * lock is held when calling this.
 */
void blk_unprep_request(struct request *req)
{
	struct request_queue *q = req->q;
    1ab0:	80 7f 00 1c 	lwz     r3,28(r31)

	req->cmd_flags &= ~REQ_DONTPREP;
    1ab4:	90 1f 00 20 	stw     r0,32(r31)
	if (q->unprep_rq_fn)
    1ab8:	80 03 00 44 	lwz     r0,68(r3)
    1abc:	2f 80 00 00 	cmpwi   cr7,r0,0
    1ac0:	41 9e 00 10 	beq-    cr7,1ad0 <blk_finish_request+0x84>
		q->unprep_rq_fn(q, req);
    1ac4:	7f e4 fb 78 	mr      r4,r31
    1ac8:	7c 09 03 a6 	mtctr   r0
    1acc:	4e 80 04 21 	bctrl
 *	b) the queue had IO stats enabled when this request was started, and
 *	c) it's a file system request or a discard request
 */
static inline int blk_do_io_stat(struct request *rq)
{
	return rq->rq_disk &&
    1ad0:	80 1f 00 68 	lwz     r0,104(r31)
    1ad4:	2f 80 00 00 	cmpwi   cr7,r0,0
    1ad8:	41 9e 01 64 	beq-    cr7,1c3c <blk_finish_request+0x1f0>
	       (rq->cmd_flags & REQ_IO_STAT) &&
    1adc:	83 bf 00 20 	lwz     r29,32(r31)
 *	b) the queue had IO stats enabled when this request was started, and
 *	c) it's a file system request or a discard request
 */
static inline int blk_do_io_stat(struct request *rq)
{
	return rq->rq_disk &&
    1ae0:	77 a0 02 00 	andis.  r0,r29,512
    1ae4:	41 82 01 58 	beq-    1c3c <blk_finish_request+0x1f0>
	       (rq->cmd_flags & REQ_IO_STAT) &&
    1ae8:	80 1f 00 24 	lwz     r0,36(r31)
    1aec:	2f 80 00 01 	cmpwi   cr7,r0,1
    1af0:	41 9e 00 0c 	beq-    cr7,1afc <blk_finish_request+0xb0>
	       (rq->cmd_type == REQ_TYPE_FS ||
    1af4:	73 a9 00 40 	andi.   r9,r29,64
    1af8:	41 82 01 44 	beq-    1c3c <blk_finish_request+0x1f0>
	/*
	 * Account IO completion.  flush_rq isn't accounted as a
	 * normal IO on queueing nor completion.  Accounting the
	 * containing request is enough.
	 */
	if (blk_do_io_stat(req) && req != &req->q->flush_rq) {
    1afc:	81 3f 00 1c 	lwz     r9,28(r31)
    1b00:	38 09 02 58 	addi    r0,r9,600
    1b04:	7f 9f 00 00 	cmpw    cr7,r31,r0
    1b08:	41 9e 01 34 	beq-    cr7,1c3c <blk_finish_request+0x1f0>
		unsigned long duration = jiffies - req->start_time;
		const int rw = rq_data_dir(req);
    1b0c:	57 bd 07 fe 	clrlwi  r29,r29,31
		struct hd_struct *part;
		int cpu;

		cpu = part_stat_lock();
		part = req->part;
    1b10:	83 df 00 6c 	lwz     r30,108(r31)
	 * Account IO completion.  flush_rq isn't accounted as a
	 * normal IO on queueing nor completion.  Accounting the
	 * containing request is enough.
	 */
	if (blk_do_io_stat(req) && req != &req->q->flush_rq) {
		unsigned long duration = jiffies - req->start_time;
    1b14:	3d 60 00 00 	lis     r11,0
		int cpu;

		cpu = part_stat_lock();
		part = req->part;

		part_stat_inc(cpu, part, ios[rw]);
    1b18:	39 3d 00 3c 	addi    r9,r29,60
	 * Account IO completion.  flush_rq isn't accounted as a
	 * normal IO on queueing nor completion.  Accounting the
	 * containing request is enough.
	 */
	if (blk_do_io_stat(req) && req != &req->q->flush_rq) {
		unsigned long duration = jiffies - req->start_time;
    1b1c:	81 6b 00 00 	lwz     r11,0(r11)
		int cpu;

		cpu = part_stat_lock();
		part = req->part;

		part_stat_inc(cpu, part, ios[rw]);
    1b20:	55 29 10 3a 	rlwinm  r9,r9,2,0,29
	 * Account IO completion.  flush_rq isn't accounted as a
	 * normal IO on queueing nor completion.  Accounting the
	 * containing request is enough.
	 */
	if (blk_do_io_stat(req) && req != &req->q->flush_rq) {
		unsigned long duration = jiffies - req->start_time;
    1b24:	80 1f 00 70 	lwz     r0,112(r31)
		int cpu;

		cpu = part_stat_lock();
		part = req->part;

		part_stat_inc(cpu, part, ios[rw]);
    1b28:	7d 3e 4a 14 	add     r9,r30,r9
    1b2c:	81 49 00 0c 	lwz     r10,12(r9)
	 * Account IO completion.  flush_rq isn't accounted as a
	 * normal IO on queueing nor completion.  Accounting the
	 * containing request is enough.
	 */
	if (blk_do_io_stat(req) && req != &req->q->flush_rq) {
		unsigned long duration = jiffies - req->start_time;
    1b30:	7c 00 58 50 	subf    r0,r0,r11
		int cpu;

		cpu = part_stat_lock();
		part = req->part;

		part_stat_inc(cpu, part, ios[rw]);
    1b34:	39 4a 00 01 	addi    r10,r10,1
    1b38:	91 49 00 0c 	stw     r10,12(r9)
    1b3c:	81 3e 00 e0 	lwz     r9,224(r30)
    1b40:	2f 89 00 00 	cmpwi   cr7,r9,0
    1b44:	41 9e 01 68 	beq-    cr7,1cac <blk_finish_request+0x260>
	int node_id;
};

static inline struct gendisk *part_to_disk(struct hd_struct *part)
{
	if (likely(part)) {
    1b48:	2f 9e 00 00 	cmpwi   cr7,r30,0
		if (part->partno)
			return dev_to_disk(part_to_dev(part)->parent);
		else
			return dev_to_disk(part_to_dev(part));
	}
	return NULL;
    1b4c:	39 20 00 00 	li      r9,0
	int node_id;
};

static inline struct gendisk *part_to_disk(struct hd_struct *part)
{
	if (likely(part)) {
    1b50:	41 9e 00 0c 	beq-    cr7,1b5c <blk_finish_request+0x110>
		if (part->partno)
			return dev_to_disk(part_to_dev(part)->parent);
    1b54:	81 3e 00 20 	lwz     r9,32(r30)
    1b58:	39 29 ff a0 	addi    r9,r9,-96
    1b5c:	39 7d 00 4c 	addi    r11,r29,76
    1b60:	55 6b 10 3a 	rlwinm  r11,r11,2,0,29
    1b64:	7d 69 5a 14 	add     r11,r9,r11
		part_stat_add(cpu, part, ticks[rw], duration);
    1b68:	39 3d 00 40 	addi    r9,r29,64
		int cpu;

		cpu = part_stat_lock();
		part = req->part;

		part_stat_inc(cpu, part, ios[rw]);
    1b6c:	81 4b 00 0c 	lwz     r10,12(r11)
		part_stat_add(cpu, part, ticks[rw], duration);
    1b70:	55 29 10 3a 	rlwinm  r9,r9,2,0,29
    1b74:	7d 3e 4a 14 	add     r9,r30,r9
		int cpu;

		cpu = part_stat_lock();
		part = req->part;

		part_stat_inc(cpu, part, ios[rw]);
    1b78:	39 4a 00 01 	addi    r10,r10,1
    1b7c:	91 4b 00 0c 	stw     r10,12(r11)
    1b80:	81 5e 00 e0 	lwz     r10,224(r30)
		part_stat_add(cpu, part, ticks[rw], duration);
    1b84:	81 69 00 0c 	lwz     r11,12(r9)
    1b88:	2f 0a 00 00 	cmpwi   cr6,r10,0
    1b8c:	7d 6b 02 14 	add     r11,r11,r0
    1b90:	91 69 00 0c 	stw     r11,12(r9)
    1b94:	41 9a 00 28 	beq-    cr6,1bbc <blk_finish_request+0x170>
	int node_id;
};

static inline struct gendisk *part_to_disk(struct hd_struct *part)
{
	if (likely(part)) {
    1b98:	41 9e 01 4c 	beq-    cr7,1ce4 <blk_finish_request+0x298>
		if (part->partno)
			return dev_to_disk(part_to_dev(part)->parent);
    1b9c:	81 7e 00 20 	lwz     r11,32(r30)
    1ba0:	39 6b ff a0 	addi    r11,r11,-96
    1ba4:	39 3d 00 50 	addi    r9,r29,80
    1ba8:	55 29 10 3a 	rlwinm  r9,r9,2,0,29
    1bac:	7d 2b 4a 14 	add     r9,r11,r9
    1bb0:	81 69 00 0c 	lwz     r11,12(r9)
    1bb4:	7c 0b 02 14 	add     r0,r11,r0
    1bb8:	90 09 00 0c 	stw     r0,12(r9)
		part_round_stats(cpu, part);
    1bbc:	38 60 00 00 	li      r3,0
    1bc0:	7f c4 f3 78 	mr      r4,r30
    1bc4:	48 00 00 01 	bl      1bc4 <blk_finish_request+0x178>
		part_to_disk(part)->part0.in_flight[rw]++;
}

static inline void part_dec_in_flight(struct hd_struct *part, int rw)
{
	part->in_flight[rw]--;
    1bc8:	39 3d 00 38 	addi    r9,r29,56
    1bcc:	55 29 10 3a 	rlwinm  r9,r9,2,0,29
    1bd0:	7d 3e 4a 14 	add     r9,r30,r9
    1bd4:	81 69 00 0c 	lwz     r11,12(r9)
    1bd8:	38 0b ff ff 	addi    r0,r11,-1
    1bdc:	90 09 00 0c 	stw     r0,12(r9)
	if (part->partno)
    1be0:	80 1e 00 e0 	lwz     r0,224(r30)
    1be4:	2f 80 00 00 	cmpwi   cr7,r0,0
    1be8:	41 9e 00 30 	beq-    cr7,1c18 <blk_finish_request+0x1cc>
	int node_id;
};

static inline struct gendisk *part_to_disk(struct hd_struct *part)
{
	if (likely(part)) {
    1bec:	2f 9e 00 00 	cmpwi   cr7,r30,0
		if (part->partno)
			return dev_to_disk(part_to_dev(part)->parent);
		else
			return dev_to_disk(part_to_dev(part));
	}
	return NULL;
    1bf0:	38 00 00 00 	li      r0,0
	int node_id;
};

static inline struct gendisk *part_to_disk(struct hd_struct *part)
{
	if (likely(part)) {
    1bf4:	41 9e 00 0c 	beq-    cr7,1c00 <blk_finish_request+0x1b4>
		if (part->partno)
			return dev_to_disk(part_to_dev(part)->parent);
    1bf8:	81 3e 00 20 	lwz     r9,32(r30)
    1bfc:	38 09 ff a0 	addi    r0,r9,-96

static inline void part_dec_in_flight(struct hd_struct *part, int rw)
{
	part->in_flight[rw]--;
	if (part->partno)
		part_to_disk(part)->part0.in_flight[rw]--;
    1c00:	3b bd 00 48 	addi    r29,r29,72
    1c04:	57 bd 10 3a 	rlwinm  r29,r29,2,0,29
    1c08:	7f a0 ea 14 	add     r29,r0,r29
    1c0c:	81 3d 00 0c 	lwz     r9,12(r29)
    1c10:	38 09 ff ff 	addi    r0,r9,-1
    1c14:	90 1d 00 0c 	stw     r0,12(r29)
	PPC405_ERR77(0,%1)
"	stwcx.	%0,0,%1\n\
	bne-	1b"
	PPC_ACQUIRE_BARRIER
	: "=&r" (t)
	: "r" (&v->counter)
    1c18:	39 3e 01 1c 	addi    r9,r30,284

static __inline__ int atomic_dec_return(atomic_t *v)
{
	int t;

	__asm__ __volatile__(
    1c1c:	7c 00 48 28 	lwarx   r0,0,r9
    1c20:	30 00 ff ff 	addic   r0,r0,-1
    1c24:	7c 00 49 2d 	stwcx.  r0,0,r9
    1c28:	40 a2 ff f4 	bne-    1c1c <blk_finish_request+0x1d0>
	return atomic_inc_not_zero(&part->ref);
}

static inline void hd_struct_put(struct hd_struct *part)
{
	if (atomic_dec_and_test(&part->ref))
    1c2c:	2f 80 00 00 	cmpwi   cr7,r0,0
    1c30:	40 be 00 0c 	bne+    cr7,1c3c <blk_finish_request+0x1f0>
		__delete_partition(part);
    1c34:	7f c3 f3 78 	mr      r3,r30
    1c38:	48 00 00 01 	bl      1c38 <blk_finish_request+0x1ec>
		blk_unprep_request(req);


	blk_account_io_done(req);

	if (req->end_io)
    1c3c:	80 1f 00 c8 	lwz     r0,200(r31)
    1c40:	2f 80 00 00 	cmpwi   cr7,r0,0
    1c44:	41 9e 00 28 	beq-    cr7,1c6c <blk_finish_request+0x220>
		req->end_io(req, error);
    1c48:	7f e3 fb 78 	mr      r3,r31
    1c4c:	7f 84 e3 78 	mr      r4,r28
    1c50:	7c 09 03 a6 	mtctr   r0
    1c54:	4e 80 04 21 	bctrl
		if (blk_bidi_rq(req))
			__blk_put_request(req->next_rq->q, req->next_rq);

		__blk_put_request(req->q, req);
	}
}
    1c58:	80 01 00 24 	lwz     r0,36(r1)
    1c5c:	bb 81 00 10 	lmw     r28,16(r1)
    1c60:	38 21 00 20 	addi    r1,r1,32
    1c64:	7c 08 03 a6 	mtlr    r0
    1c68:	4e 80 00 20 	blr
	blk_account_io_done(req);

	if (req->end_io)
		req->end_io(req, error);
	else {
		if (blk_bidi_rq(req))
    1c6c:	80 9f 00 d0 	lwz     r4,208(r31)
    1c70:	2f 84 00 00 	cmpwi   cr7,r4,0
    1c74:	41 9e 00 0c 	beq-    cr7,1c80 <blk_finish_request+0x234>
			__blk_put_request(req->next_rq->q, req->next_rq);
    1c78:	80 64 00 1c 	lwz     r3,28(r4)
    1c7c:	48 00 00 01 	bl      1c7c <blk_finish_request+0x230>

		__blk_put_request(req->q, req);
	}
}
    1c80:	80 01 00 24 	lwz     r0,36(r1)
		req->end_io(req, error);
	else {
		if (blk_bidi_rq(req))
			__blk_put_request(req->next_rq->q, req->next_rq);

		__blk_put_request(req->q, req);
    1c84:	7f e4 fb 78 	mr      r4,r31
    1c88:	80 7f 00 1c 	lwz     r3,28(r31)
	}
}
    1c8c:	7c 08 03 a6 	mtlr    r0
    1c90:	bb 81 00 10 	lmw     r28,16(r1)
    1c94:	38 21 00 20 	addi    r1,r1,32
		req->end_io(req, error);
	else {
		if (blk_bidi_rq(req))
			__blk_put_request(req->next_rq->q, req->next_rq);

		__blk_put_request(req->q, req);
    1c98:	48 00 00 00 	b       1c98 <blk_finish_request+0x24c>
 * queue lock must be held
 */
static void blk_finish_request(struct request *req, int error)
{
	if (blk_rq_tagged(req))
		blk_queue_end_tag(req->q, req);
    1c9c:	80 63 00 1c 	lwz     r3,28(r3)
    1ca0:	7f e4 fb 78 	mr      r4,r31
    1ca4:	48 00 00 01 	bl      1ca4 <blk_finish_request+0x258>
    1ca8:	4b ff fd c8 	b       1a70 <blk_finish_request+0x24>

		cpu = part_stat_lock();
		part = req->part;

		part_stat_inc(cpu, part, ios[rw]);
		part_stat_add(cpu, part, ticks[rw], duration);
    1cac:	39 3d 00 40 	addi    r9,r29,64
    1cb0:	55 29 10 3a 	rlwinm  r9,r9,2,0,29
    1cb4:	7d 3e 4a 14 	add     r9,r30,r9
    1cb8:	81 69 00 0c 	lwz     r11,12(r9)
    1cbc:	7c 0b 02 14 	add     r0,r11,r0
    1cc0:	90 09 00 0c 	stw     r0,12(r9)
    1cc4:	4b ff fe f8 	b       1bbc <blk_finish_request+0x170>
	if (blk_rq_tagged(req))
		blk_queue_end_tag(req->q, req);

	BUG_ON(blk_queued_rq(req));

	if (unlikely(laptop_mode) && req->cmd_type == REQ_TYPE_FS)
    1cc8:	80 1f 00 24 	lwz     r0,36(r31)
    1ccc:	2f 80 00 01 	cmpwi   cr7,r0,1
    1cd0:	40 9e fd c8 	bne+    cr7,1a98 <blk_finish_request+0x4c>
		laptop_io_completion(&req->q->backing_dev_info);
    1cd4:	80 7f 00 1c 	lwz     r3,28(r31)
    1cd8:	38 63 00 a0 	addi    r3,r3,160
    1cdc:	48 00 00 01 	bl      1cdc <blk_finish_request+0x290>
    1ce0:	4b ff fd b8 	b       1a98 <blk_finish_request+0x4c>
		if (part->partno)
			return dev_to_disk(part_to_dev(part)->parent);
		else
			return dev_to_disk(part_to_dev(part));
	}
	return NULL;
    1ce4:	39 60 00 00 	li      r11,0
    1ce8:	4b ff fe bc 	b       1ba4 <blk_finish_request+0x158>

00001cec <__blk_end_request>:
 * Return:
 *     %false - we are done with this request
 *     %true  - still buffers pending for this request
 **/
bool __blk_end_request(struct request *rq, int error, unsigned int nr_bytes)
{
    1cec:	94 21 ff f0 	stwu    r1,-16(r1)
    1cf0:	7c 08 02 a6 	mflr    r0
 *     %true  - still buffers pending for this request
 **/
static bool __blk_end_bidi_request(struct request *rq, int error,
				   unsigned int nr_bytes, unsigned int bidi_bytes)
{
	if (blk_update_bidi_request(rq, error, nr_bytes, bidi_bytes))
    1cf4:	38 c0 00 00 	li      r6,0
 * Return:
 *     %false - we are done with this request
 *     %true  - still buffers pending for this request
 **/
bool __blk_end_request(struct request *rq, int error, unsigned int nr_bytes)
{
    1cf8:	bf c1 00 08 	stmw    r30,8(r1)
    1cfc:	7c 7e 1b 78 	mr      r30,r3
    1d00:	7c 9f 23 78 	mr      r31,r4
    1d04:	90 01 00 14 	stw     r0,20(r1)
 *     %true  - still buffers pending for this request
 **/
static bool __blk_end_bidi_request(struct request *rq, int error,
				   unsigned int nr_bytes, unsigned int bidi_bytes)
{
	if (blk_update_bidi_request(rq, error, nr_bytes, bidi_bytes))
    1d08:	4b ff fa 39 	bl      1740 <blk_update_bidi_request>
    1d0c:	2f 83 00 00 	cmpwi   cr7,r3,0
		return true;
    1d10:	38 60 00 01 	li      r3,1
 *     %true  - still buffers pending for this request
 **/
static bool __blk_end_bidi_request(struct request *rq, int error,
				   unsigned int nr_bytes, unsigned int bidi_bytes)
{
	if (blk_update_bidi_request(rq, error, nr_bytes, bidi_bytes))
    1d14:	40 be 00 14 	bne+    cr7,1d28 <__blk_end_request+0x3c>
		return true;

	blk_finish_request(rq, error);
    1d18:	7f c3 f3 78 	mr      r3,r30
    1d1c:	7f e4 fb 78 	mr      r4,r31
    1d20:	4b ff fd 2d 	bl      1a4c <blk_finish_request>

	return false;
    1d24:	38 60 00 00 	li      r3,0
 *     %true  - still buffers pending for this request
 **/
bool __blk_end_request(struct request *rq, int error, unsigned int nr_bytes)
{
	return __blk_end_bidi_request(rq, error, nr_bytes, 0);
}
    1d28:	80 01 00 14 	lwz     r0,20(r1)
    1d2c:	bb c1 00 08 	lmw     r30,8(r1)
    1d30:	38 21 00 10 	addi    r1,r1,16
    1d34:	7c 08 03 a6 	mtlr    r0
    1d38:	4e 80 00 20 	blr

00001d3c <__blk_end_request_err>:
 * Return:
 *     %false - we are done with this request
 *     %true  - still buffers pending for this request
 */
bool __blk_end_request_err(struct request *rq, int error)
{
    1d3c:	94 21 ff e0 	stwu    r1,-32(r1)
    1d40:	7c 08 02 a6 	mflr    r0
    1d44:	90 01 00 24 	stw     r0,36(r1)
	WARN_ON(error >= 0);
    1d48:	7c 80 20 f8 	not     r0,r4
 * Return:
 *     %false - we are done with this request
 *     %true  - still buffers pending for this request
 */
bool __blk_end_request_err(struct request *rq, int error)
{
    1d4c:	93 e1 00 1c 	stw     r31,28(r1)
	WARN_ON(error >= 0);
    1d50:	54 00 0f fe 	rlwinm  r0,r0,1,31,31
 * Return:
 *     %false - we are done with this request
 *     %true  - still buffers pending for this request
 */
bool __blk_end_request_err(struct request *rq, int error)
{
    1d54:	7c 7f 1b 78 	mr      r31,r3
	WARN_ON(error >= 0);
    1d58:	0f 00 00 00 	twnei   r0,0
	return __blk_end_request(rq, error, blk_rq_err_bytes(rq));
    1d5c:	90 81 00 08 	stw     r4,8(r1)
    1d60:	48 00 00 01 	bl      1d60 <__blk_end_request_err+0x24>
}
    1d64:	80 01 00 24 	lwz     r0,36(r1)
 *     %true  - still buffers pending for this request
 */
bool __blk_end_request_err(struct request *rq, int error)
{
	WARN_ON(error >= 0);
	return __blk_end_request(rq, error, blk_rq_err_bytes(rq));
    1d68:	7c 65 1b 78 	mr      r5,r3
    1d6c:	7f e3 fb 78 	mr      r3,r31
    1d70:	80 81 00 08 	lwz     r4,8(r1)
}
    1d74:	7c 08 03 a6 	mtlr    r0
    1d78:	83 e1 00 1c 	lwz     r31,28(r1)
    1d7c:	38 21 00 20 	addi    r1,r1,32
 *     %true  - still buffers pending for this request
 */
bool __blk_end_request_err(struct request *rq, int error)
{
	WARN_ON(error >= 0);
	return __blk_end_request(rq, error, blk_rq_err_bytes(rq));
    1d80:	48 00 00 00 	b       1d80 <__blk_end_request_err+0x44>

00001d84 <__blk_end_request_cur>:

	blk_requestq_cachep = kmem_cache_create("blkdev_queue",
			sizeof(struct request_queue), 0, SLAB_PANIC, NULL);

	return 0;
}
    1d84:	81 23 00 40 	lwz     r9,64(r3)
	return rq->__data_len;
}

static inline int blk_rq_cur_bytes(const struct request *rq)
{
	return rq->bio ? bio_cur_bytes(rq->bio) : 0;
    1d88:	38 a0 00 00 	li      r5,0
    1d8c:	2f 89 00 00 	cmpwi   cr7,r9,0
    1d90:	41 9e 00 2c 	beq-    cr7,1dbc <__blk_end_request_cur+0x38>
#define bio_segments(bio)	((bio)->bi_vcnt - (bio)->bi_idx)
#define bio_sectors(bio)	((bio)->bi_size >> 9)

static inline unsigned int bio_cur_bytes(struct bio *bio)
{
	if (bio->bi_vcnt)
    1d94:	a0 09 00 18 	lhz     r0,24(r9)
    1d98:	2f 80 00 00 	cmpwi   cr7,r0,0
    1d9c:	41 9e 00 24 	beq-    cr7,1dc0 <__blk_end_request_cur+0x3c>
		return bio_iovec(bio)->bv_len;
    1da0:	a0 09 00 1a 	lhz     r0,26(r9)
    1da4:	81 29 00 38 	lwz     r9,56(r9)
    1da8:	54 0b 10 3a 	rlwinm  r11,r0,2,0,29
    1dac:	54 00 20 36 	rlwinm  r0,r0,4,0,27
    1db0:	7c 0b 00 50 	subf    r0,r11,r0
    1db4:	7d 29 02 14 	add     r9,r9,r0
    1db8:	80 a9 00 04 	lwz     r5,4(r9)
 *     %false - we are done with this request
 *     %true  - still buffers pending for this request
 */
bool __blk_end_request_cur(struct request *rq, int error)
{
	return __blk_end_request(rq, error, blk_rq_cur_bytes(rq));
    1dbc:	48 00 00 00 	b       1dbc <__blk_end_request_cur+0x38>
	else /* dataless requests such as discard */
		return bio->bi_size;
    1dc0:	80 a9 00 20 	lwz     r5,32(r9)
    1dc4:	48 00 00 00 	b       1dc4 <__blk_end_request_cur+0x40>

00001dc8 <blk_put_request>:
	}
}
EXPORT_SYMBOL_GPL(__blk_put_request);

void blk_put_request(struct request *req)
{
    1dc8:	94 21 ff f0 	stwu    r1,-16(r1)
    1dcc:	7c 08 02 a6 	mflr    r0
    1dd0:	7c 64 1b 78 	mr      r4,r3
    1dd4:	90 01 00 14 	stw     r0,20(r1)
    1dd8:	93 e1 00 0c 	stw     r31,12(r1)
	unsigned long flags;
	struct request_queue *q = req->q;
    1ddc:	80 63 00 1c 	lwz     r3,28(r3)

#define SET_MSR_EE(x)	mtmsr(x)

static inline unsigned long arch_local_save_flags(void)
{
	return mfmsr();
    1de0:	7f e0 00 a6 	mfmsr   r31

static inline unsigned long arch_local_irq_save(void)
{
	unsigned long flags = arch_local_save_flags();
#ifdef CONFIG_BOOKE
	asm volatile("wrteei 0" : : : "memory");
    1de4:	7c 00 01 46 	.long 0x7c000146

	spin_lock_irqsave(q->queue_lock, flags);
	__blk_put_request(q, req);
    1de8:	48 00 00 01 	bl      1de8 <blk_put_request+0x20>
}

static inline void arch_local_irq_restore(unsigned long flags)
{
#if defined(CONFIG_BOOKE)
	asm volatile("wrtee %0" : : "r" (flags) : "memory");
    1dec:	7f e0 01 06 	.long 0x7fe00106
	spin_unlock_irqrestore(q->queue_lock, flags);
}
    1df0:	80 01 00 14 	lwz     r0,20(r1)
    1df4:	83 e1 00 0c 	lwz     r31,12(r1)
    1df8:	38 21 00 10 	addi    r1,r1,16
    1dfc:	7c 08 03 a6 	mtlr    r0
    1e00:	4e 80 00 20 	blr

00001e04 <__blk_end_request_all>:
 *
 * Description:
 *     Completely finish @rq.  Must be called with queue lock held.
 */
void __blk_end_request_all(struct request *rq, int error)
{
    1e04:	94 21 ff f0 	stwu    r1,-16(r1)
    1e08:	7c 08 02 a6 	mflr    r0
	bool pending;
	unsigned int bidi_bytes = 0;
    1e0c:	38 c0 00 00 	li      r6,0
 *
 * Description:
 *     Completely finish @rq.  Must be called with queue lock held.
 */
void __blk_end_request_all(struct request *rq, int error)
{
    1e10:	bf c1 00 08 	stmw    r30,8(r1)
    1e14:	7c 7f 1b 78 	mr      r31,r3
    1e18:	7c 9e 23 78 	mr      r30,r4
    1e1c:	90 01 00 14 	stw     r0,20(r1)
	bool pending;
	unsigned int bidi_bytes = 0;

	if (unlikely(blk_bidi_rq(rq)))
    1e20:	81 23 00 d0 	lwz     r9,208(r3)
    1e24:	2f 89 00 00 	cmpwi   cr7,r9,0
    1e28:	40 9e 00 60 	bne-    cr7,1e88 <__blk_end_request_all+0x84>
 *     %true  - still buffers pending for this request
 **/
static bool __blk_end_bidi_request(struct request *rq, int error,
				   unsigned int nr_bytes, unsigned int bidi_bytes)
{
	if (blk_update_bidi_request(rq, error, nr_bytes, bidi_bytes))
    1e2c:	80 bf 00 30 	lwz     r5,48(r31)
    1e30:	7f e3 fb 78 	mr      r3,r31
    1e34:	7f c4 f3 78 	mr      r4,r30
    1e38:	4b ff f9 09 	bl      1740 <blk_update_bidi_request>

	if (unlikely(blk_bidi_rq(rq)))
		bidi_bytes = blk_rq_bytes(rq->next_rq);

	pending = __blk_end_bidi_request(rq, error, blk_rq_bytes(rq), bidi_bytes);
	BUG_ON(pending);
    1e3c:	38 00 00 01 	li      r0,1
 *     %true  - still buffers pending for this request
 **/
static bool __blk_end_bidi_request(struct request *rq, int error,
				   unsigned int nr_bytes, unsigned int bidi_bytes)
{
	if (blk_update_bidi_request(rq, error, nr_bytes, bidi_bytes))
    1e40:	2f 83 00 00 	cmpwi   cr7,r3,0
    1e44:	41 9e 00 1c 	beq-    cr7,1e60 <__blk_end_request_all+0x5c>

	if (unlikely(blk_bidi_rq(rq)))
		bidi_bytes = blk_rq_bytes(rq->next_rq);

	pending = __blk_end_bidi_request(rq, error, blk_rq_bytes(rq), bidi_bytes);
	BUG_ON(pending);
    1e48:	0f 00 00 00 	twnei   r0,0
}
    1e4c:	80 01 00 14 	lwz     r0,20(r1)
    1e50:	bb c1 00 08 	lmw     r30,8(r1)
    1e54:	38 21 00 10 	addi    r1,r1,16
    1e58:	7c 08 03 a6 	mtlr    r0
    1e5c:	4e 80 00 20 	blr
				   unsigned int nr_bytes, unsigned int bidi_bytes)
{
	if (blk_update_bidi_request(rq, error, nr_bytes, bidi_bytes))
		return true;

	blk_finish_request(rq, error);
    1e60:	7f e3 fb 78 	mr      r3,r31
    1e64:	7f c4 f3 78 	mr      r4,r30
    1e68:	4b ff fb e5 	bl      1a4c <blk_finish_request>

	if (unlikely(blk_bidi_rq(rq)))
		bidi_bytes = blk_rq_bytes(rq->next_rq);

	pending = __blk_end_bidi_request(rq, error, blk_rq_bytes(rq), bidi_bytes);
	BUG_ON(pending);
    1e6c:	38 00 00 00 	li      r0,0
    1e70:	0f 00 00 00 	twnei   r0,0
}
    1e74:	80 01 00 14 	lwz     r0,20(r1)
    1e78:	bb c1 00 08 	lmw     r30,8(r1)
    1e7c:	38 21 00 10 	addi    r1,r1,16
    1e80:	7c 08 03 a6 	mtlr    r0
    1e84:	4e 80 00 20 	blr

	blk_requestq_cachep = kmem_cache_create("blkdev_queue",
			sizeof(struct request_queue), 0, SLAB_PANIC, NULL);

	return 0;
}
    1e88:	80 c9 00 30 	lwz     r6,48(r9)
    1e8c:	4b ff ff a0 	b       1e2c <__blk_end_request_all+0x28>

00001e90 <blk_end_request_all>:
 *
 * Description:
 *     Completely finish @rq.
 */
void blk_end_request_all(struct request *rq, int error)
{
    1e90:	94 21 ff e0 	stwu    r1,-32(r1)
    1e94:	7c 08 02 a6 	mflr    r0
	bool pending;
	unsigned int bidi_bytes = 0;
    1e98:	38 c0 00 00 	li      r6,0
 *
 * Description:
 *     Completely finish @rq.
 */
void blk_end_request_all(struct request *rq, int error)
{
    1e9c:	bf a1 00 14 	stmw    r29,20(r1)
    1ea0:	7c 7f 1b 78 	mr      r31,r3
    1ea4:	7c 9e 23 78 	mr      r30,r4
    1ea8:	90 01 00 24 	stw     r0,36(r1)
	bool pending;
	unsigned int bidi_bytes = 0;

	if (unlikely(blk_bidi_rq(rq)))
    1eac:	81 23 00 d0 	lwz     r9,208(r3)
    1eb0:	2f 89 00 00 	cmpwi   cr7,r9,0
    1eb4:	40 9e 00 6c 	bne-    cr7,1f20 <blk_end_request_all+0x90>
				 unsigned int nr_bytes, unsigned int bidi_bytes)
{
	struct request_queue *q = rq->q;
	unsigned long flags;

	if (blk_update_bidi_request(rq, error, nr_bytes, bidi_bytes))
    1eb8:	80 bf 00 30 	lwz     r5,48(r31)
    1ebc:	7f e3 fb 78 	mr      r3,r31
    1ec0:	7f c4 f3 78 	mr      r4,r30
    1ec4:	4b ff f8 7d 	bl      1740 <blk_update_bidi_request>

	if (unlikely(blk_bidi_rq(rq)))
		bidi_bytes = blk_rq_bytes(rq->next_rq);

	pending = blk_end_bidi_request(rq, error, blk_rq_bytes(rq), bidi_bytes);
	BUG_ON(pending);
    1ec8:	38 00 00 01 	li      r0,1
				 unsigned int nr_bytes, unsigned int bidi_bytes)
{
	struct request_queue *q = rq->q;
	unsigned long flags;

	if (blk_update_bidi_request(rq, error, nr_bytes, bidi_bytes))
    1ecc:	2f 83 00 00 	cmpwi   cr7,r3,0
    1ed0:	41 9e 00 1c 	beq-    cr7,1eec <blk_end_request_all+0x5c>

	if (unlikely(blk_bidi_rq(rq)))
		bidi_bytes = blk_rq_bytes(rq->next_rq);

	pending = blk_end_bidi_request(rq, error, blk_rq_bytes(rq), bidi_bytes);
	BUG_ON(pending);
    1ed4:	0f 00 00 00 	twnei   r0,0
}
    1ed8:	80 01 00 24 	lwz     r0,36(r1)
    1edc:	bb a1 00 14 	lmw     r29,20(r1)
    1ee0:	38 21 00 20 	addi    r1,r1,32
    1ee4:	7c 08 03 a6 	mtlr    r0
    1ee8:	4e 80 00 20 	blr

#define SET_MSR_EE(x)	mtmsr(x)

static inline unsigned long arch_local_save_flags(void)
{
	return mfmsr();
    1eec:	7f a0 00 a6 	mfmsr   r29

static inline unsigned long arch_local_irq_save(void)
{
	unsigned long flags = arch_local_save_flags();
#ifdef CONFIG_BOOKE
	asm volatile("wrteei 0" : : : "memory");
    1ef0:	7c 00 01 46 	.long 0x7c000146

	if (blk_update_bidi_request(rq, error, nr_bytes, bidi_bytes))
		return true;

	spin_lock_irqsave(q->queue_lock, flags);
	blk_finish_request(rq, error);
    1ef4:	7f e3 fb 78 	mr      r3,r31
    1ef8:	7f c4 f3 78 	mr      r4,r30
    1efc:	4b ff fb 51 	bl      1a4c <blk_finish_request>
}

static inline void arch_local_irq_restore(unsigned long flags)
{
#if defined(CONFIG_BOOKE)
	asm volatile("wrtee %0" : : "r" (flags) : "memory");
    1f00:	7f a0 01 06 	.long 0x7fa00106

	if (unlikely(blk_bidi_rq(rq)))
		bidi_bytes = blk_rq_bytes(rq->next_rq);

	pending = blk_end_bidi_request(rq, error, blk_rq_bytes(rq), bidi_bytes);
	BUG_ON(pending);
    1f04:	38 00 00 00 	li      r0,0
    1f08:	0f 00 00 00 	twnei   r0,0
}
    1f0c:	80 01 00 24 	lwz     r0,36(r1)
    1f10:	bb a1 00 14 	lmw     r29,20(r1)
    1f14:	38 21 00 20 	addi    r1,r1,32
    1f18:	7c 08 03 a6 	mtlr    r0
    1f1c:	4e 80 00 20 	blr

	blk_requestq_cachep = kmem_cache_create("blkdev_queue",
			sizeof(struct request_queue), 0, SLAB_PANIC, NULL);

	return 0;
}
    1f20:	80 c9 00 30 	lwz     r6,48(r9)
    1f24:	4b ff ff 94 	b       1eb8 <blk_end_request_all+0x28>

00001f28 <blk_end_request>:
 * Return:
 *     %false - we are done with this request
 *     %true  - still buffers pending for this request
 **/
bool blk_end_request(struct request *rq, int error, unsigned int nr_bytes)
{
    1f28:	94 21 ff e0 	stwu    r1,-32(r1)
    1f2c:	7c 08 02 a6 	mflr    r0
				 unsigned int nr_bytes, unsigned int bidi_bytes)
{
	struct request_queue *q = rq->q;
	unsigned long flags;

	if (blk_update_bidi_request(rq, error, nr_bytes, bidi_bytes))
    1f30:	38 c0 00 00 	li      r6,0
 * Return:
 *     %false - we are done with this request
 *     %true  - still buffers pending for this request
 **/
bool blk_end_request(struct request *rq, int error, unsigned int nr_bytes)
{
    1f34:	bf a1 00 14 	stmw    r29,20(r1)
    1f38:	7c 7e 1b 78 	mr      r30,r3
    1f3c:	7c 9f 23 78 	mr      r31,r4
    1f40:	90 01 00 24 	stw     r0,36(r1)
				 unsigned int nr_bytes, unsigned int bidi_bytes)
{
	struct request_queue *q = rq->q;
	unsigned long flags;

	if (blk_update_bidi_request(rq, error, nr_bytes, bidi_bytes))
    1f44:	4b ff f7 fd 	bl      1740 <blk_update_bidi_request>
    1f48:	2f 83 00 00 	cmpwi   cr7,r3,0
		return true;
    1f4c:	38 60 00 01 	li      r3,1
				 unsigned int nr_bytes, unsigned int bidi_bytes)
{
	struct request_queue *q = rq->q;
	unsigned long flags;

	if (blk_update_bidi_request(rq, error, nr_bytes, bidi_bytes))
    1f50:	40 be 00 20 	bne+    cr7,1f70 <blk_end_request+0x48>

#define SET_MSR_EE(x)	mtmsr(x)

static inline unsigned long arch_local_save_flags(void)
{
	return mfmsr();
    1f54:	7f a0 00 a6 	mfmsr   r29

static inline unsigned long arch_local_irq_save(void)
{
	unsigned long flags = arch_local_save_flags();
#ifdef CONFIG_BOOKE
	asm volatile("wrteei 0" : : : "memory");
    1f58:	7c 00 01 46 	.long 0x7c000146
		return true;

	spin_lock_irqsave(q->queue_lock, flags);
	blk_finish_request(rq, error);
    1f5c:	7f c3 f3 78 	mr      r3,r30
    1f60:	7f e4 fb 78 	mr      r4,r31
    1f64:	4b ff fa e9 	bl      1a4c <blk_finish_request>
}

static inline void arch_local_irq_restore(unsigned long flags)
{
#if defined(CONFIG_BOOKE)
	asm volatile("wrtee %0" : : "r" (flags) : "memory");
    1f68:	7f a0 01 06 	.long 0x7fa00106
    1f6c:	38 60 00 00 	li      r3,0
 *     %true  - still buffers pending for this request
 **/
bool blk_end_request(struct request *rq, int error, unsigned int nr_bytes)
{
	return blk_end_bidi_request(rq, error, nr_bytes, 0);
}
    1f70:	80 01 00 24 	lwz     r0,36(r1)
    1f74:	bb a1 00 14 	lmw     r29,20(r1)
    1f78:	38 21 00 20 	addi    r1,r1,32
    1f7c:	7c 08 03 a6 	mtlr    r0
    1f80:	4e 80 00 20 	blr

00001f84 <blk_end_request_err>:
 * Return:
 *     %false - we are done with this request
 *     %true  - still buffers pending for this request
 */
bool blk_end_request_err(struct request *rq, int error)
{
    1f84:	94 21 ff e0 	stwu    r1,-32(r1)
    1f88:	7c 08 02 a6 	mflr    r0
    1f8c:	90 01 00 24 	stw     r0,36(r1)
	WARN_ON(error >= 0);
    1f90:	7c 80 20 f8 	not     r0,r4
 * Return:
 *     %false - we are done with this request
 *     %true  - still buffers pending for this request
 */
bool blk_end_request_err(struct request *rq, int error)
{
    1f94:	93 e1 00 1c 	stw     r31,28(r1)
	WARN_ON(error >= 0);
    1f98:	54 00 0f fe 	rlwinm  r0,r0,1,31,31
 * Return:
 *     %false - we are done with this request
 *     %true  - still buffers pending for this request
 */
bool blk_end_request_err(struct request *rq, int error)
{
    1f9c:	7c 7f 1b 78 	mr      r31,r3
	WARN_ON(error >= 0);
    1fa0:	0f 00 00 00 	twnei   r0,0
	return blk_end_request(rq, error, blk_rq_err_bytes(rq));
    1fa4:	90 81 00 08 	stw     r4,8(r1)
    1fa8:	48 00 00 01 	bl      1fa8 <blk_end_request_err+0x24>
}
    1fac:	80 01 00 24 	lwz     r0,36(r1)
 *     %true  - still buffers pending for this request
 */
bool blk_end_request_err(struct request *rq, int error)
{
	WARN_ON(error >= 0);
	return blk_end_request(rq, error, blk_rq_err_bytes(rq));
    1fb0:	7c 65 1b 78 	mr      r5,r3
    1fb4:	7f e3 fb 78 	mr      r3,r31
    1fb8:	80 81 00 08 	lwz     r4,8(r1)
}
    1fbc:	7c 08 03 a6 	mtlr    r0
    1fc0:	83 e1 00 1c 	lwz     r31,28(r1)
    1fc4:	38 21 00 20 	addi    r1,r1,32
 *     %true  - still buffers pending for this request
 */
bool blk_end_request_err(struct request *rq, int error)
{
	WARN_ON(error >= 0);
	return blk_end_request(rq, error, blk_rq_err_bytes(rq));
    1fc8:	48 00 00 00 	b       1fc8 <blk_end_request_err+0x44>

00001fcc <blk_end_request_cur>:

	blk_requestq_cachep = kmem_cache_create("blkdev_queue",
			sizeof(struct request_queue), 0, SLAB_PANIC, NULL);

	return 0;
}
    1fcc:	81 23 00 40 	lwz     r9,64(r3)
    1fd0:	38 a0 00 00 	li      r5,0
    1fd4:	2f 89 00 00 	cmpwi   cr7,r9,0
    1fd8:	41 9e 00 2c 	beq-    cr7,2004 <blk_end_request_cur+0x38>
#define bio_segments(bio)	((bio)->bi_vcnt - (bio)->bi_idx)
#define bio_sectors(bio)	((bio)->bi_size >> 9)

static inline unsigned int bio_cur_bytes(struct bio *bio)
{
	if (bio->bi_vcnt)
    1fdc:	a0 09 00 18 	lhz     r0,24(r9)
    1fe0:	2f 80 00 00 	cmpwi   cr7,r0,0
    1fe4:	41 9e 00 24 	beq-    cr7,2008 <blk_end_request_cur+0x3c>
		return bio_iovec(bio)->bv_len;
    1fe8:	a0 09 00 1a 	lhz     r0,26(r9)
    1fec:	81 29 00 38 	lwz     r9,56(r9)
    1ff0:	54 0b 10 3a 	rlwinm  r11,r0,2,0,29
    1ff4:	54 00 20 36 	rlwinm  r0,r0,4,0,27
    1ff8:	7c 0b 00 50 	subf    r0,r11,r0
    1ffc:	7d 29 02 14 	add     r9,r9,r0
    2000:	80 a9 00 04 	lwz     r5,4(r9)
 *     %false - we are done with this request
 *     %true  - still buffers pending for this request
 */
bool blk_end_request_cur(struct request *rq, int error)
{
	return blk_end_request(rq, error, blk_rq_cur_bytes(rq));
    2004:	48 00 00 00 	b       2004 <blk_end_request_cur+0x38>
	else /* dataless requests such as discard */
		return bio->bi_size;
    2008:	80 a9 00 20 	lwz     r5,32(r9)
    200c:	48 00 00 00 	b       200c <blk_end_request_cur+0x40>

00002010 <blk_remove_plug>:
/*
 * remove the queue from the plugged list, if present. called with
 * queue lock held and interrupts disabled.
 */
int blk_remove_plug(struct request_queue *q)
{
    2010:	94 21 ff f0 	stwu    r1,-16(r1)
    2014:	7c 08 02 a6 	mflr    r0
    2018:	7c 69 1b 78 	mr      r9,r3
    201c:	90 01 00 14 	stw     r0,20(r1)

#define SET_MSR_EE(x)	mtmsr(x)

static inline unsigned long arch_local_save_flags(void)
{
	return mfmsr();
    2020:	7c 00 00 a6 	mfmsr   r0
	WARN_ON(!irqs_disabled());
    2024:	54 00 8f fe 	rlwinm  r0,r0,17,31,31
    2028:	0f 00 00 00 	twnei   r0,0
 * @nr: bit number to test
 * @addr: Address to start counting from
 */
static inline int test_bit(int nr, const volatile unsigned long *addr)
{
	return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    202c:	80 03 01 78 	lwz     r0,376(r3)

	if (!queue_flag_test_and_clear(QUEUE_FLAG_PLUGGED, q))
		return 0;
    2030:	38 60 00 00 	li      r3,0
static inline int queue_flag_test_and_clear(unsigned int flag,
					    struct request_queue *q)
{
	WARN_ON_ONCE(!queue_is_locked(q));

	if (test_bit(flag, &q->queue_flags)) {
    2034:	70 0b 00 80 	andi.   r11,r0,128
    2038:	41 82 00 1c 	beq-    2054 <blk_remove_plug+0x44>
static inline void __clear_bit(int nr, volatile unsigned long *addr)
{
	unsigned long mask = BIT_MASK(nr);
	unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);

	*p &= ~mask;
    203c:	80 09 01 78 	lwz     r0,376(r9)

	del_timer(&q->unplug_timer);
    2040:	38 69 00 6c 	addi    r3,r9,108
    2044:	54 00 06 6e 	rlwinm  r0,r0,0,25,23
    2048:	90 09 01 78 	stw     r0,376(r9)
    204c:	48 00 00 01 	bl      204c <blk_remove_plug+0x3c>
    2050:	38 60 00 01 	li      r3,1
	return 1;
}
    2054:	80 01 00 14 	lwz     r0,20(r1)
    2058:	38 21 00 10 	addi    r1,r1,16
    205c:	7c 08 03 a6 	mtlr    r0
    2060:	4e 80 00 20 	blr

00002064 <blk_stop_queue>:
 *   call this function to prevent the request_fn from being called until
 *   the driver has signalled it's ready to go again. This happens by calling
 *   blk_start_queue() to restart queue operations. Queue lock must be held.
 **/
void blk_stop_queue(struct request_queue *q)
{
    2064:	94 21 ff f0 	stwu    r1,-16(r1)
    2068:	7c 08 02 a6 	mflr    r0
    206c:	93 e1 00 0c 	stw     r31,12(r1)
    2070:	7c 7f 1b 78 	mr      r31,r3
    2074:	90 01 00 14 	stw     r0,20(r1)
	blk_remove_plug(q);
    2078:	48 00 00 01 	bl      2078 <blk_stop_queue+0x14>
static inline void __set_bit(int nr, volatile unsigned long *addr)
{
	unsigned long mask = BIT_MASK(nr);
	unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);

	*p  |= mask;
    207c:	80 1f 01 78 	lwz     r0,376(r31)
    2080:	60 00 00 04 	ori     r0,r0,4
    2084:	90 1f 01 78 	stw     r0,376(r31)
	queue_flag_set(QUEUE_FLAG_STOPPED, q);
}
    2088:	80 01 00 14 	lwz     r0,20(r1)
    208c:	83 e1 00 0c 	lwz     r31,12(r1)
    2090:	38 21 00 10 	addi    r1,r1,16
    2094:	7c 08 03 a6 	mtlr    r0
    2098:	4e 80 00 20 	blr

0000209c <__blk_run_queue>:
 *    See @blk_run_queue. This variant must be called with the queue lock
 *    held and interrupts disabled.
 *
 */
void __blk_run_queue(struct request_queue *q, bool force_kblockd)
{
    209c:	94 21 ff f0 	stwu    r1,-16(r1)
    20a0:	7c 08 02 a6 	mflr    r0
    20a4:	bf c1 00 08 	stmw    r30,8(r1)
    20a8:	7c 7f 1b 78 	mr      r31,r3
    20ac:	7c 9e 23 78 	mr      r30,r4
    20b0:	90 01 00 14 	stw     r0,20(r1)
	blk_remove_plug(q);
    20b4:	48 00 00 01 	bl      20b4 <__blk_run_queue+0x18>
 * @nr: bit number to test
 * @addr: Address to start counting from
 */
static inline int test_bit(int nr, const volatile unsigned long *addr)
{
	return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    20b8:	80 1f 01 78 	lwz     r0,376(r31)

	if (unlikely(blk_queue_stopped(q)))
    20bc:	70 09 00 04 	andi.   r9,r0,4
    20c0:	40 82 00 50 	bne-    2110 <__blk_run_queue+0x74>
		return;

	if (elv_queue_empty(q))
    20c4:	7f e3 fb 78 	mr      r3,r31
    20c8:	48 00 00 01 	bl      20c8 <__blk_run_queue+0x2c>
    20cc:	2f 83 00 00 	cmpwi   cr7,r3,0
    20d0:	40 9e 00 40 	bne-    cr7,2110 <__blk_run_queue+0x74>

	/*
	 * Only recurse once to avoid overrunning the stack, let the unplug
	 * handling reinvoke the handler shortly if we already got there.
	 */
	if (!force_kblockd && !queue_flag_test_and_set(QUEUE_FLAG_REENTER, q)) {
    20d4:	2f 9e 00 00 	cmpwi   cr7,r30,0
    20d8:	40 9e 00 4c 	bne-    cr7,2124 <__blk_run_queue+0x88>
    20dc:	80 1f 01 78 	lwz     r0,376(r31)
static inline int queue_flag_test_and_set(unsigned int flag,
					  struct request_queue *q)
{
	WARN_ON_ONCE(!queue_is_locked(q));

	if (!test_bit(flag, &q->queue_flags)) {
    20e0:	70 09 00 40 	andi.   r9,r0,64
    20e4:	40 82 00 40 	bne-    2124 <__blk_run_queue+0x88>
static inline void __set_bit(int nr, volatile unsigned long *addr)
{
	unsigned long mask = BIT_MASK(nr);
	unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);

	*p  |= mask;
    20e8:	81 3f 01 78 	lwz     r9,376(r31)
		q->request_fn(q);
    20ec:	7f e3 fb 78 	mr      r3,r31
    20f0:	80 1f 00 38 	lwz     r0,56(r31)
    20f4:	61 29 00 40 	ori     r9,r9,64
    20f8:	7c 09 03 a6 	mtctr   r0
    20fc:	91 3f 01 78 	stw     r9,376(r31)
    2100:	4e 80 04 21 	bctrl
static inline void __clear_bit(int nr, volatile unsigned long *addr)
{
	unsigned long mask = BIT_MASK(nr);
	unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);

	*p &= ~mask;
    2104:	80 1f 01 78 	lwz     r0,376(r31)
    2108:	54 00 06 b0 	rlwinm  r0,r0,0,26,24
    210c:	90 1f 01 78 	stw     r0,376(r31)
		queue_flag_clear(QUEUE_FLAG_REENTER, q);
	} else {
		queue_flag_set(QUEUE_FLAG_PLUGGED, q);
		kblockd_schedule_work(q, &q->unplug_work);
	}
}
    2110:	80 01 00 14 	lwz     r0,20(r1)
    2114:	bb c1 00 08 	lmw     r30,8(r1)
    2118:	38 21 00 10 	addi    r1,r1,16
    211c:	7c 08 03 a6 	mtlr    r0
    2120:	4e 80 00 20 	blr
static inline void __set_bit(int nr, volatile unsigned long *addr)
{
	unsigned long mask = BIT_MASK(nr);
	unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);

	*p  |= mask;
    2124:	80 1f 01 78 	lwz     r0,376(r31)
	if (!force_kblockd && !queue_flag_test_and_set(QUEUE_FLAG_REENTER, q)) {
		q->request_fn(q);
		queue_flag_clear(QUEUE_FLAG_REENTER, q);
	} else {
		queue_flag_set(QUEUE_FLAG_PLUGGED, q);
		kblockd_schedule_work(q, &q->unplug_work);
    2128:	7f e3 fb 78 	mr      r3,r31
    212c:	38 9f 00 90 	addi    r4,r31,144
    2130:	60 00 00 80 	ori     r0,r0,128
    2134:	90 1f 01 78 	stw     r0,376(r31)
	}
}
    2138:	80 01 00 14 	lwz     r0,20(r1)
    213c:	bb c1 00 08 	lmw     r30,8(r1)
    2140:	38 21 00 10 	addi    r1,r1,16
    2144:	7c 08 03 a6 	mtlr    r0
	if (!force_kblockd && !queue_flag_test_and_set(QUEUE_FLAG_REENTER, q)) {
		q->request_fn(q);
		queue_flag_clear(QUEUE_FLAG_REENTER, q);
	} else {
		queue_flag_set(QUEUE_FLAG_PLUGGED, q);
		kblockd_schedule_work(q, &q->unplug_work);
    2148:	48 00 00 00 	b       2148 <__blk_run_queue+0xac>

0000214c <blk_insert_request>:
 *    of the queue for things like a QUEUE_FULL message from a device, or a
 *    host that is unable to accept a particular command.
 */
void blk_insert_request(struct request_queue *q, struct request *rq,
			int at_head, void *data)
{
    214c:	94 21 ff e0 	stwu    r1,-32(r1)
    2150:	7c 08 02 a6 	mflr    r0
	int where = at_head ? ELEVATOR_INSERT_FRONT : ELEVATOR_INSERT_BACK;
    2154:	7c a5 00 34 	cntlzw  r5,r5
    2158:	54 a5 d9 7e 	rlwinm  r5,r5,27,5,31
 *    of the queue for things like a QUEUE_FULL message from a device, or a
 *    host that is unable to accept a particular command.
 */
void blk_insert_request(struct request_queue *q, struct request *rq,
			int at_head, void *data)
{
    215c:	bf 81 00 10 	stmw    r28,16(r1)
    2160:	7c 9d 23 78 	mr      r29,r4
    2164:	7c 7e 1b 78 	mr      r30,r3
    2168:	90 01 00 24 	stw     r0,36(r1)
	/*
	 * tell I/O scheduler that this isn't a regular read/write (ie it
	 * must not attempt merges on this) and that it acts as a soft
	 * barrier
	 */
	rq->cmd_type = REQ_TYPE_SPECIAL;
    216c:	38 00 00 07 	li      r0,7
 *    host that is unable to accept a particular command.
 */
void blk_insert_request(struct request_queue *q, struct request *rq,
			int at_head, void *data)
{
	int where = at_head ? ELEVATOR_INSERT_FRONT : ELEVATOR_INSERT_BACK;
    2170:	3b 85 00 01 	addi    r28,r5,1
	/*
	 * tell I/O scheduler that this isn't a regular read/write (ie it
	 * must not attempt merges on this) and that it acts as a soft
	 * barrier
	 */
	rq->cmd_type = REQ_TYPE_SPECIAL;
    2174:	90 04 00 24 	stw     r0,36(r4)

	rq->special = data;
    2178:	90 c4 00 7c 	stw     r6,124(r4)
    217c:	7f e0 00 a6 	mfmsr   r31

static inline unsigned long arch_local_irq_save(void)
{
	unsigned long flags = arch_local_save_flags();
#ifdef CONFIG_BOOKE
	asm volatile("wrteei 0" : : : "memory");
    2180:	7c 00 01 46 	.long 0x7c000146
	spin_lock_irqsave(q->queue_lock, flags);

	/*
	 * If command is tagged, release the tag
	 */
	if (blk_rq_tagged(rq))
    2184:	80 04 00 20 	lwz     r0,32(r4)
    2188:	74 09 00 02 	andis.  r9,r0,2
    218c:	40 82 00 48 	bne-    21d4 <blk_insert_request+0x88>
		blk_queue_end_tag(q, rq);

	drive_stat_acct(rq, 1);
    2190:	7f a3 eb 78 	mr      r3,r29
    2194:	38 80 00 01 	li      r4,1
    2198:	4b ff e9 1d 	bl      ab4 <drive_stat_acct>
	__elv_add_request(q, rq, where, 0);
    219c:	7f c3 f3 78 	mr      r3,r30
    21a0:	7f a4 eb 78 	mr      r4,r29
    21a4:	7f 85 e3 78 	mr      r5,r28
    21a8:	38 c0 00 00 	li      r6,0
    21ac:	48 00 00 01 	bl      21ac <blk_insert_request+0x60>
	__blk_run_queue(q, false);
    21b0:	7f c3 f3 78 	mr      r3,r30
    21b4:	38 80 00 00 	li      r4,0
    21b8:	48 00 00 01 	bl      21b8 <blk_insert_request+0x6c>
}

static inline void arch_local_irq_restore(unsigned long flags)
{
#if defined(CONFIG_BOOKE)
	asm volatile("wrtee %0" : : "r" (flags) : "memory");
    21bc:	7f e0 01 06 	.long 0x7fe00106
	spin_unlock_irqrestore(q->queue_lock, flags);
}
    21c0:	80 01 00 24 	lwz     r0,36(r1)
    21c4:	bb 81 00 10 	lmw     r28,16(r1)
    21c8:	38 21 00 20 	addi    r1,r1,32
    21cc:	7c 08 03 a6 	mtlr    r0
    21d0:	4e 80 00 20 	blr

	/*
	 * If command is tagged, release the tag
	 */
	if (blk_rq_tagged(rq))
		blk_queue_end_tag(q, rq);
    21d4:	48 00 00 01 	bl      21d4 <blk_insert_request+0x88>
    21d8:	4b ff ff b8 	b       2190 <blk_insert_request+0x44>

000021dc <blk_run_queue>:
 * Description:
 *    Invoke request handling on this queue, if it has pending work to do.
 *    May be used to restart queueing when a request has completed.
 */
void blk_run_queue(struct request_queue *q)
{
    21dc:	94 21 ff f0 	stwu    r1,-16(r1)
    21e0:	7c 08 02 a6 	mflr    r0
    21e4:	93 e1 00 0c 	stw     r31,12(r1)
    21e8:	90 01 00 14 	stw     r0,20(r1)

#define SET_MSR_EE(x)	mtmsr(x)

static inline unsigned long arch_local_save_flags(void)
{
	return mfmsr();
    21ec:	7f e0 00 a6 	mfmsr   r31

static inline unsigned long arch_local_irq_save(void)
{
	unsigned long flags = arch_local_save_flags();
#ifdef CONFIG_BOOKE
	asm volatile("wrteei 0" : : : "memory");
    21f0:	7c 00 01 46 	.long 0x7c000146
	unsigned long flags;

	spin_lock_irqsave(q->queue_lock, flags);
	__blk_run_queue(q, false);
    21f4:	38 80 00 00 	li      r4,0
    21f8:	48 00 00 01 	bl      21f8 <blk_run_queue+0x1c>
}

static inline void arch_local_irq_restore(unsigned long flags)
{
#if defined(CONFIG_BOOKE)
	asm volatile("wrtee %0" : : "r" (flags) : "memory");
    21fc:	7f e0 01 06 	.long 0x7fe00106
	spin_unlock_irqrestore(q->queue_lock, flags);
}
    2200:	80 01 00 14 	lwz     r0,20(r1)
    2204:	83 e1 00 0c 	lwz     r31,12(r1)
    2208:	38 21 00 10 	addi    r1,r1,16
    220c:	7c 08 03 a6 	mtlr    r0
    2210:	4e 80 00 20 	blr

00002214 <blk_start_queue>:
 *   blk_start_queue() will clear the stop flag on the queue, and call
 *   the request_fn for the queue if it was in a stopped state when
 *   entered. Also see blk_stop_queue(). Queue lock must be held.
 **/
void blk_start_queue(struct request_queue *q)
{
    2214:	7c 69 1b 78 	mr      r9,r3

#define SET_MSR_EE(x)	mtmsr(x)

static inline unsigned long arch_local_save_flags(void)
{
	return mfmsr();
    2218:	7c 00 00 a6 	mfmsr   r0
	WARN_ON(!irqs_disabled());
    221c:	54 00 8f fe 	rlwinm  r0,r0,17,31,31
    2220:	0f 00 00 00 	twnei   r0,0
static inline void __clear_bit(int nr, volatile unsigned long *addr)
{
	unsigned long mask = BIT_MASK(nr);
	unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);

	*p &= ~mask;
    2224:	80 03 01 78 	lwz     r0,376(r3)

	queue_flag_clear(QUEUE_FLAG_STOPPED, q);
	__blk_run_queue(q, false);
    2228:	38 80 00 00 	li      r4,0
    222c:	54 00 07 b8 	rlwinm  r0,r0,0,30,28
    2230:	90 09 01 78 	stw     r0,376(r9)
    2234:	48 00 00 00 	b       2234 <blk_start_queue+0x20>

00002238 <blk_plug_device>:
 *
 * This is called with interrupts off and no requests on the queue and
 * with the queue lock held.
 */
void blk_plug_device(struct request_queue *q)
{
    2238:	94 21 ff f0 	stwu    r1,-16(r1)
    223c:	7c 08 02 a6 	mflr    r0
    2240:	90 01 00 14 	stw     r0,20(r1)
    2244:	7c 00 00 a6 	mfmsr   r0
	WARN_ON(!irqs_disabled());
    2248:	54 00 8f fe 	rlwinm  r0,r0,17,31,31
    224c:	0f 00 00 00 	twnei   r0,0
 * @nr: bit number to test
 * @addr: Address to start counting from
 */
static inline int test_bit(int nr, const volatile unsigned long *addr)
{
	return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    2250:	80 03 01 78 	lwz     r0,376(r3)

	/*
	 * don't plug a stopped queue, it must be paired with blk_start_queue()
	 * which will restart the queueing
	 */
	if (blk_queue_stopped(q))
    2254:	70 09 00 04 	andi.   r9,r0,4
    2258:	40 82 00 34 	bne-    228c <blk_plug_device+0x54>
    225c:	80 03 01 78 	lwz     r0,376(r3)
    2260:	70 09 00 80 	andi.   r9,r0,128
    2264:	40 82 00 28 	bne-    228c <blk_plug_device+0x54>
static inline void __set_bit(int nr, volatile unsigned long *addr)
{
	unsigned long mask = BIT_MASK(nr);
	unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);

	*p  |= mask;
    2268:	81 23 01 78 	lwz     r9,376(r3)
		return;

	if (!queue_flag_test_and_set(QUEUE_FLAG_PLUGGED, q)) {
		mod_timer(&q->unplug_timer, jiffies + q->unplug_delay);
    226c:	3d 60 00 00 	lis     r11,0
    2270:	80 03 00 8c 	lwz     r0,140(r3)
    2274:	61 29 00 80 	ori     r9,r9,128
    2278:	80 8b 00 00 	lwz     r4,0(r11)
    227c:	91 23 01 78 	stw     r9,376(r3)
    2280:	38 63 00 6c 	addi    r3,r3,108
    2284:	7c 84 02 14 	add     r4,r4,r0
    2288:	48 00 00 01 	bl      2288 <blk_plug_device+0x50>
		trace_block_plug(q);
	}
}
    228c:	80 01 00 14 	lwz     r0,20(r1)
    2290:	38 21 00 10 	addi    r1,r1,16
    2294:	7c 08 03 a6 	mtlr    r0
    2298:	4e 80 00 20 	blr

0000229c <blk_plug_device_unlocked>:
 * Description:
 *   Like @blk_plug_device(), but grabs the queue lock and disables
 *   interrupts.
 **/
void blk_plug_device_unlocked(struct request_queue *q)
{
    229c:	94 21 ff f0 	stwu    r1,-16(r1)
    22a0:	7c 08 02 a6 	mflr    r0
    22a4:	93 e1 00 0c 	stw     r31,12(r1)
    22a8:	90 01 00 14 	stw     r0,20(r1)
    22ac:	7f e0 00 a6 	mfmsr   r31

static inline unsigned long arch_local_irq_save(void)
{
	unsigned long flags = arch_local_save_flags();
#ifdef CONFIG_BOOKE
	asm volatile("wrteei 0" : : : "memory");
    22b0:	7c 00 01 46 	.long 0x7c000146
	unsigned long flags;

	spin_lock_irqsave(q->queue_lock, flags);
	blk_plug_device(q);
    22b4:	48 00 00 01 	bl      22b4 <blk_plug_device_unlocked+0x18>
}

static inline void arch_local_irq_restore(unsigned long flags)
{
#if defined(CONFIG_BOOKE)
	asm volatile("wrtee %0" : : "r" (flags) : "memory");
    22b8:	7f e0 01 06 	.long 0x7fe00106
	spin_unlock_irqrestore(q->queue_lock, flags);
}
    22bc:	80 01 00 14 	lwz     r0,20(r1)
    22c0:	83 e1 00 0c 	lwz     r31,12(r1)
    22c4:	38 21 00 10 	addi    r1,r1,16
    22c8:	7c 08 03 a6 	mtlr    r0
    22cc:	4e 80 00 20 	blr

000022d0 <get_request.isra.52>:
/*
 * Get a free request, queue_lock must be held.
 * Returns NULL on failure, with queue_lock held.
 * Returns !NULL on success, with queue_lock *not held*.
 */
static struct request *get_request(struct request_queue *q, int rw_flags,
    22d0:	94 21 ff d0 	stwu    r1,-48(r1)
    22d4:	7d 80 00 26 	mfcr    r12
    22d8:	7c 08 02 a6 	mflr    r0
    22dc:	bf 01 00 10 	stmw    r24,16(r1)
/*
 * We regard a request as sync, if either a read or a sync write
 */
static inline bool rw_is_sync(unsigned int rw_flags)
{
	return !(rw_flags & REQ_WRITE) || (rw_flags & REQ_SYNC);
    22e0:	70 9a 00 11 	andi.   r26,r4,17
    22e4:	7c 9d 23 78 	mr      r29,r4
    22e8:	90 01 00 34 	stw     r0,52(r1)
    22ec:	7c 7f 1b 78 	mr      r31,r3
    22f0:	7c b8 2b 78 	mr      r24,r5
    22f4:	91 81 00 0c 	stw     r12,12(r1)
	struct request_list *rl = &q->rq;
	struct io_context *ioc = NULL;
	const bool is_sync = rw_is_sync(rw_flags) != 0;
	int may_queue, priv;

	may_queue = elv_may_queue(q, rw_flags);
    22f8:	48 00 00 01 	bl      22f8 <get_request.isra.52+0x28>
	if (may_queue == ELV_MQUEUE_NO)
    22fc:	2f 83 00 01 	cmpwi   cr7,r3,1
				   struct bio *bio, gfp_t gfp_mask)
{
	struct request *rq = NULL;
	struct request_list *rl = &q->rq;
	struct io_context *ioc = NULL;
	const bool is_sync = rw_is_sync(rw_flags) != 0;
    2300:	6b 5a 00 01 	xori    r26,r26,1
    2304:	7f 5a 00 34 	cntlzw  r26,r26
	int may_queue, priv;

	may_queue = elv_may_queue(q, rw_flags);
    2308:	7c 7e 1b 78 	mr      r30,r3
				   struct bio *bio, gfp_t gfp_mask)
{
	struct request *rq = NULL;
	struct request_list *rl = &q->rq;
	struct io_context *ioc = NULL;
	const bool is_sync = rw_is_sync(rw_flags) != 0;
    230c:	57 5a d9 7e 	rlwinm  r26,r26,27,5,31
    2310:	6b 5a 00 01 	xori    r26,r26,1
    2314:	57 5c 10 3a 	rlwinm  r28,r26,2,0,29
	int may_queue, priv;

	may_queue = elv_may_queue(q, rw_flags);
	if (may_queue == ELV_MQUEUE_NO)
    2318:	41 9e 01 ec 	beq-    cr7,2504 <get_request.isra.52+0x234>
		goto rq_starved;

	if (rl->count[is_sync]+1 >= queue_congestion_on_threshold(q)) {
    231c:	7d 3f e2 14 	add     r9,r31,r28
    2320:	81 7f 01 a8 	lwz     r11,424(r31)
static struct request *get_request(struct request_queue *q, int rw_flags,
				   struct bio *bio, gfp_t gfp_mask)
{
	struct request *rq = NULL;
	struct request_list *rl = &q->rq;
	struct io_context *ioc = NULL;
    2324:	3b 60 00 00 	li      r27,0

	may_queue = elv_may_queue(q, rw_flags);
	if (may_queue == ELV_MQUEUE_NO)
		goto rq_starved;

	if (rl->count[is_sync]+1 >= queue_congestion_on_threshold(q)) {
    2328:	81 29 00 10 	lwz     r9,16(r9)
    232c:	38 09 00 01 	addi    r0,r9,1
    2330:	7f 80 58 00 	cmpw    cr7,r0,r11
    2334:	40 9c 00 e8 	bge-    cr7,241c <get_request.isra.52+0x14c>
	/*
	 * Only allow batching queuers to allocate up to 50% over the defined
	 * limit of requests, otherwise we could have thousands of requests
	 * allocated with any setting of ->nr_requests
	 */
	if (rl->count[is_sync] >= (3 * q->nr_requests / 2))
    2338:	80 1f 01 a4 	lwz     r0,420(r31)
 * Returns !NULL on success, with queue_lock *not held*.
 */
static struct request *get_request(struct request_queue *q, int rw_flags,
				   struct bio *bio, gfp_t gfp_mask)
{
	struct request *rq = NULL;
    233c:	3b c0 00 00 	li      r30,0
	/*
	 * Only allow batching queuers to allocate up to 50% over the defined
	 * limit of requests, otherwise we could have thousands of requests
	 * allocated with any setting of ->nr_requests
	 */
	if (rl->count[is_sync] >= (3 * q->nr_requests / 2))
    2340:	54 0b 08 3c 	rlwinm  r11,r0,1,0,30
    2344:	7c 0b 02 14 	add     r0,r11,r0
    2348:	54 00 f8 7e 	rlwinm  r0,r0,31,1,31
    234c:	7f 89 00 40 	cmplw   cr7,r9,r0
    2350:	40 9c 00 ac 	bge-    cr7,23fc <get_request.isra.52+0x12c>
		goto out;

	rl->count[is_sync]++;
    2354:	7d 7f e2 14 	add     r11,r31,r28
    2358:	39 29 00 01 	addi    r9,r9,1
    235c:	91 2b 00 10 	stw     r9,16(r11)
	rl->starved[is_sync] = 0;
    2360:	93 cb 00 18 	stw     r30,24(r11)
 * @nr: bit number to test
 * @addr: Address to start counting from
 */
static inline int test_bit(int nr, const volatile unsigned long *addr)
{
	return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    2364:	83 3f 01 78 	lwz     r25,376(r31)
    2368:	57 39 c7 fe 	rlwinm  r25,r25,24,31,31

	priv = !test_bit(QUEUE_FLAG_ELVSWITCH, &q->queue_flags);
    236c:	6b 39 00 01 	xori    r25,r25,1
	if (priv)
    2370:	2e 19 00 00 	cmpwi   cr4,r25,0
    2374:	41 92 00 10 	beq-    cr4,2384 <get_request.isra.52+0xb4>
		rl->elvpriv++;
    2378:	81 3f 00 20 	lwz     r9,32(r31)
    237c:	38 09 00 01 	addi    r0,r9,1
    2380:	90 1f 00 20 	stw     r0,32(r31)
    2384:	80 1f 01 78 	lwz     r0,376(r31)

	if (blk_queue_io_stat(q))
    2388:	70 0b 80 00 	andi.   r11,r0,32768
    238c:	41 82 00 08 	beq-    2394 <get_request.isra.52+0xc4>
		rw_flags |= REQ_IO_STAT;
    2390:	67 bd 02 00 	oris    r29,r29,512
}

static inline void arch_local_irq_enable(void)
{
#ifdef CONFIG_BOOKE
	asm volatile("wrteei 1" : : : "memory");
    2394:	7c 00 81 46 	.long 0x7c008146
}

static struct request *
blk_alloc_request(struct request_queue *q, int flags, int priv, gfp_t gfp_mask)
{
	struct request *rq = mempool_alloc(q->rq.rq_pool, gfp_mask);
    2398:	80 7f 00 24 	lwz     r3,36(r31)
    239c:	7f 04 c3 78 	mr      r4,r24
    23a0:	48 00 00 01 	bl      23a0 <get_request.isra.52+0xd0>

	if (!rq)
    23a4:	7c 7e 1b 79 	mr.     r30,r3
    23a8:	41 82 01 48 	beq-    24f0 <get_request.isra.52+0x220>
		return NULL;

	blk_rq_init(q, rq);
    23ac:	7f e3 fb 78 	mr      r3,r31
    23b0:	7f c4 f3 78 	mr      r4,r30

	rq->cmd_flags = flags | REQ_ALLOCED;
    23b4:	67 bd 00 40 	oris    r29,r29,64
	struct request *rq = mempool_alloc(q->rq.rq_pool, gfp_mask);

	if (!rq)
		return NULL;

	blk_rq_init(q, rq);
    23b8:	48 00 00 01 	bl      23b8 <get_request.isra.52+0xe8>

	rq->cmd_flags = flags | REQ_ALLOCED;
    23bc:	93 be 00 20 	stw     r29,32(r30)

	if (priv) {
    23c0:	40 92 01 64 	bne-    cr4,2524 <get_request.isra.52+0x254>
 * ioc_batching returns true if the ioc is a valid batching request and
 * should be given priority access to a request.
 */
static inline int ioc_batching(struct request_queue *q, struct io_context *ioc)
{
	if (!ioc)
    23c4:	2f 9b 00 00 	cmpwi   cr7,r27,0
    23c8:	41 9e 00 34 	beq-    cr7,23fc <get_request.isra.52+0x12c>
	/*
	 * Make sure the process is able to allocate at least 1 request
	 * even if the batch times out, otherwise we could theoretically
	 * lose wakeups.
	 */
	return ioc->nr_batch_requests == q->nr_batching ||
    23cc:	81 3b 00 0c 	lwz     r9,12(r27)
    23d0:	80 1f 01 b0 	lwz     r0,432(r31)
    23d4:	7f 89 00 00 	cmpw    cr7,r9,r0
    23d8:	41 9e 00 e4 	beq-    cr7,24bc <get_request.isra.52+0x1ec>
    23dc:	2f 89 00 00 	cmpwi   cr7,r9,0
    23e0:	40 9d 00 1c 	ble-    cr7,23fc <get_request.isra.52+0x12c>
		(ioc->nr_batch_requests > 0
		&& time_before(jiffies, ioc->last_waited + BLK_BATCH_TIME));
    23e4:	81 7b 00 10 	lwz     r11,16(r27)
    23e8:	3d 40 00 00 	lis     r10,0
    23ec:	80 0a 00 00 	lwz     r0,0(r10)
    23f0:	21 6b ff fb 	subfic  r11,r11,-5
    23f4:	7d 0b 02 15 	add.    r8,r11,r0
    23f8:	41 80 00 c4 	blt-    24bc <get_request.isra.52+0x1ec>
		ioc->nr_batch_requests--;

	trace_block_getrq(q, bio, rw_flags & 1);
out:
	return rq;
}
    23fc:	80 01 00 34 	lwz     r0,52(r1)
    2400:	7f c3 f3 78 	mr      r3,r30
    2404:	81 81 00 0c 	lwz     r12,12(r1)
    2408:	7c 08 03 a6 	mtlr    r0
    240c:	bb 01 00 10 	lmw     r24,16(r1)
    2410:	38 21 00 30 	addi    r1,r1,48
    2414:	7d 80 81 20 	mtcrf   8,r12
    2418:	4e 80 00 20 	blr
	may_queue = elv_may_queue(q, rw_flags);
	if (may_queue == ELV_MQUEUE_NO)
		goto rq_starved;

	if (rl->count[is_sync]+1 >= queue_congestion_on_threshold(q)) {
		if (rl->count[is_sync]+1 >= q->nr_requests) {
    241c:	81 3f 01 a4 	lwz     r9,420(r31)
    2420:	7f 80 48 40 	cmplw   cr7,r0,r9
    2424:	41 9c 00 80 	blt-    cr7,24a4 <get_request.isra.52+0x1d4>
			ioc = current_io_context(GFP_ATOMIC, q->node);
    2428:	80 9f 02 48 	lwz     r4,584(r31)
    242c:	38 60 00 20 	li      r3,32
    2430:	48 00 00 01 	bl      2430 <get_request.isra.52+0x160>
	return rw_is_sync(rq->cmd_flags);
}

static inline int blk_queue_full(struct request_queue *q, int sync)
{
	if (sync)
    2434:	2f 9a 00 00 	cmpwi   cr7,r26,0
    2438:	80 1f 01 78 	lwz     r0,376(r31)
    243c:	7c 7b 1b 78 	mr      r27,r3
    2440:	41 9e 01 54 	beq-    cr7,2594 <get_request.isra.52+0x2c4>
    2444:	54 00 ef fe 	rlwinm  r0,r0,29,31,31
			 * The queue will fill after this allocation, so set
			 * it as full, and mark this process as "batching".
			 * This process will be allowed to complete a batch of
			 * requests, others will be blocked.
			 */
			if (!blk_queue_full(q, is_sync)) {
    2448:	2f 00 00 00 	cmpwi   cr6,r0,0
    244c:	40 9a 01 00 	bne-    cr6,254c <get_request.isra.52+0x27c>
 * is the behaviour we want though - once it gets a wakeup it should be given
 * a nice run.
 */
static void ioc_set_batching(struct request_queue *q, struct io_context *ioc)
{
	if (!ioc || ioc_batching(q, ioc))
    2450:	2f 1b 00 00 	cmpwi   cr6,r27,0
    2454:	41 9a 00 40 	beq-    cr6,2494 <get_request.isra.52+0x1c4>
	/*
	 * Make sure the process is able to allocate at least 1 request
	 * even if the batch times out, otherwise we could theoretically
	 * lose wakeups.
	 */
	return ioc->nr_batch_requests == q->nr_batching ||
    2458:	80 1b 00 0c 	lwz     r0,12(r27)
    245c:	81 3f 01 b0 	lwz     r9,432(r31)
    2460:	7f 00 48 00 	cmpw    cr6,r0,r9
    2464:	41 9a 00 30 	beq-    cr6,2494 <get_request.isra.52+0x1c4>
    2468:	2f 00 00 00 	cmpwi   cr6,r0,0
		(ioc->nr_batch_requests > 0
		&& time_before(jiffies, ioc->last_waited + BLK_BATCH_TIME));
    246c:	3d 60 00 00 	lis     r11,0
	/*
	 * Make sure the process is able to allocate at least 1 request
	 * even if the batch times out, otherwise we could theoretically
	 * lose wakeups.
	 */
	return ioc->nr_batch_requests == q->nr_batching ||
    2470:	40 99 00 18 	ble-    cr6,2488 <get_request.isra.52+0x1b8>
		(ioc->nr_batch_requests > 0
		&& time_before(jiffies, ioc->last_waited + BLK_BATCH_TIME));
    2474:	81 5b 00 10 	lwz     r10,16(r27)
    2478:	80 0b 00 00 	lwz     r0,0(r11)
    247c:	21 4a ff fb 	subfic  r10,r10,-5
    2480:	7d 0a 02 15 	add.    r8,r10,r0
    2484:	41 80 00 10 	blt-    2494 <get_request.isra.52+0x1c4>
{
	if (!ioc || ioc_batching(q, ioc))
		return;

	ioc->nr_batch_requests = q->nr_batching;
	ioc->last_waited = jiffies;
    2488:	80 0b 00 00 	lwz     r0,0(r11)
static void ioc_set_batching(struct request_queue *q, struct io_context *ioc)
{
	if (!ioc || ioc_batching(q, ioc))
		return;

	ioc->nr_batch_requests = q->nr_batching;
    248c:	91 3b 00 0c 	stw     r9,12(r27)
	ioc->last_waited = jiffies;
    2490:	90 1b 00 10 	stw     r0,16(r27)
static inline void __set_bit(int nr, volatile unsigned long *addr)
{
	unsigned long mask = BIT_MASK(nr);
	unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);

	*p  |= mask;
    2494:	80 1f 01 78 	lwz     r0,376(r31)
	return test_bit(QUEUE_FLAG_ASYNCFULL, &q->queue_flags);
}

static inline void blk_set_queue_full(struct request_queue *q, int sync)
{
	if (sync)
    2498:	41 9e 01 04 	beq-    cr7,259c <get_request.isra.52+0x2cc>
    249c:	60 00 00 08 	ori     r0,r0,8
    24a0:	90 1f 01 78 	stw     r0,376(r31)
 * A queue has just entered congestion.  Flag that in the queue's VM-visible
 * state flags and increment the global gounter of congested queues.
 */
static inline void blk_set_queue_congested(struct request_queue *q, int sync)
{
	set_bdi_congested(&q->backing_dev_info, sync);
    24a4:	38 7f 00 a0 	addi    r3,r31,160
    24a8:	7f 44 d3 78 	mr      r4,r26
    24ac:	48 00 00 01 	bl      24ac <get_request.isra.52+0x1dc>
    24b0:	7d 3f e2 14 	add     r9,r31,r28
    24b4:	81 29 00 10 	lwz     r9,16(r9)
    24b8:	4b ff fe 80 	b       2338 <get_request.isra.52+0x68>
	 * OK, if the queue is under the request limit then requests need
	 * not count toward the nr_batch_requests limit. There will always
	 * be some limit enforced by BLK_BATCH_TIME.
	 */
	if (ioc_batching(q, ioc))
		ioc->nr_batch_requests--;
    24bc:	39 29 ff ff 	addi    r9,r9,-1

	trace_block_getrq(q, bio, rw_flags & 1);
out:
	return rq;
}
    24c0:	7f c3 f3 78 	mr      r3,r30
	 * OK, if the queue is under the request limit then requests need
	 * not count toward the nr_batch_requests limit. There will always
	 * be some limit enforced by BLK_BATCH_TIME.
	 */
	if (ioc_batching(q, ioc))
		ioc->nr_batch_requests--;
    24c4:	91 3b 00 0c 	stw     r9,12(r27)

	trace_block_getrq(q, bio, rw_flags & 1);
out:
	return rq;
}
    24c8:	80 01 00 34 	lwz     r0,52(r1)
    24cc:	81 81 00 0c 	lwz     r12,12(r1)
    24d0:	7c 08 03 a6 	mtlr    r0
    24d4:	bb 01 00 10 	lmw     r24,16(r1)
    24d8:	38 21 00 30 	addi    r1,r1,48
    24dc:	7d 80 81 20 	mtcrf   8,r12
    24e0:	4e 80 00 20 	blr

	rq->cmd_flags = flags | REQ_ALLOCED;

	if (priv) {
		if (unlikely(elv_set_request(q, rq, gfp_mask))) {
			mempool_free(rq, q->rq.rq_pool);
    24e4:	80 9f 00 24 	lwz     r4,36(r31)
    24e8:	7f c3 f3 78 	mr      r3,r30
    24ec:	48 00 00 01 	bl      24ec <get_request.isra.52+0x21c>
}

static inline void arch_local_irq_disable(void)
{
#ifdef CONFIG_BOOKE
	asm volatile("wrteei 0" : : : "memory");
    24f0:	7c 00 01 46 	.long 0x7c000146
		 *
		 * Allocating task should really be put onto the front of the
		 * wait queue, but this is pretty rare.
		 */
		spin_lock_irq(q->queue_lock);
		freed_request(q, is_sync, priv);
    24f4:	7f e3 fb 78 	mr      r3,r31
    24f8:	7f 44 d3 78 	mr      r4,r26
    24fc:	7f 25 cb 78 	mr      r5,r25
    2500:	4b ff f3 a1 	bl      18a0 <freed_request>
		 * so that freeing of a request in the other direction will
		 * notice us. another possible fix would be to split the
		 * rq mempool into READ and WRITE
		 */
rq_starved:
		if (unlikely(rl->count[is_sync] == 0))
    2504:	7f ff e2 14 	add     r31,r31,r28
    2508:	3b c0 00 00 	li      r30,0
    250c:	80 1f 00 10 	lwz     r0,16(r31)
    2510:	2f 80 00 00 	cmpwi   cr7,r0,0
    2514:	40 9e fe e8 	bne+    cr7,23fc <get_request.isra.52+0x12c>
			rl->starved[is_sync] = 1;
    2518:	38 00 00 01 	li      r0,1
    251c:	90 1f 00 18 	stw     r0,24(r31)
    2520:	4b ff fe dc 	b       23fc <get_request.isra.52+0x12c>
	blk_rq_init(q, rq);

	rq->cmd_flags = flags | REQ_ALLOCED;

	if (priv) {
		if (unlikely(elv_set_request(q, rq, gfp_mask))) {
    2524:	7f e3 fb 78 	mr      r3,r31
    2528:	7f c4 f3 78 	mr      r4,r30
    252c:	7f 05 c3 78 	mr      r5,r24
    2530:	48 00 00 01 	bl      2530 <get_request.isra.52+0x260>
    2534:	2f 83 00 00 	cmpwi   cr7,r3,0
    2538:	40 be ff ac 	bne-    cr7,24e4 <get_request.isra.52+0x214>
			mempool_free(rq, q->rq.rq_pool);
			return NULL;
		}
		rq->cmd_flags |= REQ_ELVPRIV;
    253c:	80 1e 00 20 	lwz     r0,32(r30)
    2540:	64 00 00 04 	oris    r0,r0,4
    2544:	90 1e 00 20 	stw     r0,32(r30)
    2548:	4b ff fe 7c 	b       23c4 <get_request.isra.52+0xf4>
			 */
			if (!blk_queue_full(q, is_sync)) {
				ioc_set_batching(q, ioc);
				blk_set_queue_full(q, is_sync);
			} else {
				if (may_queue != ELV_MQUEUE_MUST
    254c:	2f 9e 00 02 	cmpwi   cr7,r30,2
    2550:	41 be ff 54 	beq-    cr7,24a4 <get_request.isra.52+0x1d4>
 * ioc_batching returns true if the ioc is a valid batching request and
 * should be given priority access to a request.
 */
static inline int ioc_batching(struct request_queue *q, struct io_context *ioc)
{
	if (!ioc)
    2554:	2f 9b 00 00 	cmpwi   cr7,r27,0
 * Returns !NULL on success, with queue_lock *not held*.
 */
static struct request *get_request(struct request_queue *q, int rw_flags,
				   struct bio *bio, gfp_t gfp_mask)
{
	struct request *rq = NULL;
    2558:	3b c0 00 00 	li      r30,0
 * ioc_batching returns true if the ioc is a valid batching request and
 * should be given priority access to a request.
 */
static inline int ioc_batching(struct request_queue *q, struct io_context *ioc)
{
	if (!ioc)
    255c:	41 be fe a0 	beq-    cr7,23fc <get_request.isra.52+0x12c>
	/*
	 * Make sure the process is able to allocate at least 1 request
	 * even if the batch times out, otherwise we could theoretically
	 * lose wakeups.
	 */
	return ioc->nr_batch_requests == q->nr_batching ||
    2560:	80 1b 00 0c 	lwz     r0,12(r27)
    2564:	81 3f 01 b0 	lwz     r9,432(r31)
    2568:	7f 80 48 00 	cmpw    cr7,r0,r9
    256c:	41 be ff 38 	beq-    cr7,24a4 <get_request.isra.52+0x1d4>
    2570:	2f 80 00 00 	cmpwi   cr7,r0,0
    2574:	40 bd fe 88 	ble-    cr7,23fc <get_request.isra.52+0x12c>
		(ioc->nr_batch_requests > 0
		&& time_before(jiffies, ioc->last_waited + BLK_BATCH_TIME));
    2578:	81 3b 00 10 	lwz     r9,16(r27)
    257c:	3d 60 00 00 	lis     r11,0
    2580:	80 0b 00 00 	lwz     r0,0(r11)
    2584:	21 29 ff fb 	subfic  r9,r9,-5
    2588:	7d 49 02 15 	add.    r10,r9,r0
    258c:	40 80 fe 70 	bge+    23fc <get_request.isra.52+0x12c>
    2590:	4b ff ff 14 	b       24a4 <get_request.isra.52+0x1d4>
 * @nr: bit number to test
 * @addr: Address to start counting from
 */
static inline int test_bit(int nr, const volatile unsigned long *addr)
{
	return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    2594:	54 00 e7 fe 	rlwinm  r0,r0,28,31,31
    2598:	4b ff fe b0 	b       2448 <get_request.isra.52+0x178>
static inline void __set_bit(int nr, volatile unsigned long *addr)
{
	unsigned long mask = BIT_MASK(nr);
	unsigned long *p = ((unsigned long *)addr) + BIT_WORD(nr);

	*p  |= mask;
    259c:	60 00 00 10 	ori     r0,r0,16
    25a0:	90 1f 01 78 	stw     r0,376(r31)
    25a4:	4b ff ff 00 	b       24a4 <get_request.isra.52+0x1d4>

000025a8 <blk_queue_congestion_threshold>:

void blk_queue_congestion_threshold(struct request_queue *q)
{
	int nr;

	nr = q->nr_requests - (q->nr_requests / 8) + 1;
    25a8:	80 03 01 a4 	lwz     r0,420(r3)
    25ac:	54 09 e8 fe 	rlwinm  r9,r0,29,3,31
	if (nr > q->nr_requests)
		nr = q->nr_requests;
    25b0:	7c 0b 03 78 	mr      r11,r0

void blk_queue_congestion_threshold(struct request_queue *q)
{
	int nr;

	nr = q->nr_requests - (q->nr_requests / 8) + 1;
    25b4:	7d 29 00 50 	subf    r9,r9,r0
    25b8:	39 49 00 01 	addi    r10,r9,1
	if (nr > q->nr_requests)
    25bc:	7f 80 50 40 	cmplw   cr7,r0,r10
    25c0:	41 9c 00 08 	blt-    cr7,25c8 <blk_queue_congestion_threshold+0x20>

void blk_queue_congestion_threshold(struct request_queue *q)
{
	int nr;

	nr = q->nr_requests - (q->nr_requests / 8) + 1;
    25c4:	7d 4b 53 78 	mr      r11,r10
	if (nr > q->nr_requests)
		nr = q->nr_requests;
	q->nr_congestion_on = nr;

	nr = q->nr_requests - (q->nr_requests / 8) - (q->nr_requests / 16) - 1;
    25c8:	54 00 e1 3e 	rlwinm  r0,r0,28,4,31
	int nr;

	nr = q->nr_requests - (q->nr_requests / 8) + 1;
	if (nr > q->nr_requests)
		nr = q->nr_requests;
	q->nr_congestion_on = nr;
    25cc:	91 63 01 a8 	stw     r11,424(r3)

	nr = q->nr_requests - (q->nr_requests / 8) - (q->nr_requests / 16) - 1;
    25d0:	7c 00 48 50 	subf    r0,r0,r9
	if (nr < 1)
    25d4:	39 20 00 01 	li      r9,1
    25d8:	34 00 ff ff 	addic.  r0,r0,-1
    25dc:	40 81 00 08 	ble-    25e4 <blk_queue_congestion_threshold+0x3c>
    25e0:	7c 09 03 78 	mr      r9,r0
		nr = 1;
	q->nr_congestion_off = nr;
    25e4:	91 23 01 ac 	stw     r9,428(r3)
}
    25e8:	4e 80 00 20 	blr

000025ec <blk_init_allocated_queue_node>:
EXPORT_SYMBOL(blk_init_allocated_queue);

struct request_queue *
blk_init_allocated_queue_node(struct request_queue *q, request_fn_proc *rfn,
			      spinlock_t *lock, int node_id)
{
    25ec:	94 21 ff e0 	stwu    r1,-32(r1)
    25f0:	7c 08 02 a6 	mflr    r0
    25f4:	bf 61 00 0c 	stmw    r27,12(r1)
	if (!q)
    25f8:	7c 7f 1b 79 	mr.     r31,r3
EXPORT_SYMBOL(blk_init_allocated_queue);

struct request_queue *
blk_init_allocated_queue_node(struct request_queue *q, request_fn_proc *rfn,
			      spinlock_t *lock, int node_id)
{
    25fc:	7c 9d 23 78 	mr      r29,r4
    2600:	90 01 00 24 	stw     r0,36(r1)
    2604:	7c be 2b 78 	mr      r30,r5
	if (!q)
    2608:	41 82 00 dc 	beq-    26e4 <blk_init_allocated_queue_node+0xf8>

static int blk_init_free_list(struct request_queue *q)
{
	struct request_list *rl = &q->rq;

	if (unlikely(rl->rq_pool))
    260c:	80 1f 00 24 	lwz     r0,36(r31)
			      spinlock_t *lock, int node_id)
{
	if (!q)
		return NULL;

	q->node = node_id;
    2610:	90 df 02 48 	stw     r6,584(r31)

static int blk_init_free_list(struct request_queue *q)
{
	struct request_list *rl = &q->rq;

	if (unlikely(rl->rq_pool))
    2614:	2f 80 00 00 	cmpwi   cr7,r0,0
    2618:	40 9e 00 68 	bne-    cr7,2680 <blk_init_allocated_queue_node+0x94>
		return 0;

	rl->count[BLK_RW_SYNC] = rl->count[BLK_RW_ASYNC] = 0;
	rl->starved[BLK_RW_SYNC] = rl->starved[BLK_RW_ASYNC] = 0;
	rl->elvpriv = 0;
	init_waitqueue_head(&rl->wait[BLK_RW_SYNC]);
    261c:	3f 80 00 00 	lis     r28,0
	struct request_list *rl = &q->rq;

	if (unlikely(rl->rq_pool))
		return 0;

	rl->count[BLK_RW_SYNC] = rl->count[BLK_RW_ASYNC] = 0;
    2620:	90 1f 00 10 	stw     r0,16(r31)
	rl->starved[BLK_RW_SYNC] = rl->starved[BLK_RW_ASYNC] = 0;
	rl->elvpriv = 0;
	init_waitqueue_head(&rl->wait[BLK_RW_SYNC]);
    2624:	38 7f 00 30 	addi    r3,r31,48
	struct request_list *rl = &q->rq;

	if (unlikely(rl->rq_pool))
		return 0;

	rl->count[BLK_RW_SYNC] = rl->count[BLK_RW_ASYNC] = 0;
    2628:	90 1f 00 14 	stw     r0,20(r31)
	rl->starved[BLK_RW_SYNC] = rl->starved[BLK_RW_ASYNC] = 0;
	rl->elvpriv = 0;
	init_waitqueue_head(&rl->wait[BLK_RW_SYNC]);
    262c:	3b 9c 00 00 	addi    r28,r28,0

	if (unlikely(rl->rq_pool))
		return 0;

	rl->count[BLK_RW_SYNC] = rl->count[BLK_RW_ASYNC] = 0;
	rl->starved[BLK_RW_SYNC] = rl->starved[BLK_RW_ASYNC] = 0;
    2630:	90 1f 00 18 	stw     r0,24(r31)
	rl->elvpriv = 0;
	init_waitqueue_head(&rl->wait[BLK_RW_SYNC]);
    2634:	3b 7c 00 04 	addi    r27,r28,4

	if (unlikely(rl->rq_pool))
		return 0;

	rl->count[BLK_RW_SYNC] = rl->count[BLK_RW_ASYNC] = 0;
	rl->starved[BLK_RW_SYNC] = rl->starved[BLK_RW_ASYNC] = 0;
    2638:	90 1f 00 1c 	stw     r0,28(r31)
	rl->elvpriv = 0;
	init_waitqueue_head(&rl->wait[BLK_RW_SYNC]);
    263c:	7f 64 db 78 	mr      r4,r27
	if (unlikely(rl->rq_pool))
		return 0;

	rl->count[BLK_RW_SYNC] = rl->count[BLK_RW_ASYNC] = 0;
	rl->starved[BLK_RW_SYNC] = rl->starved[BLK_RW_ASYNC] = 0;
	rl->elvpriv = 0;
    2640:	90 1f 00 20 	stw     r0,32(r31)
	init_waitqueue_head(&rl->wait[BLK_RW_SYNC]);
    2644:	48 00 00 01 	bl      2644 <blk_init_allocated_queue_node+0x58>
	init_waitqueue_head(&rl->wait[BLK_RW_ASYNC]);
    2648:	38 7f 00 28 	addi    r3,r31,40
    264c:	7f 64 db 78 	mr      r4,r27
    2650:	48 00 00 01 	bl      2650 <blk_init_allocated_queue_node+0x64>

	rl->rq_pool = mempool_create_node(BLKDEV_MIN_RQ, mempool_alloc_slab,
    2654:	80 dc 00 04 	lwz     r6,4(r28)
    2658:	3c 80 00 00 	lis     r4,0
    265c:	3c a0 00 00 	lis     r5,0
    2660:	80 ff 02 48 	lwz     r7,584(r31)
    2664:	38 60 00 04 	li      r3,4
    2668:	38 84 00 00 	addi    r4,r4,0
    266c:	38 a5 00 00 	addi    r5,r5,0
    2670:	48 00 00 01 	bl      2670 <blk_init_allocated_queue_node+0x84>
				mempool_free_slab, request_cachep, q->node);

	if (!rl->rq_pool)
    2674:	2f 83 00 00 	cmpwi   cr7,r3,0
	rl->starved[BLK_RW_SYNC] = rl->starved[BLK_RW_ASYNC] = 0;
	rl->elvpriv = 0;
	init_waitqueue_head(&rl->wait[BLK_RW_SYNC]);
	init_waitqueue_head(&rl->wait[BLK_RW_ASYNC]);

	rl->rq_pool = mempool_create_node(BLKDEV_MIN_RQ, mempool_alloc_slab,
    2678:	90 7f 00 24 	stw     r3,36(r31)
				mempool_free_slab, request_cachep, q->node);

	if (!rl->rq_pool)
    267c:	41 9e 00 80 	beq-    cr7,26fc <blk_init_allocated_queue_node+0x110>
	q->node = node_id;
	if (blk_init_free_list(q))
		return NULL;

	q->request_fn		= rfn;
	q->prep_rq_fn		= NULL;
    2680:	38 00 00 00 	li      r0,0
	q->unprep_rq_fn		= NULL;
	q->unplug_fn		= generic_unplug_device;
    2684:	3d 20 00 00 	lis     r9,0

	q->node = node_id;
	if (blk_init_free_list(q))
		return NULL;

	q->request_fn		= rfn;
    2688:	93 bf 00 38 	stw     r29,56(r31)
	q->prep_rq_fn		= NULL;
    268c:	90 1f 00 40 	stw     r0,64(r31)
	q->queue_lock		= lock;

	/*
	 * This also sets hw/phys segments, boundary and size
	 */
	blk_queue_make_request(q, __make_request);
    2690:	3c 80 00 00 	lis     r4,0
    2694:	7f e3 fb 78 	mr      r3,r31
	if (blk_init_free_list(q))
		return NULL;

	q->request_fn		= rfn;
	q->prep_rq_fn		= NULL;
	q->unprep_rq_fn		= NULL;
    2698:	90 1f 00 44 	stw     r0,68(r31)
	q->unplug_fn		= generic_unplug_device;
    269c:	38 09 00 00 	addi    r0,r9,0
	q->queue_lock		= lock;

	/*
	 * This also sets hw/phys segments, boundary and size
	 */
	blk_queue_make_request(q, __make_request);
    26a0:	38 84 2f 38 	addi    r4,r4,12088
		return NULL;

	q->request_fn		= rfn;
	q->prep_rq_fn		= NULL;
	q->unprep_rq_fn		= NULL;
	q->unplug_fn		= generic_unplug_device;
    26a4:	90 1f 00 48 	stw     r0,72(r31)
	q->queue_flags		= QUEUE_FLAG_DEFAULT;
    26a8:	3c 00 00 04 	lis     r0,4
    26ac:	60 00 a8 00 	ori     r0,r0,43008
	q->queue_lock		= lock;
    26b0:	93 df 01 7c 	stw     r30,380(r31)

	q->request_fn		= rfn;
	q->prep_rq_fn		= NULL;
	q->unprep_rq_fn		= NULL;
	q->unplug_fn		= generic_unplug_device;
	q->queue_flags		= QUEUE_FLAG_DEFAULT;
    26b4:	90 1f 01 78 	stw     r0,376(r31)
	q->queue_lock		= lock;

	/*
	 * This also sets hw/phys segments, boundary and size
	 */
	blk_queue_make_request(q, __make_request);
    26b8:	48 00 00 01 	bl      26b8 <blk_init_allocated_queue_node+0xcc>

	q->sg_reserved_size = INT_MAX;
    26bc:	3c 00 7f ff 	lis     r0,32767

	/*
	 * all done
	 */
	if (!elevator_init(q, NULL)) {
    26c0:	7f e3 fb 78 	mr      r3,r31
	/*
	 * This also sets hw/phys segments, boundary and size
	 */
	blk_queue_make_request(q, __make_request);

	q->sg_reserved_size = INT_MAX;
    26c4:	60 00 ff ff 	ori     r0,r0,65535

	/*
	 * all done
	 */
	if (!elevator_init(q, NULL)) {
    26c8:	38 80 00 00 	li      r4,0
	/*
	 * This also sets hw/phys segments, boundary and size
	 */
	blk_queue_make_request(q, __make_request);

	q->sg_reserved_size = INT_MAX;
    26cc:	90 1f 02 44 	stw     r0,580(r31)

	/*
	 * all done
	 */
	if (!elevator_init(q, NULL)) {
    26d0:	48 00 00 01 	bl      26d0 <blk_init_allocated_queue_node+0xe4>
    26d4:	2f 83 00 00 	cmpwi   cr7,r3,0
    26d8:	40 9e 00 24 	bne-    cr7,26fc <blk_init_allocated_queue_node+0x110>
		blk_queue_congestion_threshold(q);
    26dc:	7f e3 fb 78 	mr      r3,r31
    26e0:	48 00 00 01 	bl      26e0 <blk_init_allocated_queue_node+0xf4>
		return q;
	}

	return NULL;
}
    26e4:	80 01 00 24 	lwz     r0,36(r1)
    26e8:	7f e3 fb 78 	mr      r3,r31
    26ec:	bb 61 00 0c 	lmw     r27,12(r1)
    26f0:	38 21 00 20 	addi    r1,r1,32
    26f4:	7c 08 03 a6 	mtlr    r0
    26f8:	4e 80 00 20 	blr
    26fc:	80 01 00 24 	lwz     r0,36(r1)
	if (!elevator_init(q, NULL)) {
		blk_queue_congestion_threshold(q);
		return q;
	}

	return NULL;
    2700:	3b e0 00 00 	li      r31,0
}
    2704:	7f e3 fb 78 	mr      r3,r31
    2708:	bb 61 00 0c 	lmw     r27,12(r1)
    270c:	38 21 00 20 	addi    r1,r1,32
    2710:	7c 08 03 a6 	mtlr    r0
    2714:	4e 80 00 20 	blr

00002718 <blk_init_allocated_queue>:

struct request_queue *
blk_init_allocated_queue(struct request_queue *q, request_fn_proc *rfn,
			 spinlock_t *lock)
{
	return blk_init_allocated_queue_node(q, rfn, lock, -1);
    2718:	38 c0 ff ff 	li      r6,-1
    271c:	48 00 00 00 	b       271c <blk_init_allocated_queue+0x4>

00002720 <blk_init_queue_node>:
}
EXPORT_SYMBOL(blk_init_queue);

struct request_queue *
blk_init_queue_node(request_fn_proc *rfn, spinlock_t *lock, int node_id)
{
    2720:	94 21 ff e0 	stwu    r1,-32(r1)
    2724:	7c 08 02 a6 	mflr    r0
    2728:	bf 61 00 0c 	stmw    r27,12(r1)
    272c:	7c 7c 1b 78 	mr      r28,r3
    2730:	7c 9d 23 78 	mr      r29,r4
	struct request_queue *uninit_q, *q;

	uninit_q = blk_alloc_queue_node(GFP_KERNEL, node_id);
    2734:	38 60 00 d0 	li      r3,208
    2738:	7c a4 2b 78 	mr      r4,r5
}
EXPORT_SYMBOL(blk_init_queue);

struct request_queue *
blk_init_queue_node(request_fn_proc *rfn, spinlock_t *lock, int node_id)
{
    273c:	90 01 00 24 	stw     r0,36(r1)
    2740:	7c be 2b 78 	mr      r30,r5
	struct request_queue *uninit_q, *q;

	uninit_q = blk_alloc_queue_node(GFP_KERNEL, node_id);
    2744:	48 00 00 01 	bl      2744 <blk_init_queue_node+0x24>
	if (!uninit_q)
		return NULL;
    2748:	3b 60 00 00 	li      r27,0
blk_init_queue_node(request_fn_proc *rfn, spinlock_t *lock, int node_id)
{
	struct request_queue *uninit_q, *q;

	uninit_q = blk_alloc_queue_node(GFP_KERNEL, node_id);
	if (!uninit_q)
    274c:	7c 7f 1b 79 	mr.     r31,r3
    2750:	41 82 00 1c 	beq-    276c <blk_init_queue_node+0x4c>
		return NULL;

	q = blk_init_allocated_queue_node(uninit_q, rfn, lock, node_id);
    2754:	7f 84 e3 78 	mr      r4,r28
    2758:	7f a5 eb 78 	mr      r5,r29
    275c:	7f c6 f3 78 	mr      r6,r30
    2760:	48 00 00 01 	bl      2760 <blk_init_queue_node+0x40>
	if (!q)
    2764:	7c 7b 1b 79 	mr.     r27,r3
    2768:	41 82 00 1c 	beq-    2784 <blk_init_queue_node+0x64>
		blk_cleanup_queue(uninit_q);

	return q;
}
    276c:	80 01 00 24 	lwz     r0,36(r1)
    2770:	7f 63 db 78 	mr      r3,r27
    2774:	bb 61 00 0c 	lmw     r27,12(r1)
    2778:	38 21 00 20 	addi    r1,r1,32
    277c:	7c 08 03 a6 	mtlr    r0
    2780:	4e 80 00 20 	blr
	if (!uninit_q)
		return NULL;

	q = blk_init_allocated_queue_node(uninit_q, rfn, lock, node_id);
	if (!q)
		blk_cleanup_queue(uninit_q);
    2784:	7f e3 fb 78 	mr      r3,r31
    2788:	48 00 00 01 	bl      2788 <blk_init_queue_node+0x68>
    278c:	4b ff ff e0 	b       276c <blk_init_queue_node+0x4c>

00002790 <blk_init_queue>:
 *    when the block device is deactivated (such as at module unload).
 **/

struct request_queue *blk_init_queue(request_fn_proc *rfn, spinlock_t *lock)
{
	return blk_init_queue_node(rfn, lock, -1);
    2790:	38 a0 ff ff 	li      r5,-1
    2794:	48 00 00 00 	b       2794 <blk_init_queue+0x4>

00002798 <__generic_unplug_device>:

/*
 * remove the plug and let it rip..
 */
void __generic_unplug_device(struct request_queue *q)
{
    2798:	94 21 ff f0 	stwu    r1,-16(r1)
    279c:	7c 08 02 a6 	mflr    r0
    27a0:	93 e1 00 0c 	stw     r31,12(r1)
    27a4:	7c 7f 1b 78 	mr      r31,r3
    27a8:	90 01 00 14 	stw     r0,20(r1)
 * @nr: bit number to test
 * @addr: Address to start counting from
 */
static inline int test_bit(int nr, const volatile unsigned long *addr)
{
	return 1UL & (addr[BIT_WORD(nr)] >> (nr & (BITS_PER_LONG-1)));
    27ac:	80 03 01 78 	lwz     r0,376(r3)
	if (unlikely(blk_queue_stopped(q)))
    27b0:	70 09 00 04 	andi.   r9,r0,4
    27b4:	40 82 00 2c 	bne-    27e0 <__generic_unplug_device+0x48>
		return;
	if (!blk_remove_plug(q) && !blk_queue_nonrot(q))
    27b8:	48 00 00 01 	bl      27b8 <__generic_unplug_device+0x20>
    27bc:	2f 83 00 00 	cmpwi   cr7,r3,0
    27c0:	40 9e 00 10 	bne-    cr7,27d0 <__generic_unplug_device+0x38>
    27c4:	80 1f 01 78 	lwz     r0,376(r31)
    27c8:	70 09 40 00 	andi.   r9,r0,16384
    27cc:	41 82 00 14 	beq-    27e0 <__generic_unplug_device+0x48>
		return;

	q->request_fn(q);
    27d0:	80 1f 00 38 	lwz     r0,56(r31)
    27d4:	7f e3 fb 78 	mr      r3,r31
    27d8:	7c 09 03 a6 	mtctr   r0
    27dc:	4e 80 04 21 	bctrl
}
    27e0:	80 01 00 14 	lwz     r0,20(r1)
    27e4:	83 e1 00 0c 	lwz     r31,12(r1)
    27e8:	38 21 00 10 	addi    r1,r1,16
    27ec:	7c 08 03 a6 	mtlr    r0
    27f0:	4e 80 00 20 	blr

000027f4 <get_request_wait.isra.53>:
 * No available requests for this queue, unplug the device and wait for some
 * requests to become available.
 *
 * Called with q->queue_lock held, and returns with it unlocked.
 */
static struct request *get_request_wait(struct request_queue *q, int rw_flags,
    27f4:	94 21 ff b0 	stwu    r1,-80(r1)
    27f8:	7c 08 02 a6 	mflr    r0
					struct bio *bio)
{
	const bool is_sync = rw_is_sync(rw_flags) != 0;
	struct request *rq;

	rq = get_request(q, rw_flags, bio, GFP_NOIO);
    27fc:	38 a0 00 10 	li      r5,16
 * No available requests for this queue, unplug the device and wait for some
 * requests to become available.
 *
 * Called with q->queue_lock held, and returns with it unlocked.
 */
static struct request *get_request_wait(struct request_queue *q, int rw_flags,
    2800:	bf 21 00 34 	stmw    r25,52(r1)
    2804:	7c 7f 1b 78 	mr      r31,r3
    2808:	7c 9b 23 78 	mr      r27,r4
    280c:	90 01 00 54 	stw     r0,84(r1)
					struct bio *bio)
{
	const bool is_sync = rw_is_sync(rw_flags) != 0;
	struct request *rq;

	rq = get_request(q, rw_flags, bio, GFP_NOIO);
    2810:	4b ff fa c1 	bl      22d0 <get_request.isra.52>
	while (!rq) {
    2814:	2c 03 00 00 	cmpwi   r3,0
    2818:	40 82 00 e8 	bne-    2900 <get_request_wait.isra.53+0x10c>
/*
 * We regard a request as sync, if either a read or a sync write
 */
static inline bool rw_is_sync(unsigned int rw_flags)
{
	return !(rw_flags & REQ_WRITE) || (rw_flags & REQ_SYNC);
    281c:	73 60 00 11 	andi.   r0,r27,17
    2820:	3f 20 00 00 	lis     r25,0
    2824:	3b 39 00 00 	addi    r25,r25,0
    2828:	3b a1 00 14 	addi    r29,r1,20
		DEFINE_WAIT(wait);
    282c:	3b 40 00 00 	li      r26,0
	 * even if the batch times out, otherwise we could theoretically
	 * lose wakeups.
	 */
	return ioc->nr_batch_requests == q->nr_batching ||
		(ioc->nr_batch_requests > 0
		&& time_before(jiffies, ioc->last_waited + BLK_BATCH_TIME));
    2830:	3f 80 00 00 	lis     r28,0
{
	const bool is_sync = rw_is_sync(rw_flags) != 0;
	struct request *rq;

	rq = get_request(q, rw_flags, bio, GFP_NOIO);
	while (!rq) {
    2834:	68 00 00 01 	xori    r0,r0,1
    2838:	7c 00 00 34 	cntlzw  r0,r0
    283c:	54 00 d9 7e 	rlwinm  r0,r0,27,5,31
    2840:	68 1e 00 01 	xori    r30,r0,1
    2844:	3b de 00 02 	addi    r30,r30,2
		DEFINE_WAIT(wait);
		struct io_context *ioc;
		struct request_list *rl = &q->rq;

		prepare_to_wait_exclusive(&rl->wait[is_sync], &wait,
    2848:	57 de 18 38 	rlwinm  r30,r30,3,0,28
    284c:	7f df f2 14 	add     r30,r31,r30
    2850:	3b de 00 18 	addi    r30,r30,24
    2854:	38 81 00 08 	addi    r4,r1,8
    2858:	38 a0 00 02 	li      r5,2
	const bool is_sync = rw_is_sync(rw_flags) != 0;
	struct request *rq;

	rq = get_request(q, rw_flags, bio, GFP_NOIO);
	while (!rq) {
		DEFINE_WAIT(wait);
    285c:	93 41 00 08 	stw     r26,8(r1)
		struct io_context *ioc;
		struct request_list *rl = &q->rq;

		prepare_to_wait_exclusive(&rl->wait[is_sync], &wait,
    2860:	7f c3 f3 78 	mr      r3,r30
	const bool is_sync = rw_is_sync(rw_flags) != 0;
	struct request *rq;

	rq = get_request(q, rw_flags, bio, GFP_NOIO);
	while (!rq) {
		DEFINE_WAIT(wait);
    2864:	90 41 00 0c 	stw     r2,12(r1)
    2868:	93 21 00 10 	stw     r25,16(r1)
    286c:	93 a1 00 14 	stw     r29,20(r1)
    2870:	93 a1 00 18 	stw     r29,24(r1)
		struct io_context *ioc;
		struct request_list *rl = &q->rq;

		prepare_to_wait_exclusive(&rl->wait[is_sync], &wait,
    2874:	48 00 00 01 	bl      2874 <get_request_wait.isra.53+0x80>
				TASK_UNINTERRUPTIBLE);

		trace_block_sleeprq(q, bio, rw_flags & 1);

		__generic_unplug_device(q);
    2878:	7f e3 fb 78 	mr      r3,r31
    287c:	48 00 00 01 	bl      287c <get_request_wait.isra.53+0x88>
}

static inline void arch_local_irq_enable(void)
{
#ifdef CONFIG_BOOKE
	asm volatile("wrteei 1" : : : "memory");
    2880:	7c 00 81 46 	.long 0x7c008146
		spin_unlock_irq(q->queue_lock);
		io_schedule();
    2884:	48 00 00 01 	bl      2884 <get_request_wait.isra.53+0x90>
		 * After sleeping, we become a "batching" process and
		 * will be able to allocate at least one request, and
		 * up to a big batch of them for a small period time.
		 * See ioc_batching, ioc_set_batching
		 */
		ioc = current_io_context(GFP_NOIO, q->node);
    2888:	80 9f 02 48 	lwz     r4,584(r31)
    288c:	38 60 00 10 	li      r3,16
    2890:	48 00 00 01 	bl      2890 <get_request_wait.isra.53+0x9c>
	 * even if the batch times out, otherwise we could theoretically
	 * lose wakeups.
	 */
	return ioc->nr_batch_requests == q->nr_batching ||
		(ioc->nr_batch_requests > 0
		&& time_before(jiffies, ioc->last_waited + BLK_BATCH_TIME));
    2894:	39 7c 00 00 	addi    r11,r28,0
 * is the behaviour we want though - once it gets a wakeup it should be given
 * a nice run.
 */
static void ioc_set_batching(struct request_queue *q, struct io_context *ioc)
{
	if (!ioc || ioc_batching(q, ioc))
    2898:	2c 03 00 00 	cmpwi   r3,0
    289c:	41 82 00 3c 	beq-    28d8 <get_request_wait.isra.53+0xe4>
	/*
	 * Make sure the process is able to allocate at least 1 request
	 * even if the batch times out, otherwise we could theoretically
	 * lose wakeups.
	 */
	return ioc->nr_batch_requests == q->nr_batching ||
    28a0:	80 03 00 0c 	lwz     r0,12(r3)
    28a4:	81 3f 01 b0 	lwz     r9,432(r31)
    28a8:	2f 00 00 00 	cmpwi   cr6,r0,0
    28ac:	7f 80 48 00 	cmpw    cr7,r0,r9
    28b0:	41 9e 00 28 	beq-    cr7,28d8 <get_request_wait.isra.53+0xe4>
    28b4:	40 99 00 18 	ble-    cr6,28cc <get_request_wait.isra.53+0xd8>
		(ioc->nr_batch_requests > 0
		&& time_before(jiffies, ioc->last_waited + BLK_BATCH_TIME));
    28b8:	81 43 00 10 	lwz     r10,16(r3)
    28bc:	80 1c 00 00 	lwz     r0,0(r28)
    28c0:	21 4a ff fb 	subfic  r10,r10,-5
    28c4:	7d 0a 02 15 	add.    r8,r10,r0
    28c8:	41 80 00 10 	blt-    28d8 <get_request_wait.isra.53+0xe4>
{
	if (!ioc || ioc_batching(q, ioc))
		return;

	ioc->nr_batch_requests = q->nr_batching;
	ioc->last_waited = jiffies;
    28cc:	80 0b 00 00 	lwz     r0,0(r11)
static void ioc_set_batching(struct request_queue *q, struct io_context *ioc)
{
	if (!ioc || ioc_batching(q, ioc))
		return;

	ioc->nr_batch_requests = q->nr_batching;
    28d0:	91 23 00 0c 	stw     r9,12(r3)
	ioc->last_waited = jiffies;
    28d4:	90 03 00 10 	stw     r0,16(r3)
}

static inline void arch_local_irq_disable(void)
{
#ifdef CONFIG_BOOKE
	asm volatile("wrteei 0" : : : "memory");
    28d8:	7c 00 01 46 	.long 0x7c000146
		 */
		ioc = current_io_context(GFP_NOIO, q->node);
		ioc_set_batching(q, ioc);

		spin_lock_irq(q->queue_lock);
		finish_wait(&rl->wait[is_sync], &wait);
    28dc:	7f c3 f3 78 	mr      r3,r30
    28e0:	38 81 00 08 	addi    r4,r1,8
    28e4:	48 00 00 01 	bl      28e4 <get_request_wait.isra.53+0xf0>

		rq = get_request(q, rw_flags, bio, GFP_NOIO);
    28e8:	7f e3 fb 78 	mr      r3,r31
    28ec:	7f 64 db 78 	mr      r4,r27
    28f0:	38 a0 00 10 	li      r5,16
    28f4:	4b ff f9 dd 	bl      22d0 <get_request.isra.52>
{
	const bool is_sync = rw_is_sync(rw_flags) != 0;
	struct request *rq;

	rq = get_request(q, rw_flags, bio, GFP_NOIO);
	while (!rq) {
    28f8:	2c 03 00 00 	cmpwi   r3,0
    28fc:	41 82 ff 58 	beq+    2854 <get_request_wait.isra.53+0x60>

		rq = get_request(q, rw_flags, bio, GFP_NOIO);
	};

	return rq;
}
    2900:	80 01 00 54 	lwz     r0,84(r1)
    2904:	bb 21 00 34 	lmw     r25,52(r1)
    2908:	38 21 00 50 	addi    r1,r1,80
    290c:	7c 08 03 a6 	mtlr    r0
    2910:	4e 80 00 20 	blr

00002914 <blk_get_request>:

struct request *blk_get_request(struct request_queue *q, int rw, gfp_t gfp_mask)
{
    2914:	94 21 ff f0 	stwu    r1,-16(r1)
    2918:	7c 08 02 a6 	mflr    r0
    291c:	90 01 00 14 	stw     r0,20(r1)
	struct request *rq;

	BUG_ON(rw != READ && rw != WRITE);
    2920:	20 04 00 01 	subfic  r0,r4,1
    2924:	7c 00 01 10 	subfe   r0,r0,r0
    2928:	7c 00 00 d0 	neg     r0,r0
    292c:	0f 00 00 00 	twnei   r0,0
    2930:	7c 00 01 46 	.long 0x7c000146

	spin_lock_irq(q->queue_lock);
	if (gfp_mask & __GFP_WAIT) {
    2934:	70 a0 00 10 	andi.   r0,r5,16
    2938:	40 82 00 20 	bne-    2958 <blk_get_request+0x44>
		rq = get_request_wait(q, rw, NULL);
	} else {
		rq = get_request(q, rw, NULL, gfp_mask);
    293c:	4b ff f9 95 	bl      22d0 <get_request.isra.52>
		if (!rq)
    2940:	2c 03 00 00 	cmpwi   r3,0
    2944:	41 82 00 24 	beq-    2968 <blk_get_request+0x54>
			spin_unlock_irq(q->queue_lock);
	}
	/* q->queue_lock is unlocked at this point */

	return rq;
}
    2948:	80 01 00 14 	lwz     r0,20(r1)
    294c:	38 21 00 10 	addi    r1,r1,16
    2950:	7c 08 03 a6 	mtlr    r0
    2954:	4e 80 00 20 	blr
    2958:	80 01 00 14 	lwz     r0,20(r1)
    295c:	38 21 00 10 	addi    r1,r1,16
    2960:	7c 08 03 a6 	mtlr    r0

	BUG_ON(rw != READ && rw != WRITE);

	spin_lock_irq(q->queue_lock);
	if (gfp_mask & __GFP_WAIT) {
		rq = get_request_wait(q, rw, NULL);
    2964:	4b ff fe 90 	b       27f4 <get_request_wait.isra.53>
}

static inline void arch_local_irq_enable(void)
{
#ifdef CONFIG_BOOKE
	asm volatile("wrteei 1" : : : "memory");
    2968:	7c 00 81 46 	.long 0x7c008146
			spin_unlock_irq(q->queue_lock);
	}
	/* q->queue_lock is unlocked at this point */

	return rq;
}
    296c:	80 01 00 14 	lwz     r0,20(r1)
    2970:	38 21 00 10 	addi    r1,r1,16
    2974:	7c 08 03 a6 	mtlr    r0
    2978:	4e 80 00 20 	blr

0000297c <blk_make_request>:
 * If possible a big IO should be split into smaller parts when allocation
 * fails. Partial allocation should not be an error, or you risk a live-lock.
 */
struct request *blk_make_request(struct request_queue *q, struct bio *bio,
				 gfp_t gfp_mask)
{
    297c:	94 21 ff d0 	stwu    r1,-48(r1)
    2980:	7c 08 02 a6 	mflr    r0
    2984:	bf 81 00 20 	stmw    r28,32(r1)
    2988:	7c 9f 23 78 	mr      r31,r4
    298c:	7c 7e 1b 78 	mr      r30,r3
    2990:	90 01 00 34 	stw     r0,52(r1)
	struct request *rq = blk_get_request(q, bio_data_dir(bio), gfp_mask);
    2994:	80 84 00 14 	lwz     r4,20(r4)
    2998:	54 84 07 fe 	clrlwi  r4,r4,31
    299c:	48 00 00 01 	bl      299c <blk_make_request+0x20>

	if (unlikely(!rq))
    29a0:	7c 7d 1b 79 	mr.     r29,r3
    29a4:	41 82 00 68 	beq-    2a0c <blk_make_request+0x90>
		return ERR_PTR(-ENOMEM);

	for_each_bio(bio) {
    29a8:	2f 9f 00 00 	cmpwi   cr7,r31,0
    29ac:	41 9e 00 38 	beq-    cr7,29e4 <blk_make_request+0x68>
		struct bio *bounce_bio = bio;
		int ret;

		blk_queue_bounce(q, &bounce_bio);
    29b0:	7f c3 f3 78 	mr      r3,r30
    29b4:	38 81 00 08 	addi    r4,r1,8

	if (unlikely(!rq))
		return ERR_PTR(-ENOMEM);

	for_each_bio(bio) {
		struct bio *bounce_bio = bio;
    29b8:	93 e1 00 08 	stw     r31,8(r1)
		int ret;

		blk_queue_bounce(q, &bounce_bio);
    29bc:	48 00 00 01 	bl      29bc <blk_make_request+0x40>
		ret = blk_rq_append_bio(q, rq, bounce_bio);
    29c0:	80 a1 00 08 	lwz     r5,8(r1)
    29c4:	7f c3 f3 78 	mr      r3,r30
    29c8:	7f a4 eb 78 	mr      r4,r29
    29cc:	48 00 00 01 	bl      29cc <blk_make_request+0x50>
		if (unlikely(ret)) {
    29d0:	7c 7c 1b 79 	mr.     r28,r3
    29d4:	40 82 00 28 	bne-    29fc <blk_make_request+0x80>
	struct request *rq = blk_get_request(q, bio_data_dir(bio), gfp_mask);

	if (unlikely(!rq))
		return ERR_PTR(-ENOMEM);

	for_each_bio(bio) {
    29d8:	83 ff 00 08 	lwz     r31,8(r31)
    29dc:	2f 9f 00 00 	cmpwi   cr7,r31,0
    29e0:	40 9e ff d0 	bne+    cr7,29b0 <blk_make_request+0x34>
			return ERR_PTR(ret);
		}
	}

	return rq;
}
    29e4:	80 01 00 34 	lwz     r0,52(r1)
    29e8:	7f a3 eb 78 	mr      r3,r29
    29ec:	bb 81 00 20 	lmw     r28,32(r1)
    29f0:	38 21 00 30 	addi    r1,r1,48
    29f4:	7c 08 03 a6 	mtlr    r0
    29f8:	4e 80 00 20 	blr
		int ret;

		blk_queue_bounce(q, &bounce_bio);
		ret = blk_rq_append_bio(q, rq, bounce_bio);
		if (unlikely(ret)) {
			blk_put_request(rq);
    29fc:	7f a3 eb 78 	mr      r3,r29

#define IS_ERR_VALUE(x) unlikely((x) >= (unsigned long)-MAX_ERRNO)

static inline void * __must_check ERR_PTR(long error)
{
	return (void *) error;
    2a00:	7f 9d e3 78 	mr      r29,r28
    2a04:	48 00 00 01 	bl      2a04 <blk_make_request+0x88>
			return ERR_PTR(ret);
    2a08:	4b ff ff dc 	b       29e4 <blk_make_request+0x68>
				 gfp_t gfp_mask)
{
	struct request *rq = blk_get_request(q, bio_data_dir(bio), gfp_mask);

	if (unlikely(!rq))
		return ERR_PTR(-ENOMEM);
    2a0c:	3b a0 ff f4 	li      r29,-12
    2a10:	4b ff ff d4 	b       29e4 <blk_make_request+0x68>

00002a14 <generic_unplug_device>:
 *   is still adding and merging requests on the queue. Once the queue
 *   gets unplugged, the request_fn defined for the queue is invoked and
 *   transfers started.
 **/
void generic_unplug_device(struct request_queue *q)
{
    2a14:	7c 08 02 a6 	mflr    r0
    2a18:	94 21 ff f0 	stwu    r1,-16(r1)
    2a1c:	90 01 00 14 	stw     r0,20(r1)
    2a20:	80 03 01 78 	lwz     r0,376(r3)
	if (blk_queue_plugged(q)) {
    2a24:	70 09 00 80 	andi.   r9,r0,128
    2a28:	41 a2 00 10 	beq+    2a38 <generic_unplug_device+0x24>
}

static inline void arch_local_irq_disable(void)
{
#ifdef CONFIG_BOOKE
	asm volatile("wrteei 0" : : : "memory");
    2a2c:	7c 00 01 46 	.long 0x7c000146
		spin_lock_irq(q->queue_lock);
		__generic_unplug_device(q);
    2a30:	48 00 00 01 	bl      2a30 <generic_unplug_device+0x1c>
}

static inline void arch_local_irq_enable(void)
{
#ifdef CONFIG_BOOKE
	asm volatile("wrteei 1" : : : "memory");
    2a34:	7c 00 81 46 	.long 0x7c008146
		spin_unlock_irq(q->queue_lock);
	}
}
    2a38:	80 01 00 14 	lwz     r0,20(r1)
    2a3c:	38 21 00 10 	addi    r1,r1,16
    2a40:	7c 08 03 a6 	mtlr    r0
    2a44:	4e 80 00 20 	blr

00002a48 <blk_unplug_timeout>:
void blk_unplug_timeout(unsigned long data)
{
	struct request_queue *q = (struct request_queue *)data;

	trace_block_unplug_timer(q);
	kblockd_schedule_work(q, &q->unplug_work);
    2a48:	38 83 00 90 	addi    r4,r3,144
    2a4c:	48 00 00 00 	b       2a4c <blk_unplug_timeout+0x4>

00002a50 <blk_put_queue>:
	spin_unlock_irqrestore(q->queue_lock, flags);
}
EXPORT_SYMBOL(blk_run_queue);

void blk_put_queue(struct request_queue *q)
{
    2a50:	94 21 ff f0 	stwu    r1,-16(r1)
    2a54:	7c 08 02 a6 	mflr    r0
	kobject_put(&q->kobj);
    2a58:	38 63 01 80 	addi    r3,r3,384
	spin_unlock_irqrestore(q->queue_lock, flags);
}
EXPORT_SYMBOL(blk_run_queue);

void blk_put_queue(struct request_queue *q)
{
    2a5c:	90 01 00 14 	stw     r0,20(r1)
	kobject_put(&q->kobj);
    2a60:	48 00 00 01 	bl      2a60 <blk_put_queue+0x10>
}
    2a64:	80 01 00 14 	lwz     r0,20(r1)
    2a68:	38 21 00 10 	addi    r1,r1,16
    2a6c:	7c 08 03 a6 	mtlr    r0
    2a70:	4e 80 00 20 	blr

00002a74 <blk_get_queue>:
	return NULL;
}
EXPORT_SYMBOL(blk_init_allocated_queue_node);

int blk_get_queue(struct request_queue *q)
{
    2a74:	94 21 ff f0 	stwu    r1,-16(r1)
    2a78:	7c 08 02 a6 	mflr    r0
    2a7c:	90 01 00 14 	stw     r0,20(r1)
	if (likely(!test_bit(QUEUE_FLAG_DEAD, &q->queue_flags))) {
		kobject_get(&q->kobj);
		return 0;
	}

	return 1;
    2a80:	38 00 00 01 	li      r0,1
    2a84:	81 23 01 78 	lwz     r9,376(r3)
}
EXPORT_SYMBOL(blk_init_allocated_queue_node);

int blk_get_queue(struct request_queue *q)
{
	if (likely(!test_bit(QUEUE_FLAG_DEAD, &q->queue_flags))) {
    2a88:	71 2b 00 20 	andi.   r11,r9,32
    2a8c:	40 82 00 10 	bne-    2a9c <blk_get_queue+0x28>
		kobject_get(&q->kobj);
    2a90:	38 63 01 80 	addi    r3,r3,384
    2a94:	48 00 00 01 	bl      2a94 <blk_get_queue+0x20>
		return 0;
    2a98:	38 00 00 00 	li      r0,0
	}

	return 1;
}
    2a9c:	7c 03 03 78 	mr      r3,r0
    2aa0:	80 01 00 14 	lwz     r0,20(r1)
    2aa4:	38 21 00 10 	addi    r1,r1,16
    2aa8:	7c 08 03 a6 	mtlr    r0
    2aac:	4e 80 00 20 	blr

00002ab0 <blk_dequeue_request>:
 * list_empty - tests whether a list is empty
 * @head: the list to test.
 */
static inline int list_empty(const struct list_head *head)
{
	return head->next == head;
    2ab0:	81 23 00 00 	lwz     r9,0(r3)
}
EXPORT_SYMBOL(blk_peek_request);

void blk_dequeue_request(struct request *rq)
{
	struct request_queue *q = rq->q;
    2ab4:	81 43 00 1c 	lwz     r10,28(r3)
    2ab8:	7c 60 4a 78 	xor     r0,r3,r9
    2abc:	7c 00 00 34 	cntlzw  r0,r0
    2ac0:	54 00 d9 7e 	rlwinm  r0,r0,27,5,31

	BUG_ON(list_empty(&rq->queuelist));
    2ac4:	0f 00 00 00 	twnei   r0,0
	BUG_ON(ELV_ON_HASH(rq));
    2ac8:	80 03 00 4c 	lwz     r0,76(r3)
    2acc:	7c 00 00 34 	cntlzw  r0,r0
    2ad0:	54 00 d9 7e 	rlwinm  r0,r0,27,5,31
    2ad4:	68 00 00 01 	xori    r0,r0,1
    2ad8:	0f 00 00 00 	twnei   r0,0
 * in an undefined state.
 */
#ifndef CONFIG_DEBUG_LIST
static inline void __list_del_entry(struct list_head *entry)
{
	__list_del(entry->prev, entry->next);
    2adc:	81 63 00 04 	lwz     r11,4(r3)
 * This is only for internal list manipulation where we know
 * the prev/next entries already!
 */
static inline void __list_del(struct list_head * prev, struct list_head * next)
{
	next->prev = prev;
    2ae0:	91 69 00 04 	stw     r11,4(r9)
	prev->next = next;
    2ae4:	91 2b 00 00 	stw     r9,0(r11)
#define LIST_HEAD(name) \
	struct list_head name = LIST_HEAD_INIT(name)

static inline void INIT_LIST_HEAD(struct list_head *list)
{
	list->next = list;
    2ae8:	90 63 00 00 	stw     r3,0(r3)
	list->prev = list;
    2aec:	90 63 00 04 	stw     r3,4(r3)
	/*
	 * the time frame between a request being removed from the lists
	 * and to it is freed is accounted as io that is in progress at
	 * the driver side.
	 */
	if (blk_account_rq(rq)) {
    2af0:	80 03 00 20 	lwz     r0,32(r3)
    2af4:	70 09 80 00 	andi.   r9,r0,32768
    2af8:	4d 82 00 20 	beqlr   
    2afc:	81 23 00 24 	lwz     r9,36(r3)
    2b00:	2f 89 00 01 	cmpwi   cr7,r9,1
    2b04:	41 9e 00 0c 	beq-    cr7,2b10 <blk_dequeue_request+0x60>
    2b08:	70 09 00 40 	andi.   r9,r0,64
    2b0c:	4d 82 00 20 	beqlr   
    2b10:	70 00 00 11 	andi.   r0,r0,17
		q->in_flight[rq_is_sync(rq)]++;
    2b14:	68 00 00 01 	xori    r0,r0,1
    2b18:	7c 00 00 34 	cntlzw  r0,r0
    2b1c:	54 00 d9 7e 	rlwinm  r0,r0,27,5,31
    2b20:	68 09 00 01 	xori    r9,r0,1
    2b24:	38 09 00 74 	addi    r0,r9,116
    2b28:	54 00 10 3a 	rlwinm  r0,r0,2,0,29
    2b2c:	7d 4a 02 14 	add     r10,r10,r0
    2b30:	81 2a 00 04 	lwz     r9,4(r10)
    2b34:	38 09 00 01 	addi    r0,r9,1
    2b38:	90 0a 00 04 	stw     r0,4(r10)
    2b3c:	4e 80 00 20 	blr

00002b40 <blk_start_request>:
 *
 * Context:
 *     queue_lock must be held.
 */
void blk_start_request(struct request *req)
{
    2b40:	94 21 ff f0 	stwu    r1,-16(r1)
    2b44:	7c 08 02 a6 	mflr    r0
    2b48:	93 e1 00 0c 	stw     r31,12(r1)
    2b4c:	7c 7f 1b 78 	mr      r31,r3
    2b50:	90 01 00 14 	stw     r0,20(r1)
	blk_dequeue_request(req);
    2b54:	48 00 00 01 	bl      2b54 <blk_start_request+0x14>
	/*
	 * We are now handing the request to the hardware, initialize
	 * resid_len to full count and add the timeout handler.
	 */
	req->resid_len = blk_rq_bytes(req);
	if (unlikely(blk_bidi_rq(req)))
    2b58:	81 3f 00 d0 	lwz     r9,208(r31)

	/*
	 * We are now handing the request to the hardware, initialize
	 * resid_len to full count and add the timeout handler.
	 */
	req->resid_len = blk_rq_bytes(req);
    2b5c:	80 1f 00 30 	lwz     r0,48(r31)
	if (unlikely(blk_bidi_rq(req)))
    2b60:	2f 89 00 00 	cmpwi   cr7,r9,0

	/*
	 * We are now handing the request to the hardware, initialize
	 * resid_len to full count and add the timeout handler.
	 */
	req->resid_len = blk_rq_bytes(req);
    2b64:	90 1f 00 ac 	stw     r0,172(r31)
	if (unlikely(blk_bidi_rq(req)))
    2b68:	40 9e 00 20 	bne-    cr7,2b88 <blk_start_request+0x48>
		req->next_rq->resid_len = blk_rq_bytes(req->next_rq);

	blk_add_timer(req);
    2b6c:	7f e3 fb 78 	mr      r3,r31
    2b70:	48 00 00 01 	bl      2b70 <blk_start_request+0x30>
}
    2b74:	80 01 00 14 	lwz     r0,20(r1)
    2b78:	83 e1 00 0c 	lwz     r31,12(r1)
    2b7c:	38 21 00 10 	addi    r1,r1,16
    2b80:	7c 08 03 a6 	mtlr    r0
    2b84:	4e 80 00 20 	blr
	 * We are now handing the request to the hardware, initialize
	 * resid_len to full count and add the timeout handler.
	 */
	req->resid_len = blk_rq_bytes(req);
	if (unlikely(blk_bidi_rq(req)))
		req->next_rq->resid_len = blk_rq_bytes(req->next_rq);
    2b88:	80 09 00 30 	lwz     r0,48(r9)
    2b8c:	90 09 00 ac 	stw     r0,172(r9)
    2b90:	4b ff ff dc 	b       2b6c <blk_start_request+0x2c>

00002b94 <blk_peek_request>:
 *
 * Context:
 *     queue_lock must be held.
 */
struct request *blk_peek_request(struct request_queue *q)
{
    2b94:	94 21 ff d0 	stwu    r1,-48(r1)
    2b98:	7c 08 02 a6 	mflr    r0
    2b9c:	bf 21 00 14 	stmw    r25,20(r1)
	struct request *rq;

	while (1) {
		while (!list_empty(&q->queue_head)) {
			rq = list_entry_rq(q->queue_head.next);
			if (!(rq->cmd_flags & (REQ_FLUSH | REQ_FUA)) ||
    2ba0:	3f 60 01 00 	lis     r27,256
    2ba4:	7c 7e 1b 78 	mr      r30,r3
    2ba8:	90 01 00 34 	stw     r0,52(r1)
    2bac:	63 7b 20 00 	ori     r27,r27,8192
			    rq == &q->flush_rq)
    2bb0:	3b 43 02 58 	addi    r26,r3,600
			trace_block_rq_issue(q, rq);
		}

		if (!q->boundary_rq || q->boundary_rq == rq) {
			q->end_sector = rq_end_sector(rq);
			q->boundary_rq = NULL;
    2bb4:	3b 20 00 00 	li      r25,0
struct request *blk_peek_request(struct request_queue *q)
{
	struct request *rq;
	int ret;

	while ((rq = __elv_next_request(q)) != NULL) {
    2bb8:	48 00 00 fc 	b       2cb4 <blk_peek_request+0x120>
	struct request *rq;

	while (1) {
		while (!list_empty(&q->queue_head)) {
			rq = list_entry_rq(q->queue_head.next);
			if (!(rq->cmd_flags & (REQ_FLUSH | REQ_FUA)) ||
    2bbc:	7f 9f d0 00 	cmpw    cr7,r31,r26
    2bc0:	41 9e 01 0c 	beq-    cr7,2ccc <blk_peek_request+0x138>
			    rq == &q->flush_rq)
				return rq;
			rq = blk_do_flush(q, rq);
    2bc4:	7f e4 fb 78 	mr      r4,r31
    2bc8:	7f c3 f3 78 	mr      r3,r30
    2bcc:	48 00 00 01 	bl      2bcc <blk_peek_request+0x38>
			if (rq)
    2bd0:	7c 7f 1b 79 	mr.     r31,r3
    2bd4:	41 82 00 e0 	beq-    2cb4 <blk_peek_request+0x120>
    2bd8:	81 3f 00 20 	lwz     r9,32(r31)
		if (!(rq->cmd_flags & REQ_STARTED)) {
    2bdc:	71 20 80 00 	andi.   r0,r9,32768
    2be0:	40 82 00 3c 	bne-    2c1c <blk_peek_request+0x88>
			/*
			 * This is the first time the device driver
			 * sees this request (possibly after
			 * requeueing).  Notify IO scheduler.
			 */
			if (rq->cmd_flags & REQ_SORTED)
    2be4:	71 20 08 00 	andi.   r0,r9,2048
    2be8:	41 82 00 2c 	beq-    2c14 <blk_peek_request+0x80>

static inline void elv_activate_rq(struct request_queue *q, struct request *rq)
{
	struct elevator_queue *e = q->elevator;

	if (e->ops->elevator_activate_req_fn)
    2bec:	81 7e 00 0c 	lwz     r11,12(r30)
    2bf0:	81 6b 00 00 	lwz     r11,0(r11)
    2bf4:	80 0b 00 1c 	lwz     r0,28(r11)
    2bf8:	2f 80 00 00 	cmpwi   cr7,r0,0
    2bfc:	41 9e 00 18 	beq-    cr7,2c14 <blk_peek_request+0x80>
		e->ops->elevator_activate_req_fn(q, rq);
    2c00:	7f c3 f3 78 	mr      r3,r30
    2c04:	7f e4 fb 78 	mr      r4,r31
    2c08:	7c 09 03 a6 	mtctr   r0
    2c0c:	4e 80 04 21 	bctrl
    2c10:	81 3f 00 20 	lwz     r9,32(r31)
			/*
			 * just mark as started even if we don't start
			 * it, a request that has been delayed should
			 * not be passed by new incoming requests
			 */
			rq->cmd_flags |= REQ_STARTED;
    2c14:	61 29 80 00 	ori     r9,r9,32768
    2c18:	91 3f 00 20 	stw     r9,32(r31)
			trace_block_rq_issue(q, rq);
		}

		if (!q->boundary_rq || q->boundary_rq == rq) {
    2c1c:	80 1e 00 68 	lwz     r0,104(r30)
    2c20:	2f 80 00 00 	cmpwi   cr7,r0,0
    2c24:	41 9e 00 c8 	beq-    cr7,2cec <blk_peek_request+0x158>
    2c28:	7f 80 f8 00 	cmpw    cr7,r0,r31
    2c2c:	41 9e 00 c0 	beq-    cr7,2cec <blk_peek_request+0x158>
			q->end_sector = rq_end_sector(rq);
			q->boundary_rq = NULL;
		}

		if (rq->cmd_flags & REQ_DONTPREP)
    2c30:	80 1f 00 20 	lwz     r0,32(r31)
    2c34:	74 09 00 01 	andis.  r9,r0,1
    2c38:	40 82 00 9c 	bne-    2cd4 <blk_peek_request+0x140>
			break;

		if (q->dma_drain_size && blk_rq_bytes(rq)) {
    2c3c:	80 1e 01 b8 	lwz     r0,440(r30)
    2c40:	2f 80 00 00 	cmpwi   cr7,r0,0
    2c44:	41 9e 00 1c 	beq-    cr7,2c60 <blk_peek_request+0xcc>
    2c48:	80 1f 00 30 	lwz     r0,48(r31)
    2c4c:	2f 80 00 00 	cmpwi   cr7,r0,0
    2c50:	41 9e 00 10 	beq-    cr7,2c60 <blk_peek_request+0xcc>
			 * make sure space for the drain appears we
			 * know we can do this because max_hw_segments
			 * has been adjusted to be one fewer than the
			 * device can handle
			 */
			rq->nr_phys_segments++;
    2c54:	a1 3f 00 74 	lhz     r9,116(r31)
    2c58:	38 09 00 01 	addi    r0,r9,1
    2c5c:	b0 1f 00 74 	sth     r0,116(r31)
		}

		if (!q->prep_rq_fn)
    2c60:	80 1e 00 40 	lwz     r0,64(r30)
    2c64:	2f 80 00 00 	cmpwi   cr7,r0,0
    2c68:	41 9e 00 6c 	beq-    cr7,2cd4 <blk_peek_request+0x140>
			break;

		ret = q->prep_rq_fn(q, rq);
    2c6c:	7f c3 f3 78 	mr      r3,r30
    2c70:	7f e4 fb 78 	mr      r4,r31
    2c74:	7c 09 03 a6 	mtctr   r0
    2c78:	4e 80 04 21 	bctrl
		if (ret == BLKPREP_OK) {
    2c7c:	7c 65 1b 79 	mr.     r5,r3
    2c80:	41 82 00 54 	beq-    2cd4 <blk_peek_request+0x140>
			break;
		} else if (ret == BLKPREP_DEFER) {
    2c84:	2f 85 00 02 	cmpwi   cr7,r5,2
    2c88:	41 9e 00 d0 	beq-    cr7,2d58 <blk_peek_request+0x1c4>
				--rq->nr_phys_segments;
			}

			rq = NULL;
			break;
		} else if (ret == BLKPREP_KILL) {
    2c8c:	2f 85 00 01 	cmpwi   cr7,r5,1
    2c90:	40 9e 01 00 	bne-    cr7,2d90 <blk_peek_request+0x1fc>
			rq->cmd_flags |= REQ_QUIET;
    2c94:	80 1f 00 20 	lwz     r0,32(r31)
			/*
			 * Mark this request as started so we don't trigger
			 * any debug logic in the end I/O path.
			 */
			blk_start_request(rq);
    2c98:	7f e3 fb 78 	mr      r3,r31
			}

			rq = NULL;
			break;
		} else if (ret == BLKPREP_KILL) {
			rq->cmd_flags |= REQ_QUIET;
    2c9c:	64 00 00 10 	oris    r0,r0,16
    2ca0:	90 1f 00 20 	stw     r0,32(r31)
			/*
			 * Mark this request as started so we don't trigger
			 * any debug logic in the end I/O path.
			 */
			blk_start_request(rq);
    2ca4:	48 00 00 01 	bl      2ca4 <blk_peek_request+0x110>
			__blk_end_request_all(rq, -EIO);
    2ca8:	7f e3 fb 78 	mr      r3,r31
    2cac:	38 80 ff fb 	li      r4,-5
    2cb0:	48 00 00 01 	bl      2cb0 <blk_peek_request+0x11c>
 * list_empty - tests whether a list is empty
 * @head: the list to test.
 */
static inline int list_empty(const struct list_head *head)
{
	return head->next == head;
    2cb4:	83 fe 00 00 	lwz     r31,0(r30)
static inline struct request *__elv_next_request(struct request_queue *q)
{
	struct request *rq;

	while (1) {
		while (!list_empty(&q->queue_head)) {
    2cb8:	7f 9e f8 00 	cmpw    cr7,r30,r31
    2cbc:	41 9e 00 5c 	beq-    cr7,2d18 <blk_peek_request+0x184>
			rq = list_entry_rq(q->queue_head.next);
			if (!(rq->cmd_flags & (REQ_FLUSH | REQ_FUA)) ||
    2cc0:	81 3f 00 20 	lwz     r9,32(r31)
    2cc4:	7d 20 d8 39 	and.    r0,r9,r27
    2cc8:	40 a2 fe f4 	bne-    2bbc <blk_peek_request+0x28>
struct request *blk_peek_request(struct request_queue *q)
{
	struct request *rq;
	int ret;

	while ((rq = __elv_next_request(q)) != NULL) {
    2ccc:	2f 9f 00 00 	cmpwi   cr7,r31,0
    2cd0:	40 9e ff 0c 	bne+    cr7,2bdc <blk_peek_request+0x48>
			break;
		}
	}

	return rq;
}
    2cd4:	80 01 00 34 	lwz     r0,52(r1)
    2cd8:	7f e3 fb 78 	mr      r3,r31
    2cdc:	bb 21 00 14 	lmw     r25,20(r1)
    2ce0:	38 21 00 30 	addi    r1,r1,48
    2ce4:	7c 08 03 a6 	mtlr    r0
    2ce8:	4e 80 00 20 	blr

extern unsigned int blk_rq_err_bytes(const struct request *rq);

static inline unsigned int blk_rq_sectors(const struct request *rq)
{
	return blk_rq_bytes(rq) >> 9;
    2cec:	80 1f 00 30 	lwz     r0,48(r31)
			rq->cmd_flags |= REQ_STARTED;
			trace_block_rq_issue(q, rq);
		}

		if (!q->boundary_rq || q->boundary_rq == rq) {
			q->end_sector = rq_end_sector(rq);
    2cf0:	3b 80 00 00 	li      r28,0
    2cf4:	81 5f 00 38 	lwz     r10,56(r31)
    2cf8:	81 7f 00 3c 	lwz     r11,60(r31)
    2cfc:	54 1d ba 7e 	rlwinm  r29,r0,23,9,31
			q->boundary_rq = NULL;
    2d00:	93 3e 00 68 	stw     r25,104(r30)
			rq->cmd_flags |= REQ_STARTED;
			trace_block_rq_issue(q, rq);
		}

		if (!q->boundary_rq || q->boundary_rq == rq) {
			q->end_sector = rq_end_sector(rq);
    2d04:	7d 6b e8 14 	addc    r11,r11,r29
    2d08:	7d 4a e1 14 	adde    r10,r10,r28
    2d0c:	91 5e 00 60 	stw     r10,96(r30)
    2d10:	91 7e 00 64 	stw     r11,100(r30)
    2d14:	4b ff ff 1c 	b       2c30 <blk_peek_request+0x9c>
			rq = blk_do_flush(q, rq);
			if (rq)
				return rq;
		}

		if (!q->elevator->ops->elevator_dispatch_fn(q, 0))
    2d18:	81 3f 00 0c 	lwz     r9,12(r31)
    2d1c:	7f e3 fb 78 	mr      r3,r31
    2d20:	38 80 00 00 	li      r4,0
    2d24:	81 29 00 00 	lwz     r9,0(r9)
    2d28:	80 09 00 14 	lwz     r0,20(r9)
    2d2c:	7c 09 03 a6 	mtctr   r0
    2d30:	4e 80 04 21 	bctrl
    2d34:	2f 83 00 00 	cmpwi   cr7,r3,0
    2d38:	40 be ff 7c 	bne-    cr7,2cb4 <blk_peek_request+0x120>
			break;
		}
	}

	return rq;
}
    2d3c:	80 01 00 34 	lwz     r0,52(r1)
    2d40:	3b e0 00 00 	li      r31,0
    2d44:	7f e3 fb 78 	mr      r3,r31
    2d48:	bb 21 00 14 	lmw     r25,20(r1)
    2d4c:	38 21 00 30 	addi    r1,r1,48
    2d50:	7c 08 03 a6 	mtlr    r0
    2d54:	4e 80 00 20 	blr
			 * the request may have been (partially) prepped.
			 * we need to keep this request in the front to
			 * avoid resource deadlock.  REQ_STARTED will
			 * prevent other fs requests from passing this one.
			 */
			if (q->dma_drain_size && blk_rq_bytes(rq) &&
    2d58:	80 1e 01 b8 	lwz     r0,440(r30)
    2d5c:	2f 80 00 00 	cmpwi   cr7,r0,0
    2d60:	41 be ff dc 	beq-    cr7,2d3c <blk_peek_request+0x1a8>
    2d64:	80 1f 00 30 	lwz     r0,48(r31)
    2d68:	2f 80 00 00 	cmpwi   cr7,r0,0
    2d6c:	41 be ff d0 	beq-    cr7,2d3c <blk_peek_request+0x1a8>
			    !(rq->cmd_flags & REQ_DONTPREP)) {
    2d70:	80 1f 00 20 	lwz     r0,32(r31)
			 * the request may have been (partially) prepped.
			 * we need to keep this request in the front to
			 * avoid resource deadlock.  REQ_STARTED will
			 * prevent other fs requests from passing this one.
			 */
			if (q->dma_drain_size && blk_rq_bytes(rq) &&
    2d74:	74 09 00 01 	andis.  r9,r0,1
    2d78:	40 a2 ff c4 	bne-    2d3c <blk_peek_request+0x1a8>
			    !(rq->cmd_flags & REQ_DONTPREP)) {
				/*
				 * remove the space for the drain we added
				 * so that we don't add it again
				 */
				--rq->nr_phys_segments;
    2d7c:	a1 3f 00 74 	lhz     r9,116(r31)
    2d80:	38 09 ff ff 	addi    r0,r9,-1
    2d84:	b0 1f 00 74 	sth     r0,116(r31)
			}

			rq = NULL;
    2d88:	3b e0 00 00 	li      r31,0
    2d8c:	4b ff ff 48 	b       2cd4 <blk_peek_request+0x140>
			 * any debug logic in the end I/O path.
			 */
			blk_start_request(rq);
			__blk_end_request_all(rq, -EIO);
		} else {
			printk(KERN_ERR "%s: bad return=%d\n", __func__, ret);
    2d90:	3c 80 00 00 	lis     r4,0
    2d94:	3c 60 00 00 	lis     r3,0
    2d98:	38 84 00 00 	addi    r4,r4,0
    2d9c:	38 63 02 98 	addi    r3,r3,664
    2da0:	38 84 00 38 	addi    r4,r4,56
    2da4:	48 00 00 01 	bl      2da4 <blk_peek_request+0x210>
			break;
    2da8:	4b ff ff 2c 	b       2cd4 <blk_peek_request+0x140>

00002dac <blk_fetch_request>:
 *
 * Context:
 *     queue_lock must be held.
 */
struct request *blk_fetch_request(struct request_queue *q)
{
    2dac:	94 21 ff f0 	stwu    r1,-16(r1)
    2db0:	7c 08 02 a6 	mflr    r0
    2db4:	93 e1 00 0c 	stw     r31,12(r1)
    2db8:	90 01 00 14 	stw     r0,20(r1)
	struct request *rq;

	rq = blk_peek_request(q);
    2dbc:	48 00 00 01 	bl      2dbc <blk_fetch_request+0x10>
	if (rq)
    2dc0:	7c 7f 1b 79 	mr.     r31,r3
    2dc4:	41 82 00 08 	beq-    2dcc <blk_fetch_request+0x20>
		blk_start_request(rq);
    2dc8:	48 00 00 01 	bl      2dc8 <blk_fetch_request+0x1c>
	return rq;
}
    2dcc:	80 01 00 14 	lwz     r0,20(r1)
    2dd0:	7f e3 fb 78 	mr      r3,r31
    2dd4:	83 e1 00 0c 	lwz     r31,12(r1)
    2dd8:	38 21 00 10 	addi    r1,r1,16
    2ddc:	7c 08 03 a6 	mtlr    r0
    2de0:	4e 80 00 20 	blr

00002de4 <blk_rq_bio_prep>:
}
EXPORT_SYMBOL_GPL(__blk_end_request_err);

void blk_rq_bio_prep(struct request_queue *q, struct request *rq,
		     struct bio *bio)
{
    2de4:	94 21 ff f0 	stwu    r1,-16(r1)
    2de8:	7c 08 02 a6 	mflr    r0
/*
 * Check whether this bio carries any data or not. A NULL bio is allowed.
 */
static inline int bio_has_data(struct bio *bio)
{
	return bio && bio->bi_io_vec != NULL;
    2dec:	2f 85 00 00 	cmpwi   cr7,r5,0
    2df0:	bf c1 00 08 	stmw    r30,8(r1)
    2df4:	7c bf 2b 78 	mr      r31,r5
    2df8:	7c 9e 23 78 	mr      r30,r4
    2dfc:	90 01 00 14 	stw     r0,20(r1)
	/* Bit 0 (R/W) is identical in rq->cmd_flags and bio->bi_rw */
	rq->cmd_flags |= bio->bi_rw & REQ_WRITE;
    2e00:	80 05 00 14 	lwz     r0,20(r5)
    2e04:	81 24 00 20 	lwz     r9,32(r4)
    2e08:	54 00 07 fe 	clrlwi  r0,r0,31
    2e0c:	7d 20 03 78 	or      r0,r9,r0
    2e10:	90 04 00 20 	stw     r0,32(r4)
    2e14:	41 9e 00 30 	beq-    cr7,2e44 <blk_rq_bio_prep+0x60>
    2e18:	80 05 00 38 	lwz     r0,56(r5)
    2e1c:	2f 80 00 00 	cmpwi   cr7,r0,0
    2e20:	41 9e 00 24 	beq-    cr7,2e44 <blk_rq_bio_prep+0x60>

	if (bio_has_data(bio)) {
		rq->nr_phys_segments = bio_phys_segments(q, bio);
    2e24:	7c a4 2b 78 	mr      r4,r5
    2e28:	48 00 00 01 	bl      2e28 <blk_rq_bio_prep+0x44>
static inline void *bio_data(struct bio *bio)
{
	if (bio->bi_vcnt)
		return page_address(bio_page(bio)) + bio_offset(bio);

	return NULL;
    2e2c:	38 00 00 00 	li      r0,0
    2e30:	b0 7e 00 74 	sth     r3,116(r30)
		return bio->bi_size;
}

static inline void *bio_data(struct bio *bio)
{
	if (bio->bi_vcnt)
    2e34:	a1 3f 00 18 	lhz     r9,24(r31)
    2e38:	2f 89 00 00 	cmpwi   cr7,r9,0
    2e3c:	40 9e 00 40 	bne-    cr7,2e7c <blk_rq_bio_prep+0x98>
		rq->buffer = bio_data(bio);
    2e40:	90 1e 00 80 	stw     r0,128(r30)
	}
	rq->__data_len = bio->bi_size;
    2e44:	80 1f 00 20 	lwz     r0,32(r31)
	rq->bio = rq->biotail = bio;
    2e48:	93 fe 00 44 	stw     r31,68(r30)

	if (bio_has_data(bio)) {
		rq->nr_phys_segments = bio_phys_segments(q, bio);
		rq->buffer = bio_data(bio);
	}
	rq->__data_len = bio->bi_size;
    2e4c:	90 1e 00 30 	stw     r0,48(r30)
	rq->bio = rq->biotail = bio;
    2e50:	93 fe 00 40 	stw     r31,64(r30)

	if (bio->bi_bdev)
    2e54:	81 3f 00 0c 	lwz     r9,12(r31)
    2e58:	2f 89 00 00 	cmpwi   cr7,r9,0
    2e5c:	41 9e 00 0c 	beq-    cr7,2e68 <blk_rq_bio_prep+0x84>
		rq->rq_disk = bio->bi_bdev->bd_disk;
    2e60:	80 09 00 50 	lwz     r0,80(r9)
    2e64:	90 1e 00 68 	stw     r0,104(r30)
}
    2e68:	80 01 00 14 	lwz     r0,20(r1)
    2e6c:	bb c1 00 08 	lmw     r30,8(r1)
    2e70:	38 21 00 10 	addi    r1,r1,16
    2e74:	7c 08 03 a6 	mtlr    r0
    2e78:	4e 80 00 20 	blr
		return page_address(bio_page(bio)) + bio_offset(bio);
    2e7c:	a0 1f 00 1a 	lhz     r0,26(r31)
    2e80:	81 3f 00 38 	lwz     r9,56(r31)
    2e84:	54 0b 10 3a 	rlwinm  r11,r0,2,0,29
    2e88:	54 00 20 36 	rlwinm  r0,r0,4,0,27
    2e8c:	7c 0b 00 50 	subf    r0,r11,r0
    2e90:	7c 69 00 2e 	lwzx    r3,r9,r0
    2e94:	48 00 00 01 	bl      2e94 <blk_rq_bio_prep+0xb0>
    2e98:	a0 1f 00 1a 	lhz     r0,26(r31)
    2e9c:	81 3f 00 38 	lwz     r9,56(r31)
    2ea0:	54 0b 10 3a 	rlwinm  r11,r0,2,0,29
    2ea4:	54 00 20 36 	rlwinm  r0,r0,4,0,27
    2ea8:	7c 0b 00 50 	subf    r0,r11,r0
    2eac:	7d 29 02 14 	add     r9,r9,r0
    2eb0:	80 09 00 08 	lwz     r0,8(r9)
    2eb4:	7c 03 02 14 	add     r0,r3,r0
	/* Bit 0 (R/W) is identical in rq->cmd_flags and bio->bi_rw */
	rq->cmd_flags |= bio->bi_rw & REQ_WRITE;

	if (bio_has_data(bio)) {
		rq->nr_phys_segments = bio_phys_segments(q, bio);
		rq->buffer = bio_data(bio);
    2eb8:	90 1e 00 80 	stw     r0,128(r30)
    2ebc:	4b ff ff 88 	b       2e44 <blk_rq_bio_prep+0x60>

00002ec0 <init_request_from_bio>:
}
EXPORT_SYMBOL_GPL(blk_add_request_payload);

void init_request_from_bio(struct request *req, struct bio *bio)
{
	req->cpu = bio->bi_comp_cpu;
    2ec0:	80 04 00 30 	lwz     r0,48(r4)
	rq->buffer = bio_data(bio);
}
EXPORT_SYMBOL_GPL(blk_add_request_payload);

void init_request_from_bio(struct request *req, struct bio *bio)
{
    2ec4:	7c 88 23 78 	mr      r8,r4
    2ec8:	7c 69 1b 78 	mr      r9,r3
	req->cpu = bio->bi_comp_cpu;
	req->cmd_type = REQ_TYPE_FS;

	req->cmd_flags |= bio->bi_rw & REQ_COMMON_MASK;
    2ecc:	81 63 00 20 	lwz     r11,32(r3)
}
EXPORT_SYMBOL_GPL(blk_add_request_payload);

void init_request_from_bio(struct request *req, struct bio *bio)
{
	req->cpu = bio->bi_comp_cpu;
    2ed0:	90 03 00 2c 	stw     r0,44(r3)
	req->cmd_type = REQ_TYPE_FS;
    2ed4:	38 00 00 01 	li      r0,1
    2ed8:	90 03 00 24 	stw     r0,36(r3)

	req->cmd_flags |= bio->bi_rw & REQ_COMMON_MASK;
    2edc:	3c 00 01 00 	lis     r0,256
    2ee0:	60 00 20 ff 	ori     r0,r0,8447
    2ee4:	81 44 00 14 	lwz     r10,20(r4)
    2ee8:	7c 00 50 38 	and     r0,r0,r10
    2eec:	7c 00 5b 78 	or      r0,r0,r11
    2ef0:	90 03 00 20 	stw     r0,32(r3)
	if (bio->bi_rw & REQ_RAHEAD)
    2ef4:	81 64 00 14 	lwz     r11,20(r4)
    2ef8:	71 6a 02 00 	andi.   r10,r11,512
    2efc:	41 82 00 0c 	beq-    2f08 <init_request_from_bio+0x48>
		req->cmd_flags |= REQ_FAILFAST_MASK;
    2f00:	60 00 00 0e 	ori     r0,r0,14
    2f04:	90 03 00 20 	stw     r0,32(r3)

	req->errors = 0;
    2f08:	38 00 00 00 	li      r0,0
	req->__sector = bio->bi_sector;
	req->ioprio = bio_prio(bio);
	blk_rq_bio_prep(req->q, req, bio);
    2f0c:	80 69 00 1c 	lwz     r3,28(r9)
    2f10:	7d 24 4b 78 	mr      r4,r9

	req->cmd_flags |= bio->bi_rw & REQ_COMMON_MASK;
	if (bio->bi_rw & REQ_RAHEAD)
		req->cmd_flags |= REQ_FAILFAST_MASK;

	req->errors = 0;
    2f14:	90 09 00 88 	stw     r0,136(r9)
	req->__sector = bio->bi_sector;
	req->ioprio = bio_prio(bio);
	blk_rq_bio_prep(req->q, req, bio);
    2f18:	7d 05 43 78 	mr      r5,r8
	req->cmd_flags |= bio->bi_rw & REQ_COMMON_MASK;
	if (bio->bi_rw & REQ_RAHEAD)
		req->cmd_flags |= REQ_FAILFAST_MASK;

	req->errors = 0;
	req->__sector = bio->bi_sector;
    2f1c:	81 48 00 00 	lwz     r10,0(r8)
    2f20:	81 68 00 04 	lwz     r11,4(r8)
    2f24:	91 49 00 38 	stw     r10,56(r9)
    2f28:	91 69 00 3c 	stw     r11,60(r9)
	req->ioprio = bio_prio(bio);
    2f2c:	a0 08 00 14 	lhz     r0,20(r8)
    2f30:	b0 09 00 76 	sth     r0,118(r9)
	blk_rq_bio_prep(req->q, req, bio);
    2f34:	48 00 00 00 	b       2f34 <init_request_from_bio+0x74>

00002f38 <__make_request>:
{
	return !(blk_queue_nonrot(q) && blk_queue_tagged(q));
}

static int __make_request(struct request_queue *q, struct bio *bio)
{
    2f38:	94 21 ff c0 	stwu    r1,-64(r1)
    2f3c:	7c 08 02 a6 	mflr    r0
    2f40:	bf 21 00 24 	stmw    r25,36(r1)
    2f44:	7c 9b 23 78 	mr      r27,r4
    2f48:	7c 24 0b 78 	mr      r4,r1
    2f4c:	90 01 00 44 	stw     r0,68(r1)
    2f50:	7c 7f 1b 78 	mr      r31,r3
    2f54:	97 64 00 18 	stwu    r27,24(r4)
	struct request *req;
	int el_ret;
	unsigned int bytes = bio->bi_size;
	const unsigned short prio = bio_prio(bio);
    2f58:	83 bb 00 14 	lwz     r29,20(r27)

static int __make_request(struct request_queue *q, struct bio *bio)
{
	struct request *req;
	int el_ret;
	unsigned int bytes = bio->bi_size;
    2f5c:	83 9b 00 20 	lwz     r28,32(r27)
	/*
	 * low level driver can indicate that it wants pages above a
	 * certain limit bounced to low memory (ie for highmem, or even
	 * ISA dma in theory)
	 */
	blk_queue_bounce(q, &bio);
    2f60:	48 00 00 01 	bl      2f60 <__make_request+0x28>
	struct request *req;
	int el_ret;
	unsigned int bytes = bio->bi_size;
	const unsigned short prio = bio_prio(bio);
	const bool sync = !!(bio->bi_rw & REQ_SYNC);
	const bool unplug = !!(bio->bi_rw & REQ_UNPLUG);
    2f64:	57 be 05 ee 	rlwinm  r30,r29,0,23,23
}

static inline void arch_local_irq_disable(void)
{
#ifdef CONFIG_BOOKE
	asm volatile("wrteei 0" : : : "memory");
    2f68:	7c 00 01 46 	.long 0x7c000146
	 */
	blk_queue_bounce(q, &bio);

	spin_lock_irq(q->queue_lock);

	if (bio->bi_rw & (REQ_FLUSH | REQ_FUA)) {
    2f6c:	81 21 00 18 	lwz     r9,24(r1)
    2f70:	3c 00 01 00 	lis     r0,256
		where = ELEVATOR_INSERT_FRONT;
    2f74:	3b 60 00 01 	li      r27,1
	 */
	blk_queue_bounce(q, &bio);

	spin_lock_irq(q->queue_lock);

	if (bio->bi_rw & (REQ_FLUSH | REQ_FUA)) {
    2f78:	60 00 20 00 	ori     r0,r0,8192
    2f7c:	80 89 00 14 	lwz     r4,20(r9)
    2f80:	7c 89 00 39 	and.    r9,r4,r0
    2f84:	41 82 01 00 	beq-    3084 <__make_request+0x14c>
	 * This sync check and mask will be re-done in init_request_from_bio(),
	 * but we need to set it earlier to expose the sync flag to the
	 * rq allocator and io schedulers.
	 */
	rw_flags = bio_data_dir(bio);
	if (sync)
    2f88:	73 ab 00 10 	andi.   r11,r29,16
	/*
	 * This sync check and mask will be re-done in init_request_from_bio(),
	 * but we need to set it earlier to expose the sync flag to the
	 * rq allocator and io schedulers.
	 */
	rw_flags = bio_data_dir(bio);
    2f8c:	54 84 07 fe 	clrlwi  r4,r4,31
	if (sync)
    2f90:	41 82 00 08 	beq-    2f98 <__make_request+0x60>
		rw_flags |= REQ_SYNC;
    2f94:	60 84 00 10 	ori     r4,r4,16

	/*
	 * Grab a free request. This is might sleep but can not fail.
	 * Returns with the queue unlocked.
	 */
	req = get_request_wait(q, rw_flags, bio);
    2f98:	7f e3 fb 78 	mr      r3,r31
    2f9c:	4b ff f8 59 	bl      27f4 <get_request_wait.isra.53>
	 * After dropping the lock and possibly sleeping here, our request
	 * may now be mergeable after it had proven unmergeable (above).
	 * We don't worry about that case for efficiency. It won't happen
	 * often, and the elevators are able to handle it.
	 */
	init_request_from_bio(req, bio);
    2fa0:	80 81 00 18 	lwz     r4,24(r1)

	/*
	 * Grab a free request. This is might sleep but can not fail.
	 * Returns with the queue unlocked.
	 */
	req = get_request_wait(q, rw_flags, bio);
    2fa4:	90 61 00 08 	stw     r3,8(r1)
	 * After dropping the lock and possibly sleeping here, our request
	 * may now be mergeable after it had proven unmergeable (above).
	 * We don't worry about that case for efficiency. It won't happen
	 * often, and the elevators are able to handle it.
	 */
	init_request_from_bio(req, bio);
    2fa8:	48 00 00 01 	bl      2fa8 <__make_request+0x70>
    2fac:	7c 00 01 46 	.long 0x7c000146
    2fb0:	80 1f 01 78 	lwz     r0,376(r31)

	spin_lock_irq(q->queue_lock);
	if (test_bit(QUEUE_FLAG_SAME_COMP, &q->queue_flags) ||
    2fb4:	70 09 08 00 	andi.   r9,r0,2048
    2fb8:	41 82 00 94 	beq-    304c <__make_request+0x114>
	    bio_flagged(bio, BIO_CPU_AFFINE))
		req->cpu = blk_cpu_to_group(smp_processor_id());
    2fbc:	81 21 00 08 	lwz     r9,8(r1)
    2fc0:	38 00 00 00 	li      r0,0
    2fc4:	90 09 00 2c 	stw     r0,44(r9)
    2fc8:	80 1f 01 78 	lwz     r0,376(r31)
 * Only disabling plugging for non-rotational devices if it does tagging
 * as well, otherwise we do need the proper merging
 */
static inline bool queue_should_plug(struct request_queue *q)
{
	return !(blk_queue_nonrot(q) && blk_queue_tagged(q));
    2fcc:	70 09 40 00 	andi.   r9,r0,16384
    2fd0:	41 82 00 10 	beq-    2fe0 <__make_request+0xa8>
    2fd4:	80 1f 01 78 	lwz     r0,376(r31)
    2fd8:	70 0b 00 02 	andi.   r11,r0,2
    2fdc:	40 82 00 14 	bne-    2ff0 <__make_request+0xb8>

	spin_lock_irq(q->queue_lock);
	if (test_bit(QUEUE_FLAG_SAME_COMP, &q->queue_flags) ||
	    bio_flagged(bio, BIO_CPU_AFFINE))
		req->cpu = blk_cpu_to_group(smp_processor_id());
	if (queue_should_plug(q) && elv_queue_empty(q))
    2fe0:	7f e3 fb 78 	mr      r3,r31
    2fe4:	48 00 00 01 	bl      2fe4 <__make_request+0xac>
    2fe8:	2f 83 00 00 	cmpwi   cr7,r3,0
    2fec:	40 9e 01 d0 	bne-    cr7,31bc <__make_request+0x284>
		blk_plug_device(q);

	/* insert the request into the elevator */
	drive_stat_acct(req, 1);
    2ff0:	80 61 00 08 	lwz     r3,8(r1)
    2ff4:	38 80 00 01 	li      r4,1
    2ff8:	4b ff da bd 	bl      ab4 <drive_stat_acct>
	__elv_add_request(q, req, where, 0);
    2ffc:	80 81 00 08 	lwz     r4,8(r1)
    3000:	7f e3 fb 78 	mr      r3,r31
    3004:	7f 65 db 78 	mr      r5,r27
    3008:	38 c0 00 00 	li      r6,0
    300c:	48 00 00 01 	bl      300c <__make_request+0xd4>
out:
	if (unplug || !queue_should_plug(q))
    3010:	2f 9e 00 00 	cmpwi   cr7,r30,0
    3014:	40 9e 00 4c 	bne-    cr7,3060 <__make_request+0x128>
    3018:	80 1f 01 78 	lwz     r0,376(r31)
 * Only disabling plugging for non-rotational devices if it does tagging
 * as well, otherwise we do need the proper merging
 */
static inline bool queue_should_plug(struct request_queue *q)
{
	return !(blk_queue_nonrot(q) && blk_queue_tagged(q));
    301c:	70 09 40 00 	andi.   r9,r0,16384
    3020:	41 82 00 10 	beq-    3030 <__make_request+0xf8>
    3024:	80 1f 01 78 	lwz     r0,376(r31)
    3028:	70 0b 00 02 	andi.   r11,r0,2
    302c:	40 82 00 34 	bne-    3060 <__make_request+0x128>
}

static inline void arch_local_irq_enable(void)
{
#ifdef CONFIG_BOOKE
	asm volatile("wrteei 1" : : : "memory");
    3030:	7c 00 81 46 	.long 0x7c008146
out:
	if (unplug || !queue_should_plug(q))
		__generic_unplug_device(q);
	spin_unlock_irq(q->queue_lock);
	return 0;
}
    3034:	80 01 00 44 	lwz     r0,68(r1)
    3038:	38 60 00 00 	li      r3,0
    303c:	bb 21 00 24 	lmw     r25,36(r1)
    3040:	38 21 00 40 	addi    r1,r1,64
    3044:	7c 08 03 a6 	mtlr    r0
    3048:	4e 80 00 20 	blr
	 */
	init_request_from_bio(req, bio);

	spin_lock_irq(q->queue_lock);
	if (test_bit(QUEUE_FLAG_SAME_COMP, &q->queue_flags) ||
	    bio_flagged(bio, BIO_CPU_AFFINE))
    304c:	81 21 00 18 	lwz     r9,24(r1)
    3050:	80 09 00 10 	lwz     r0,16(r9)
	 * often, and the elevators are able to handle it.
	 */
	init_request_from_bio(req, bio);

	spin_lock_irq(q->queue_lock);
	if (test_bit(QUEUE_FLAG_SAME_COMP, &q->queue_flags) ||
    3054:	70 0b 01 00 	andi.   r11,r0,256
    3058:	41 a2 ff 70 	beq-    2fc8 <__make_request+0x90>
    305c:	4b ff ff 60 	b       2fbc <__make_request+0x84>
	/* insert the request into the elevator */
	drive_stat_acct(req, 1);
	__elv_add_request(q, req, where, 0);
out:
	if (unplug || !queue_should_plug(q))
		__generic_unplug_device(q);
    3060:	7f e3 fb 78 	mr      r3,r31
    3064:	48 00 00 01 	bl      3064 <__make_request+0x12c>
    3068:	7c 00 81 46 	.long 0x7c008146
	spin_unlock_irq(q->queue_lock);
	return 0;
}
    306c:	80 01 00 44 	lwz     r0,68(r1)
    3070:	38 60 00 00 	li      r3,0
    3074:	bb 21 00 24 	lmw     r25,36(r1)
    3078:	38 21 00 40 	addi    r1,r1,64
    307c:	7c 08 03 a6 	mtlr    r0
    3080:	4e 80 00 20 	blr
	if (bio->bi_rw & (REQ_FLUSH | REQ_FUA)) {
		where = ELEVATOR_INSERT_FRONT;
		goto get_rq;
	}

	if (elv_queue_empty(q))
    3084:	7f e3 fb 78 	mr      r3,r31
    3088:	48 00 00 01 	bl      3088 <__make_request+0x150>
    308c:	2f 83 00 00 	cmpwi   cr7,r3,0
    3090:	41 9e 00 14 	beq-    cr7,30a4 <__make_request+0x16c>
		goto out;

	case ELEVATOR_FRONT_MERGE:
		BUG_ON(!rq_mergeable(req));

		if (!ll_front_merge_fn(q, req, bio))
    3094:	81 21 00 18 	lwz     r9,24(r1)
	unsigned int bytes = bio->bi_size;
	const unsigned short prio = bio_prio(bio);
	const bool sync = !!(bio->bi_rw & REQ_SYNC);
	const bool unplug = !!(bio->bi_rw & REQ_UNPLUG);
	const unsigned long ff = bio->bi_rw & REQ_FAILFAST_MASK;
	int where = ELEVATOR_INSERT_SORT;
    3098:	3b 60 00 03 	li      r27,3
		goto out;

	case ELEVATOR_FRONT_MERGE:
		BUG_ON(!rq_mergeable(req));

		if (!ll_front_merge_fn(q, req, bio))
    309c:	80 89 00 14 	lwz     r4,20(r9)
    30a0:	4b ff fe e8 	b       2f88 <__make_request+0x50>
	}

	if (elv_queue_empty(q))
		goto get_rq;

	el_ret = elv_merge(q, &req, bio);
    30a4:	80 a1 00 18 	lwz     r5,24(r1)
    30a8:	7f e3 fb 78 	mr      r3,r31
    30ac:	38 81 00 08 	addi    r4,r1,8
	int el_ret;
	unsigned int bytes = bio->bi_size;
	const unsigned short prio = bio_prio(bio);
	const bool sync = !!(bio->bi_rw & REQ_SYNC);
	const bool unplug = !!(bio->bi_rw & REQ_UNPLUG);
	const unsigned long ff = bio->bi_rw & REQ_FAILFAST_MASK;
    30b0:	57 bb 07 3c 	rlwinm  r27,r29,0,28,30
static int __make_request(struct request_queue *q, struct bio *bio)
{
	struct request *req;
	int el_ret;
	unsigned int bytes = bio->bi_size;
	const unsigned short prio = bio_prio(bio);
    30b4:	57 ba 84 3e 	rlwinm  r26,r29,16,16,31
	}

	if (elv_queue_empty(q))
		goto get_rq;

	el_ret = elv_merge(q, &req, bio);
    30b8:	48 00 00 01 	bl      30b8 <__make_request+0x180>
	switch (el_ret) {
    30bc:	2f 83 00 01 	cmpwi   cr7,r3,1
    30c0:	41 9e 01 08 	beq-    cr7,31c8 <__make_request+0x290>
    30c4:	2f 83 00 02 	cmpwi   cr7,r3,2
    30c8:	40 9e ff cc 	bne+    cr7,3094 <__make_request+0x15c>
	case ELEVATOR_BACK_MERGE:
		BUG_ON(!rq_mergeable(req));
    30cc:	80 81 00 08 	lwz     r4,8(r1)
    30d0:	3d 20 01 00 	lis     r9,256
    30d4:	61 29 f0 00 	ori     r9,r9,61440
    30d8:	80 04 00 20 	lwz     r0,32(r4)
    30dc:	7c 0b 48 39 	and.    r11,r0,r9
    30e0:	39 20 00 01 	li      r9,1
    30e4:	40 82 00 24 	bne-    3108 <__make_request+0x1d0>
    30e8:	70 09 00 40 	andi.   r9,r0,64
    30ec:	39 20 00 00 	li      r9,0
    30f0:	40 82 00 18 	bne-    3108 <__make_request+0x1d0>
static inline bool queue_should_plug(struct request_queue *q)
{
	return !(blk_queue_nonrot(q) && blk_queue_tagged(q));
}

static int __make_request(struct request_queue *q, struct bio *bio)
    30f4:	81 24 00 24 	lwz     r9,36(r4)
    30f8:	69 29 00 01 	xori    r9,r9,1
    30fc:	7d 29 00 34 	cntlzw  r9,r9
    3100:	55 29 d9 7e 	rlwinm  r9,r9,27,5,31
    3104:	69 29 00 01 	xori    r9,r9,1
		goto get_rq;

	el_ret = elv_merge(q, &req, bio);
	switch (el_ret) {
	case ELEVATOR_BACK_MERGE:
		BUG_ON(!rq_mergeable(req));
    3108:	0f 09 00 00 	twnei   r9,0

		if (!ll_back_merge_fn(q, req, bio))
    310c:	80 a1 00 18 	lwz     r5,24(r1)
    3110:	7f e3 fb 78 	mr      r3,r31
    3114:	48 00 00 01 	bl      3114 <__make_request+0x1dc>
    3118:	2f 83 00 00 	cmpwi   cr7,r3,0
    311c:	41 be ff 78 	beq-    cr7,3094 <__make_request+0x15c>
			break;

		trace_block_bio_backmerge(q, bio);

		if ((req->cmd_flags & REQ_FAILFAST_MASK) != ff)
    3120:	83 a1 00 08 	lwz     r29,8(r1)
    3124:	80 1d 00 20 	lwz     r0,32(r29)
    3128:	54 00 07 3c 	rlwinm  r0,r0,0,28,30
    312c:	7f 80 d8 00 	cmpw    cr7,r0,r27
    3130:	41 9e 00 10 	beq-    cr7,3140 <__make_request+0x208>
			blk_rq_set_mixed_merge(req);
    3134:	7f a3 eb 78 	mr      r3,r29
    3138:	48 00 00 01 	bl      3138 <__make_request+0x200>
    313c:	83 a1 00 08 	lwz     r29,8(r1)

		req->biotail->bi_next = bio;
    3140:	80 01 00 18 	lwz     r0,24(r1)
		req->biotail = bio;
		req->__data_len += bytes;
		req->ioprio = ioprio_best(req->ioprio, prio);
    3144:	7f 44 d3 78 	mr      r4,r26
		trace_block_bio_backmerge(q, bio);

		if ((req->cmd_flags & REQ_FAILFAST_MASK) != ff)
			blk_rq_set_mixed_merge(req);

		req->biotail->bi_next = bio;
    3148:	81 3d 00 44 	lwz     r9,68(r29)
    314c:	90 09 00 08 	stw     r0,8(r9)
		req->biotail = bio;
    3150:	90 1d 00 44 	stw     r0,68(r29)
		req->__data_len += bytes;
    3154:	80 1d 00 30 	lwz     r0,48(r29)
		req->ioprio = ioprio_best(req->ioprio, prio);
    3158:	a0 7d 00 76 	lhz     r3,118(r29)
		if ((req->cmd_flags & REQ_FAILFAST_MASK) != ff)
			blk_rq_set_mixed_merge(req);

		req->biotail->bi_next = bio;
		req->biotail = bio;
		req->__data_len += bytes;
    315c:	7f 80 e2 14 	add     r28,r0,r28
    3160:	93 9d 00 30 	stw     r28,48(r29)
		req->ioprio = ioprio_best(req->ioprio, prio);
    3164:	48 00 00 01 	bl      3164 <__make_request+0x22c>
    3168:	b0 7d 00 76 	sth     r3,118(r29)
		if (!blk_rq_cpu_valid(req))
    316c:	80 61 00 08 	lwz     r3,8(r1)
    3170:	80 03 00 2c 	lwz     r0,44(r3)
    3174:	2f 80 ff ff 	cmpwi   cr7,r0,-1
    3178:	41 9e 01 c4 	beq-    cr7,333c <__make_request+0x404>
			req->cpu = bio->bi_comp_cpu;
		drive_stat_acct(req, 0);
    317c:	38 80 00 00 	li      r4,0
    3180:	4b ff d9 35 	bl      ab4 <drive_stat_acct>
		elv_bio_merged(q, req, bio);
    3184:	80 81 00 08 	lwz     r4,8(r1)
    3188:	7f e3 fb 78 	mr      r3,r31
    318c:	80 a1 00 18 	lwz     r5,24(r1)
    3190:	48 00 00 01 	bl      3190 <__make_request+0x258>
		if (!attempt_back_merge(q, req))
    3194:	80 81 00 08 	lwz     r4,8(r1)
    3198:	7f e3 fb 78 	mr      r3,r31
    319c:	48 00 00 01 	bl      319c <__make_request+0x264>
    31a0:	2f 83 00 00 	cmpwi   cr7,r3,0
    31a4:	40 9e fe 6c 	bne+    cr7,3010 <__make_request+0xd8>
			elv_merged_request(q, req, el_ret);
    31a8:	80 81 00 08 	lwz     r4,8(r1)
    31ac:	7f e3 fb 78 	mr      r3,r31
    31b0:	38 a0 00 02 	li      r5,2
    31b4:	48 00 00 01 	bl      31b4 <__make_request+0x27c>
    31b8:	4b ff fe 58 	b       3010 <__make_request+0xd8>
	spin_lock_irq(q->queue_lock);
	if (test_bit(QUEUE_FLAG_SAME_COMP, &q->queue_flags) ||
	    bio_flagged(bio, BIO_CPU_AFFINE))
		req->cpu = blk_cpu_to_group(smp_processor_id());
	if (queue_should_plug(q) && elv_queue_empty(q))
		blk_plug_device(q);
    31bc:	7f e3 fb 78 	mr      r3,r31
    31c0:	48 00 00 01 	bl      31c0 <__make_request+0x288>
    31c4:	4b ff fe 2c 	b       2ff0 <__make_request+0xb8>
		if (!attempt_back_merge(q, req))
			elv_merged_request(q, req, el_ret);
		goto out;

	case ELEVATOR_FRONT_MERGE:
		BUG_ON(!rq_mergeable(req));
    31c8:	80 81 00 08 	lwz     r4,8(r1)
    31cc:	3d 20 01 00 	lis     r9,256
    31d0:	61 29 f0 00 	ori     r9,r9,61440
    31d4:	80 04 00 20 	lwz     r0,32(r4)
    31d8:	7c 0b 48 39 	and.    r11,r0,r9
    31dc:	39 20 00 01 	li      r9,1
    31e0:	40 82 00 24 	bne-    3204 <__make_request+0x2cc>
    31e4:	70 09 00 40 	andi.   r9,r0,64
    31e8:	39 20 00 00 	li      r9,0
    31ec:	40 82 00 18 	bne-    3204 <__make_request+0x2cc>
static inline bool queue_should_plug(struct request_queue *q)
{
	return !(blk_queue_nonrot(q) && blk_queue_tagged(q));
}

static int __make_request(struct request_queue *q, struct bio *bio)
    31f0:	81 24 00 24 	lwz     r9,36(r4)
    31f4:	69 29 00 01 	xori    r9,r9,1
    31f8:	7d 29 00 34 	cntlzw  r9,r9
    31fc:	55 29 d9 7e 	rlwinm  r9,r9,27,5,31
    3200:	69 29 00 01 	xori    r9,r9,1
		if (!attempt_back_merge(q, req))
			elv_merged_request(q, req, el_ret);
		goto out;

	case ELEVATOR_FRONT_MERGE:
		BUG_ON(!rq_mergeable(req));
    3204:	0f 09 00 00 	twnei   r9,0

		if (!ll_front_merge_fn(q, req, bio))
    3208:	80 a1 00 18 	lwz     r5,24(r1)
    320c:	7f e3 fb 78 	mr      r3,r31
    3210:	48 00 00 01 	bl      3210 <__make_request+0x2d8>
    3214:	2f 83 00 00 	cmpwi   cr7,r3,0
    3218:	41 be fe 7c 	beq-    cr7,3094 <__make_request+0x15c>
			break;

		trace_block_bio_frontmerge(q, bio);

		if ((req->cmd_flags & REQ_FAILFAST_MASK) != ff) {
    321c:	83 a1 00 08 	lwz     r29,8(r1)
    3220:	80 1d 00 20 	lwz     r0,32(r29)
    3224:	7f b9 eb 78 	mr      r25,r29
    3228:	54 00 07 3c 	rlwinm  r0,r0,0,28,30
    322c:	7f 80 d8 00 	cmpw    cr7,r0,r27
    3230:	41 9e 00 24 	beq-    cr7,3254 <__make_request+0x31c>
			blk_rq_set_mixed_merge(req);
    3234:	7f a3 eb 78 	mr      r3,r29
    3238:	48 00 00 01 	bl      3238 <__make_request+0x300>
			req->cmd_flags &= ~REQ_FAILFAST_MASK;
    323c:	83 a1 00 08 	lwz     r29,8(r1)
    3240:	80 1d 00 20 	lwz     r0,32(r29)
    3244:	7f b9 eb 78 	mr      r25,r29
    3248:	54 00 07 f6 	rlwinm  r0,r0,0,31,27
			req->cmd_flags |= ff;
    324c:	7c 1b db 78 	or      r27,r0,r27
    3250:	93 7d 00 20 	stw     r27,32(r29)
		}

		bio->bi_next = req->bio;
    3254:	83 61 00 18 	lwz     r27,24(r1)

	return NULL;
    3258:	38 00 00 00 	li      r0,0
    325c:	81 3d 00 40 	lwz     r9,64(r29)
    3260:	91 3b 00 08 	stw     r9,8(r27)
		req->bio = bio;
    3264:	93 7d 00 40 	stw     r27,64(r29)
		return bio->bi_size;
}

static inline void *bio_data(struct bio *bio)
{
	if (bio->bi_vcnt)
    3268:	a1 3b 00 18 	lhz     r9,24(r27)
    326c:	2f 89 00 00 	cmpwi   cr7,r9,0
    3270:	40 9e 00 84 	bne-    cr7,32f4 <__make_request+0x3bc>
		/*
		 * may not be valid. if the low level driver said
		 * it didn't need a bounce buffer then it better
		 * not touch req->buffer either...
		 */
		req->buffer = bio_data(bio);
    3274:	90 19 00 80 	stw     r0,128(r25)
		req->__sector = bio->bi_sector;
		req->__data_len += bytes;
		req->ioprio = ioprio_best(req->ioprio, prio);
    3278:	7f 44 d3 78 	mr      r4,r26
		 * it didn't need a bounce buffer then it better
		 * not touch req->buffer either...
		 */
		req->buffer = bio_data(bio);
		req->__sector = bio->bi_sector;
		req->__data_len += bytes;
    327c:	80 1d 00 30 	lwz     r0,48(r29)
		 * may not be valid. if the low level driver said
		 * it didn't need a bounce buffer then it better
		 * not touch req->buffer either...
		 */
		req->buffer = bio_data(bio);
		req->__sector = bio->bi_sector;
    3280:	81 5b 00 00 	lwz     r10,0(r27)
    3284:	81 7b 00 04 	lwz     r11,4(r27)
		req->__data_len += bytes;
    3288:	7f 80 e2 14 	add     r28,r0,r28
    328c:	93 9d 00 30 	stw     r28,48(r29)
		 * may not be valid. if the low level driver said
		 * it didn't need a bounce buffer then it better
		 * not touch req->buffer either...
		 */
		req->buffer = bio_data(bio);
		req->__sector = bio->bi_sector;
    3290:	91 5d 00 38 	stw     r10,56(r29)
    3294:	91 7d 00 3c 	stw     r11,60(r29)
		req->__data_len += bytes;
		req->ioprio = ioprio_best(req->ioprio, prio);
    3298:	a0 7d 00 76 	lhz     r3,118(r29)
    329c:	48 00 00 01 	bl      329c <__make_request+0x364>
    32a0:	b0 7d 00 76 	sth     r3,118(r29)
		if (!blk_rq_cpu_valid(req))
    32a4:	80 61 00 08 	lwz     r3,8(r1)
    32a8:	80 03 00 2c 	lwz     r0,44(r3)
    32ac:	2f 80 ff ff 	cmpwi   cr7,r0,-1
    32b0:	41 9e 00 9c 	beq-    cr7,334c <__make_request+0x414>
			req->cpu = bio->bi_comp_cpu;
		drive_stat_acct(req, 0);
    32b4:	38 80 00 00 	li      r4,0
    32b8:	4b ff d7 fd 	bl      ab4 <drive_stat_acct>
		elv_bio_merged(q, req, bio);
    32bc:	80 81 00 08 	lwz     r4,8(r1)
    32c0:	7f e3 fb 78 	mr      r3,r31
    32c4:	80 a1 00 18 	lwz     r5,24(r1)
    32c8:	48 00 00 01 	bl      32c8 <__make_request+0x390>
		if (!attempt_front_merge(q, req))
    32cc:	80 81 00 08 	lwz     r4,8(r1)
    32d0:	7f e3 fb 78 	mr      r3,r31
    32d4:	48 00 00 01 	bl      32d4 <__make_request+0x39c>
    32d8:	2f 83 00 00 	cmpwi   cr7,r3,0
    32dc:	40 9e fd 34 	bne+    cr7,3010 <__make_request+0xd8>
			elv_merged_request(q, req, el_ret);
    32e0:	80 81 00 08 	lwz     r4,8(r1)
    32e4:	7f e3 fb 78 	mr      r3,r31
    32e8:	38 a0 00 01 	li      r5,1
    32ec:	48 00 00 01 	bl      32ec <__make_request+0x3b4>
    32f0:	4b ff fd 20 	b       3010 <__make_request+0xd8>
		return page_address(bio_page(bio)) + bio_offset(bio);
    32f4:	a0 1b 00 1a 	lhz     r0,26(r27)
    32f8:	81 3b 00 38 	lwz     r9,56(r27)
    32fc:	54 0b 10 3a 	rlwinm  r11,r0,2,0,29
    3300:	54 00 20 36 	rlwinm  r0,r0,4,0,27
    3304:	7c 0b 00 50 	subf    r0,r11,r0
    3308:	7c 69 00 2e 	lwzx    r3,r9,r0
    330c:	48 00 00 01 	bl      330c <__make_request+0x3d4>
    3310:	a0 1b 00 1a 	lhz     r0,26(r27)
    3314:	81 3b 00 38 	lwz     r9,56(r27)
    3318:	54 0b 10 3a 	rlwinm  r11,r0,2,0,29
    331c:	54 00 20 36 	rlwinm  r0,r0,4,0,27
    3320:	83 61 00 18 	lwz     r27,24(r1)
    3324:	7c 0b 00 50 	subf    r0,r11,r0
    3328:	83 a1 00 08 	lwz     r29,8(r1)
    332c:	7d 29 02 14 	add     r9,r9,r0
    3330:	80 09 00 08 	lwz     r0,8(r9)
    3334:	7c 03 02 14 	add     r0,r3,r0
    3338:	4b ff ff 3c 	b       3274 <__make_request+0x33c>
		req->biotail->bi_next = bio;
		req->biotail = bio;
		req->__data_len += bytes;
		req->ioprio = ioprio_best(req->ioprio, prio);
		if (!blk_rq_cpu_valid(req))
			req->cpu = bio->bi_comp_cpu;
    333c:	81 21 00 18 	lwz     r9,24(r1)
    3340:	80 09 00 30 	lwz     r0,48(r9)
    3344:	90 03 00 2c 	stw     r0,44(r3)
    3348:	4b ff fe 34 	b       317c <__make_request+0x244>
		req->buffer = bio_data(bio);
		req->__sector = bio->bi_sector;
		req->__data_len += bytes;
		req->ioprio = ioprio_best(req->ioprio, prio);
		if (!blk_rq_cpu_valid(req))
			req->cpu = bio->bi_comp_cpu;
    334c:	81 21 00 18 	lwz     r9,24(r1)
    3350:	80 09 00 30 	lwz     r0,48(r9)
    3354:	90 03 00 2c 	stw     r0,44(r3)
    3358:	4b ff ff 5c 	b       32b4 <__make_request+0x37c>

Disassembly of section .init.text:

00000000 <blk_dev_init>:
	return queue_work(kblockd_workqueue, work);
}
EXPORT_SYMBOL(kblockd_schedule_work);

int __init blk_dev_init(void)
{
   0:	94 21 ff f0 	stwu    r1,-16(r1)
   4:	7c 08 02 a6 	mflr    r0
	BUILD_BUG_ON(__REQ_NR_BITS > 8 *
			sizeof(((struct request *)0)->cmd_flags));

	/* used for unplugging and affects IO latency/throughput - HIGHPRI */
	kblockd_workqueue = alloc_workqueue("kblockd",
   8:	3c 60 00 00 	lis     r3,0
   c:	38 80 00 18 	li      r4,24
  10:	38 63 02 b0 	addi    r3,r3,688
  14:	38 a0 00 00 	li      r5,0
  18:	38 c0 00 00 	li      r6,0
  1c:	38 e0 00 00 	li      r7,0
	return queue_work(kblockd_workqueue, work);
}
EXPORT_SYMBOL(kblockd_schedule_work);

int __init blk_dev_init(void)
{
  20:	93 e1 00 0c 	stw     r31,12(r1)
  24:	90 01 00 14 	stw     r0,20(r1)
	BUILD_BUG_ON(__REQ_NR_BITS > 8 *
			sizeof(((struct request *)0)->cmd_flags));

	/* used for unplugging and affects IO latency/throughput - HIGHPRI */
	kblockd_workqueue = alloc_workqueue("kblockd",
  28:	48 00 00 01 	bl      28 <blk_dev_init+0x28>
  2c:	3d 20 00 00 	lis     r9,0
					    WQ_MEM_RECLAIM | WQ_HIGHPRI, 0);
	if (!kblockd_workqueue)
  30:	2f 83 00 00 	cmpwi   cr7,r3,0
{
	BUILD_BUG_ON(__REQ_NR_BITS > 8 *
			sizeof(((struct request *)0)->cmd_flags));

	/* used for unplugging and affects IO latency/throughput - HIGHPRI */
	kblockd_workqueue = alloc_workqueue("kblockd",
  34:	3b e9 00 00 	addi    r31,r9,0
  38:	90 69 00 00 	stw     r3,0(r9)
					    WQ_MEM_RECLAIM | WQ_HIGHPRI, 0);
	if (!kblockd_workqueue)
  3c:	40 be 00 10 	bne+    cr7,4c <blk_dev_init+0x4c>
		panic("Failed to create kblockd\n");
  40:	3c 60 00 00 	lis     r3,0
  44:	38 63 02 b8 	addi    r3,r3,696
  48:	48 00 00 01 	bl      48 <blk_dev_init+0x48>

	request_cachep = kmem_cache_create("blkdev_requests",
  4c:	3c 60 00 00 	lis     r3,0
  50:	38 80 00 d8 	li      r4,216
  54:	38 a0 00 00 	li      r5,0
  58:	3c c0 00 04 	lis     r6,4
  5c:	38 e0 00 00 	li      r7,0
  60:	38 63 02 d4 	addi    r3,r3,724
  64:	48 00 00 01 	bl      64 <blk_dev_init+0x64>
			sizeof(struct request), 0, SLAB_PANIC, NULL);

	blk_requestq_cachep = kmem_cache_create("blkdev_queue",
  68:	38 80 03 48 	li      r4,840
  6c:	38 a0 00 00 	li      r5,0
	kblockd_workqueue = alloc_workqueue("kblockd",
					    WQ_MEM_RECLAIM | WQ_HIGHPRI, 0);
	if (!kblockd_workqueue)
		panic("Failed to create kblockd\n");

	request_cachep = kmem_cache_create("blkdev_requests",
  70:	90 7f 00 04 	stw     r3,4(r31)
			sizeof(struct request), 0, SLAB_PANIC, NULL);

	blk_requestq_cachep = kmem_cache_create("blkdev_queue",
  74:	3c 60 00 00 	lis     r3,0
  78:	3c c0 00 04 	lis     r6,4
  7c:	38 63 02 e4 	addi    r3,r3,740
  80:	38 e0 00 00 	li      r7,0
  84:	48 00 00 01 	bl      84 <blk_dev_init+0x84>
			sizeof(struct request_queue), 0, SLAB_PANIC, NULL);

	return 0;
}
  88:	80 01 00 14 	lwz     r0,20(r1)
		panic("Failed to create kblockd\n");

	request_cachep = kmem_cache_create("blkdev_requests",
			sizeof(struct request), 0, SLAB_PANIC, NULL);

	blk_requestq_cachep = kmem_cache_create("blkdev_queue",
  8c:	3d 20 00 00 	lis     r9,0
  90:	90 69 00 00 	stw     r3,0(r9)
			sizeof(struct request_queue), 0, SLAB_PANIC, NULL);

	return 0;
}
  94:	38 60 00 00 	li      r3,0
  98:	7c 08 03 a6 	mtlr    r0
  9c:	83 e1 00 0c 	lwz     r31,12(r1)
  a0:	38 21 00 10 	addi    r1,r1,16
  a4:	4e 80 00 20 	blr
